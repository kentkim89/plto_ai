import streamlit as st
import pandas as pd
import io
import numpy as np
import openpyxl
from openpyxl.styles import PatternFill, Border, Side, Alignment
from openpyxl.utils import get_column_letter
from datetime import datetime, timedelta
import requests
from office365.runtime.auth.client_credential import ClientCredential
from office365.sharepoint.client_context import ClientContext
from office365.sharepoint.files.file import File
import tempfile
import os
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import google.generativeai as genai
import json
from typing import Dict, List, Tuple, Any
import base64
from urllib.parse import urlparse, parse_qs

# --------------------------------------------------------------------------
# Streamlit ÌéòÏù¥ÏßÄ ÏÑ§Ï†ï
# --------------------------------------------------------------------------
st.set_page_config(
    page_title="Ï£ºÎ¨∏ Ï≤òÎ¶¨ ÏûêÎèôÌôî ÏãúÏä§ÌÖú v2.0", 
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'About': "Ï£ºÎ¨∏ Ï≤òÎ¶¨ ÏûêÎèôÌôî ÏãúÏä§ÌÖú v2.0 - SharePoint & AI Powered"
    }
)

# --------------------------------------------------------------------------
# SharePoint Ïó∞Í≤∞ Ìï®Ïàò
# --------------------------------------------------------------------------
@st.cache_resource
def get_sharepoint_context():
    """SharePoint Ïù∏Ï¶ù Ïª®ÌÖçÏä§Ìä∏ ÏÉùÏÑ±"""
    try:
        site_url = f"https://goremi.sharepoint.com/sites/{st.secrets['sharepoint_files']['site_name']}"
        client_id = st.secrets["sharepoint"]["client_id"]
        client_secret = st.secrets["sharepoint"]["client_secret"]
        
        credentials = ClientCredential(client_id, client_secret)
        ctx = ClientContext(site_url).with_credentials(credentials)
        
        # Ïó∞Í≤∞ ÌÖåÏä§Ìä∏
        web = ctx.web
        ctx.load(web)
        ctx.execute_query()
        
        return ctx
    except Exception as e:
        st.error(f"SharePoint Ïó∞Í≤∞ Ïò§Î•ò: {e}")
        return None

def extract_file_path_from_url(sharepoint_url):
    """SharePoint URLÏóêÏÑú ÌååÏùº Í≤ΩÎ°ú Ï∂îÏ∂ú"""
    try:
        # URLÏóêÏÑú ÌååÏùº ID Ï∂îÏ∂ú
        parsed = urlparse(sharepoint_url)
        
        # ÏßÅÏ†ë Îã§Ïö¥Î°úÎìú URL ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò
        if "sharepoint.com/:x:" in sharepoint_url:  # Excel ÌååÏùº
            # Í≥µÏú† ÎßÅÌÅ¨Î•º Îã§Ïö¥Î°úÎìú URLÎ°ú Î≥ÄÌôò
            download_url = sharepoint_url.replace("/:x:/", "/_layouts/15/download.aspx?share=")
            download_url = download_url.split("?")[0] + "?share=" + sharepoint_url.split("/")[-1].split("?")[0]
            return download_url
        else:
            return sharepoint_url
    except Exception as e:
        st.error(f"URL ÌååÏã± Ïò§Î•ò: {e}")
        return None

@st.cache_data(ttl=600)  # 10Î∂Ñ Ï∫êÏãú
def load_master_from_sharepoint():
    """SharePointÏóêÏÑú ÎßàÏä§ÌÑ∞ Îç∞Ïù¥ÌÑ∞ Î°úÎìú"""
    try:
        ctx = get_sharepoint_context()
        if not ctx:
            raise Exception("SharePoint Ïª®ÌÖçÏä§Ìä∏ ÏÉùÏÑ± Ïã§Ìå®")
        
        # ÌååÏùº Í≤ΩÎ°ú ÏÑ§Ï†ï
        site_name = st.secrets["sharepoint_files"]["site_name"]
        file_name = st.secrets["sharepoint_files"]["file_name"]
        
        # ÏÉÅÎåÄ URL Íµ¨ÏÑ±
        file_url = f"/sites/{site_name}/Shared Documents/{file_name}"
        
        try:
            # SharePointÏóêÏÑú ÌååÏùº Îã§Ïö¥Î°úÎìú
            response = File.open_binary(ctx, file_url)
            
            # BytesIO Í∞ùÏ≤¥Î°ú Î≥ÄÌôò
            bytes_file_obj = io.BytesIO()
            bytes_file_obj.write(response.content)
            bytes_file_obj.seek(0)
            
            # Excel ÌååÏùº ÏùΩÍ∏∞
            df_master = pd.read_excel(bytes_file_obj)
            df_master = df_master.drop_duplicates(subset=['SKUÏΩîÎìú'], keep='first')
            
            st.success(f"‚úÖ ÎßàÏä§ÌÑ∞ Îç∞Ïù¥ÌÑ∞ Î°úÎìú ÏôÑÎ£å: {len(df_master)}Í∞ú ÌíàÎ™©")
            return df_master
            
        except Exception as e:
            st.warning(f"SharePoint ÌååÏùº Ï†ëÍ∑º Ïã§Ìå®: {e}")
            
            # ÎåÄÏ≤¥ Î∞©Î≤ï: ÏßÅÏ†ë URL Ï†ëÍ∑º
            master_url = st.secrets["sharepoint_files"]["plto_master_data_file_url"]
            response = requests.get(master_url)
            if response.status_code == 200:
                df_master = pd.read_excel(io.BytesIO(response.content))
                df_master = df_master.drop_duplicates(subset=['SKUÏΩîÎìú'], keep='first')
                st.success(f"‚úÖ ÎßàÏä§ÌÑ∞ Îç∞Ïù¥ÌÑ∞ Î°úÎìú ÏôÑÎ£å (ÏßÅÏ†ë URL): {len(df_master)}Í∞ú ÌíàÎ™©")
                return df_master
            else:
                raise Exception(f"ÌååÏùº Îã§Ïö¥Î°úÎìú Ïã§Ìå®: {response.status_code}")
                
    except Exception as e:
        st.error(f"ÎßàÏä§ÌÑ∞ Îç∞Ïù¥ÌÑ∞ Î°úÎìú Ïã§Ìå®: {e}")
        
        # ÏµúÏ¢Ö Ìè¥Î∞±: Î°úÏª¨ ÌååÏùº
        try:
            if os.path.exists("master_data.csv"):
                df_master = pd.read_csv("master_data.csv")
                df_master = df_master.drop_duplicates(subset=['SKUÏΩîÎìú'], keep='first')
                st.warning(f"‚ö†Ô∏è Î°úÏª¨ Î∞±ÏóÖ ÌååÏùº ÏÇ¨Ïö©: {len(df_master)}Í∞ú ÌíàÎ™©")
                return df_master
        except:
            pass
            
        return None

def load_record_data_from_sharepoint():
    """SharePointÏóêÏÑú Í∏∞Î°ù Îç∞Ïù¥ÌÑ∞ Î°úÎìú"""
    try:
        ctx = get_sharepoint_context()
        if not ctx:
            return pd.DataFrame()
        
        site_name = st.secrets["sharepoint_files"]["site_name"]
        record_file_name = st.secrets["sharepoint_files"].get("record_file_name", "plto_record_data.xlsx")
        
        file_url = f"/sites/{site_name}/Shared Documents/{record_file_name}"
        
        try:
            response = File.open_binary(ctx, file_url)
            bytes_file_obj = io.BytesIO()
            bytes_file_obj.write(response.content)
            bytes_file_obj.seek(0)
            
            df_record = pd.read_excel(bytes_file_obj)
            return df_record
        except Exception as e:
            # ÌååÏùºÏù¥ ÏóÜÍ±∞ÎÇò ÎπÑÏñ¥ÏûàÏúºÎ©¥ Îπà DataFrame Î∞òÌôò
            st.info(f"Í∏∞Î°ù ÌååÏùºÏù¥ ÏóÜÍ±∞ÎÇò ÎπÑÏñ¥ÏûàÏäµÎãàÎã§. ÏÉàÎ°ú ÏÉùÏÑ±Îê©ÎãàÎã§.")
            return pd.DataFrame()
            
    except Exception as e:
        st.error(f"Í∏∞Î°ù Îç∞Ïù¥ÌÑ∞ Î°úÎìú Ïã§Ìå®: {e}")
        return pd.DataFrame()

def save_record_to_sharepoint(df_new_records):
    """SharePointÏóê Í∏∞Î°ù Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•"""
    try:
        ctx = get_sharepoint_context()
        if not ctx:
            return False, 0
        
        site_name = st.secrets["sharepoint_files"]["site_name"]
        record_file_name = st.secrets["sharepoint_files"].get("record_file_name", "plto_record_data.xlsx")
        
        # Í∏∞Ï°¥ Îç∞Ïù¥ÌÑ∞ Î°úÎìú
        df_existing = load_record_data_from_sharepoint()
        
        # Ìó§Îçî Ï†ïÏùò
        expected_columns = [
            'Ï≤òÎ¶¨ÏùºÏãú', 'Ï£ºÎ¨∏ÏùºÏûê', 'ÏáºÌïëÎ™∞', 'Í±∞ÎûòÏ≤òÎ™Ö', 'ÌíàÎ™©ÏΩîÎìú', 'SKUÏÉÅÌíàÎ™Ö', 
            'Ï£ºÎ¨∏ÏàòÎüâ', 'Ïã§Í≤∞Ï†úÍ∏àÏï°', 'Í≥µÍ∏âÍ∞ÄÏï°', 'Î∂ÄÍ∞ÄÏÑ∏', 'ÏàòÎ†πÏûêÎ™Ö', 
            'Í≥ºÏÑ∏Ïó¨Î∂Ä', 'Í±∞ÎûòÏú†Ìòï', 'Ï≤òÎ¶¨Ïûê', 'ÏõêÎ≥∏ÌååÏùºÎ™Ö'
        ]
        
        # Í∏∞Ï°¥ Îç∞Ïù¥ÌÑ∞Í∞Ä ÎπÑÏñ¥ÏûàÏúºÎ©¥ Ìó§Îçî ÏÑ§Ï†ï
        if df_existing.empty:
            df_existing = pd.DataFrame(columns=expected_columns)
        
        # ÏÉà Î†àÏΩîÎìú Ï§ÄÎπÑ
        df_new_records['Ï≤òÎ¶¨ÏùºÏãú'] = datetime.now()
        df_new_records['Ï≤òÎ¶¨Ïûê'] = st.session_state.get('user_name', 'Unknown')
        df_new_records['ÏõêÎ≥∏ÌååÏùºÎ™Ö'] = st.session_state.get('current_files', '')
        
        # Ï§ëÎ≥µ Ï≤¥ÌÅ¨ (Ï£ºÎ¨∏ÏùºÏûê, ÏáºÌïëÎ™∞, ÏàòÎ†πÏûêÎ™Ö, ÌíàÎ™©ÏΩîÎìú Í∏∞Ï§Ä)
        if not df_existing.empty:
            merge_keys = ['Ï£ºÎ¨∏ÏùºÏûê', 'ÏáºÌïëÎ™∞', 'ÏàòÎ†πÏûêÎ™Ö', 'ÌíàÎ™©ÏΩîÎìú']
            
            # Í∞Å Ïª¨ÎüºÏù¥ Ï°¥Ïû¨ÌïòÎäîÏßÄ ÌôïÏù∏
            for key in merge_keys:
                if key not in df_existing.columns:
                    df_existing[key] = ''
                if key not in df_new_records.columns:
                    df_new_records[key] = ''
            
            df_existing['check_key'] = df_existing[merge_keys].astype(str).agg('_'.join, axis=1)
            df_new_records['check_key'] = df_new_records[merge_keys].astype(str).agg('_'.join, axis=1)
            
            # Ï§ëÎ≥µÎêòÏßÄ ÏïäÎäî Î†àÏΩîÎìúÎßå ÌïÑÌÑ∞ÎßÅ
            new_keys = set(df_new_records['check_key']) - set(df_existing['check_key'])
            df_new_records = df_new_records[df_new_records['check_key'].isin(new_keys)]
            
            # check_key Ïª¨Îüº Ï†úÍ±∞
            df_existing = df_existing.drop('check_key', axis=1, errors='ignore')
            df_new_records = df_new_records.drop('check_key', axis=1, errors='ignore')
        
        # ÎàÑÎùΩÎêú Ïª¨Îüº Ï∂îÍ∞Ä
        for col in expected_columns:
            if col not in df_new_records.columns:
                df_new_records[col] = ''
            if col not in df_existing.columns:
                df_existing[col] = ''
        
        # Îç∞Ïù¥ÌÑ∞ Í≤∞Ìï©
        df_combined = pd.concat([df_existing, df_new_records], ignore_index=True)
        
        # ÎÇ†Ïßú ÌòïÏãù Ï†ïÎ¶¨
        if 'Ï≤òÎ¶¨ÏùºÏãú' in df_combined.columns:
            df_combined['Ï≤òÎ¶¨ÏùºÏãú'] = pd.to_datetime(df_combined['Ï≤òÎ¶¨ÏùºÏãú'], errors='coerce')
        if 'Ï£ºÎ¨∏ÏùºÏûê' in df_combined.columns:
            df_combined['Ï£ºÎ¨∏ÏùºÏûê'] = pd.to_datetime(df_combined['Ï£ºÎ¨∏ÏùºÏûê'], errors='coerce')
        
        # Ï†ïÎ†¨
        df_combined = df_combined.sort_values('Ï≤òÎ¶¨ÏùºÏãú', ascending=False)
        
        # ÏûÑÏãú ÌååÏùºÎ°ú Ï†ÄÏû•
        with tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx') as tmp:
            with pd.ExcelWriter(tmp.name, engine='openpyxl') as writer:
                df_combined.to_excel(writer, index=False, sheet_name='Records')
                
                # ÏõåÌÅ¨ÏãúÌä∏ Í∞ÄÏ†∏Ïò§Í∏∞
                worksheet = writer.sheets['Records']
                
                # Ìó§Îçî ÏÑúÏãù Ï†ÅÏö©
                header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
                header_font = openpyxl.styles.Font(color="FFFFFF", bold=True)
                
                for cell in worksheet[1]:
                    cell.fill = header_fill
                    cell.font = header_font
                
                # Ïó¥ ÎÑàÎπÑ ÏûêÎèô Ï°∞Ï†ï
                for column_cells in worksheet.columns:
                    length = max(len(str(cell.value or '')) for cell in column_cells)
                    worksheet.column_dimensions[column_cells[0].column_letter].width = min(length + 2, 50)
            
            tmp.flush()
            
            # SharePointÏóê ÏóÖÎ°úÎìú
            with open(tmp.name, 'rb') as file_content:
                file_url = f"/sites/{site_name}/Shared Documents/{record_file_name}"
                
                folder = ctx.web.get_folder_by_server_relative_path(f"sites/{site_name}/Shared Documents")
                target_file = folder.upload_file(record_file_name, file_content.read())
                ctx.execute_query()
            
            os.unlink(tmp.name)
        
        return True, len(df_new_records)
        
    except Exception as e:
        st.error(f"Í∏∞Î°ù Ï†ÄÏû• Ïã§Ìå®: {e}")
        import traceback
        st.error(traceback.format_exc())
        return False, 0

# --------------------------------------------------------------------------
# Gemini AI Î∂ÑÏÑù Ìï®Ïàò
# --------------------------------------------------------------------------
def initialize_gemini():
    """Gemini AI Ï¥àÍ∏∞Ìôî"""
    try:
        genai.configure(api_key=st.secrets["gemini"]["api_key"])
        model = genai.GenerativeModel('gemini-pro')
        return model
    except Exception as e:
        st.error(f"Gemini AI Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
        return None

def analyze_with_gemini(df_data, analysis_type="trend"):
    """Gemini AIÎ•º ÏÇ¨Ïö©Ìïú Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù"""
    model = initialize_gemini()
    if not model or df_data.empty:
        return None
    
    try:
        # Îç∞Ïù¥ÌÑ∞ ÏöîÏïΩ ÏÉùÏÑ±
        summary_stats = df_data.describe().to_string()
        
        # ÏÉÅÏúÑ ÌíàÎ™© Ï†ïÎ≥¥
        top_products = df_data.groupby('SKUÏÉÅÌíàÎ™Ö')['Ïã§Í≤∞Ï†úÍ∏àÏï°'].sum().nlargest(10).to_string()
        
        # ÏáºÌïëÎ™∞Î≥Ñ Îß§Ï∂ú
        mall_sales = df_data.groupby('ÏáºÌïëÎ™∞')['Ïã§Í≤∞Ï†úÍ∏àÏï°'].sum().to_string()
        
        # Î∂ÑÏÑù Ïú†ÌòïÎ≥Ñ ÌîÑÎ°¨ÌîÑÌä∏
        prompts = {
            "trend": f"""
            Îã§Ïùå ÌåêÎß§ Îç∞Ïù¥ÌÑ∞Î•º Î∂ÑÏÑùÌïòÏó¨ Ï£ºÏöî Ìä∏Î†åÎìúÏôÄ Ïù∏ÏÇ¨Ïù¥Ìä∏Î•º ÌïúÍµ≠Ïñ¥Î°ú Ï†úÍ≥µÌï¥Ï£ºÏÑ∏Ïöî:
            
            [ÌÜµÍ≥Ñ ÏöîÏïΩ]
            {summary_stats}
            
            [Î≤†Ïä§Ìä∏ÏÖÄÎü¨ TOP 10]
            {top_products}
            
            [ÏáºÌïëÎ™∞Î≥Ñ Îß§Ï∂ú]
            {mall_sales}
            
            Îã§Ïùå Ìï≠Î™©Îì§ÏùÑ Ìè¨Ìï®Ìï¥Ï£ºÏÑ∏Ïöî:
            1. üìà Ï†ÑÏ≤¥Ï†ÅÏù∏ ÌåêÎß§ Ìä∏Î†åÎìú
            2. üèÜ Î≤†Ïä§Ìä∏ÏÖÄÎü¨ ÏÉÅÌíà Î∂ÑÏÑù
            3. üõçÔ∏è ÏáºÌïëÎ™∞Î≥Ñ ÌäπÏßï
            4. üìä Í≥ÑÏ†àÏÑ± ÎòêÎäî Ï£ºÍ∏∞Ï†Å Ìå®ÌÑ¥
            5. üí° Í∞úÏÑ† Ï†úÏïàÏÇ¨Ìï≠
            
            Í∞Å Ìï≠Î™©ÏùÑ Î™ÖÌôïÌïòÍ≤å Íµ¨Î∂ÑÌïòÍ≥†, Ïù¥Î™®ÏßÄÎ•º ÌôúÏö©ÌïòÏó¨ ÏùΩÍ∏∞ ÏâΩÍ≤å ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.
            """,
            
            "forecast": f"""
            Îã§Ïùå ÌåêÎß§ Îç∞Ïù¥ÌÑ∞Î•º Î∞îÌÉïÏúºÎ°ú Ìñ•ÌõÑ ÏòàÏ∏°ÏùÑ ÌïúÍµ≠Ïñ¥Î°ú Ï†úÍ≥µÌï¥Ï£ºÏÑ∏Ïöî:
            
            [ÌÜµÍ≥Ñ ÏöîÏïΩ]
            {summary_stats}
            
            [ÏÉÅÏúÑ ÌíàÎ™©]
            {top_products}
            
            Îã§ÏùåÏùÑ Ìè¨Ìï®Ìï¥Ï£ºÏÑ∏Ïöî:
            1. üìÖ Îã§Ïùå Ï£º/Ïõî ÏòàÏÉÅ ÌåêÎß§Îüâ
            2. üì¶ Ï£ºÏùòÍ∞Ä ÌïÑÏöîÌïú Ïû¨Í≥† ÌíàÎ™©
            3. üöÄ ÏÑ±Ïû• Í∞ÄÎä•ÏÑ±Ïù¥ ÎÜíÏùÄ Ïπ¥ÌÖåÍ≥†Î¶¨
            4. ‚ö†Ô∏è Î¶¨Ïä§ÌÅ¨ ÏöîÏù∏
            
            Íµ¨Ï≤¥Ï†ÅÏù∏ ÏàòÏπòÏôÄ Ìï®Íªò Ïã§Ìñâ Í∞ÄÎä•Ìïú Ï†úÏïàÏùÑ Ìï¥Ï£ºÏÑ∏Ïöî.
            """,
            
            "anomaly": f"""
            Îã§Ïùå ÌåêÎß§ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Ïù¥ÏÉÅ Ìå®ÌÑ¥Ïù¥ÎÇò ÌäπÏù¥ÏÇ¨Ìï≠ÏùÑ ÌïúÍµ≠Ïñ¥Î°ú Ï∞æÏïÑÏ£ºÏÑ∏Ïöî:
            
            [ÌÜµÍ≥Ñ ÏöîÏïΩ]
            {summary_stats}
            
            [ÏáºÌïëÎ™∞Î≥Ñ Îß§Ï∂ú]
            {mall_sales}
            
            Îã§ÏùåÏùÑ ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî:
            1. üîç ÎπÑÏ†ïÏÉÅÏ†ÅÏù∏ Ï£ºÎ¨∏ Ìå®ÌÑ¥
            2. üìâ Í∏âÍ≤©Ìïú Î≥ÄÌôîÍ∞Ä ÏûàÎäî ÌíàÎ™©
            3. ‚ö° Ï£ºÏùòÍ∞Ä ÌïÑÏöîÌïú Í±∞Îûò
            4. üîß Îç∞Ïù¥ÌÑ∞ ÌíàÏßà Ïù¥Ïäà
            
            Î∞úÍ≤¨Îêú Ïù¥ÏÉÅ Ìï≠Î™©Ïóê ÎåÄÌïú ÎåÄÏùë Î∞©ÏïàÎèÑ Ï†úÏãúÌï¥Ï£ºÏÑ∏Ïöî.
            """
        }
        
        prompt = prompts.get(analysis_type, prompts["trend"])
        response = model.generate_content(prompt)
        
        return response.text
        
    except Exception as e:
        st.error(f"AI Î∂ÑÏÑù Ïã§Ìå®: {e}")
        return None

# --------------------------------------------------------------------------
# Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî Ìï®Ïàò
# --------------------------------------------------------------------------
def create_dashboard(df_record):
    """ÎåÄÏãúÎ≥¥Îìú ÏÉùÏÑ±"""
    if df_record.empty:
        st.warning("üìä Î∂ÑÏÑùÌï† Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.")
        return
    
    # Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨
    df_record['Ï£ºÎ¨∏ÏùºÏûê'] = pd.to_datetime(df_record['Ï£ºÎ¨∏ÏùºÏûê'], errors='coerce')
    df_record = df_record.dropna(subset=['Ï£ºÎ¨∏ÏùºÏûê'])
    
    # ÏÉâÏÉÅ ÌåîÎ†àÌä∏
    colors = px.colors.qualitative.Set3
    
    # Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        total_revenue = df_record['Ïã§Í≤∞Ï†úÍ∏àÏï°'].sum()
        st.metric("Ï¥ù Îß§Ï∂ú", f"‚Ç©{total_revenue:,.0f}")
    
    with col2:
        total_orders = len(df_record)
        st.metric("Ï¥ù Ï£ºÎ¨∏ Ïàò", f"{total_orders:,}")
    
    with col3:
        avg_order_value = total_revenue / total_orders if total_orders > 0 else 0
        st.metric("ÌèâÍ∑† Ï£ºÎ¨∏ Í∏àÏï°", f"‚Ç©{avg_order_value:,.0f}")
    
    with col4:
        unique_customers = df_record['ÏàòÎ†πÏûêÎ™Ö'].nunique()
        st.metric("Í≥†Ïú† Í≥†Í∞ù Ïàò", f"{unique_customers:,}")
    
    # Ï∞®Ìä∏ ÏÉùÏÑ±
    tab1, tab2, tab3, tab4 = st.tabs(["üìà ÏùºÎ≥Ñ Ìä∏Î†åÎìú", "üè™ ÏáºÌïëÎ™∞Î≥Ñ Î∂ÑÏÑù", "üì¶ ÏÉÅÌíàÎ≥Ñ Î∂ÑÏÑù", "ü§ñ AI Ïù∏ÏÇ¨Ïù¥Ìä∏"])
    
    with tab1:
        col1, col2 = st.columns(2)
        
        with col1:
            # ÏùºÎ≥Ñ Îß§Ï∂ú Ìä∏Î†åÎìú
            daily_sales = df_record.groupby(df_record['Ï£ºÎ¨∏ÏùºÏûê'].dt.date)['Ïã§Í≤∞Ï†úÍ∏àÏï°'].sum().reset_index()
            
            fig = px.line(daily_sales, x='Ï£ºÎ¨∏ÏùºÏûê', y='Ïã§Í≤∞Ï†úÍ∏àÏï°',
                         title='ÏùºÎ≥Ñ Îß§Ï∂ú Ìä∏Î†åÎìú',
                         labels={'Ïã§Í≤∞Ï†úÍ∏àÏï°': 'Îß§Ï∂ú (Ïõê)', 'Ï£ºÎ¨∏ÏùºÏûê': 'ÎÇ†Ïßú'},
                         color_discrete_sequence=[colors[0]])
            fig.update_layout(hovermode='x unified', height=400)
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Ï£ºÎ≥Ñ Îß§Ï∂ú Ìä∏Î†åÎìú
            df_record['Ï£ºÏ∞®'] = df_record['Ï£ºÎ¨∏ÏùºÏûê'].dt.isocalendar().week
            df_record['Ïó∞ÎèÑ'] = df_record['Ï£ºÎ¨∏ÏùºÏûê'].dt.year
            weekly_sales = df_record.groupby(['Ïó∞ÎèÑ', 'Ï£ºÏ∞®'])['Ïã§Í≤∞Ï†úÍ∏àÏï°'].sum().reset_index()
            weekly_sales['Ïó∞ÎèÑ_Ï£ºÏ∞®'] = weekly_sales['Ïó∞ÎèÑ'].astype(str) + '-W' + weekly_sales['Ï£ºÏ∞®'].astype(str).str.zfill(2)
            
            fig2 = px.bar(weekly_sales, x='Ïó∞ÎèÑ_Ï£ºÏ∞®', y='Ïã§Í≤∞Ï†úÍ∏àÏï°',
                         title='Ï£ºÎ≥Ñ Îß§Ï∂ú Ìä∏Î†åÎìú',
                         labels={'Ïã§Í≤∞Ï†úÍ∏àÏï°': 'Îß§Ï∂ú (Ïõê)', 'Ïó∞ÎèÑ_Ï£ºÏ∞®': 'Ïó∞ÎèÑ-Ï£ºÏ∞®'},
                         color_discrete_sequence=[colors[1]])
            fig2.update_layout(height=400)
            st.plotly_chart(fig2, use_container_width=True)
        
        # ÏãúÍ∞ÑÎåÄÎ≥Ñ Î∂ÑÏÑù
        if 'Ï≤òÎ¶¨ÏùºÏãú' in df_record.columns:
            df_record['Ï≤òÎ¶¨ÏãúÍ∞Ñ'] = pd.to_datetime(df_record['Ï≤òÎ¶¨ÏùºÏãú'], errors='coerce').dt.hour
            hourly_orders = df_record.groupby('Ï≤òÎ¶¨ÏãúÍ∞Ñ').size().reset_index(name='Ï£ºÎ¨∏Ïàò')
            
            fig3 = px.bar(hourly_orders, x='Ï≤òÎ¶¨ÏãúÍ∞Ñ', y='Ï£ºÎ¨∏Ïàò',
                         title='ÏãúÍ∞ÑÎåÄÎ≥Ñ Ï£ºÎ¨∏ Î∂ÑÌè¨',
                         labels={'Ï≤òÎ¶¨ÏãúÍ∞Ñ': 'ÏãúÍ∞Ñ', 'Ï£ºÎ¨∏Ïàò': 'Ï£ºÎ¨∏ Í±¥Ïàò'},
                         color_discrete_sequence=[colors[2]])
            fig3.update_layout(height=300)
            st.plotly_chart(fig3, use_container_width=True)
    
    with tab2:
        col1, col2 = st.columns(2)
        
        with col1:
            # ÏáºÌïëÎ™∞Î≥Ñ Îß§Ï∂ú ÎπÑÏ§ë
            mall_sales = df_record.groupby('ÏáºÌïëÎ™∞')['Ïã§Í≤∞Ï†úÍ∏àÏï°'].sum().reset_index()
            
            fig = px.pie(mall_sales, values='Ïã§Í≤∞Ï†úÍ∏àÏï°', names='ÏáºÌïëÎ™∞',
                        title='ÏáºÌïëÎ™∞Î≥Ñ Îß§Ï∂ú ÎπÑÏ§ë',
                        color_discrete_sequence=colors)
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # ÏáºÌïëÎ™∞Î≥Ñ ÌèâÍ∑† Ï£ºÎ¨∏ Í∏àÏï°
            mall_avg = df_record.groupby('ÏáºÌïëÎ™∞')['Ïã§Í≤∞Ï†úÍ∏àÏï°'].mean().reset_index()
            mall_avg.columns = ['ÏáºÌïëÎ™∞', 'ÌèâÍ∑†Ï£ºÎ¨∏Í∏àÏï°']
            
            fig2 = px.bar(mall_avg, x='ÏáºÌïëÎ™∞', y='ÌèâÍ∑†Ï£ºÎ¨∏Í∏àÏï°',
                          title='ÏáºÌïëÎ™∞Î≥Ñ ÌèâÍ∑† Ï£ºÎ¨∏ Í∏àÏï°',
                          labels={'ÌèâÍ∑†Ï£ºÎ¨∏Í∏àÏï°': 'ÌèâÍ∑† Í∏àÏï° (Ïõê)'},
                          color='ÏáºÌïëÎ™∞',
                          color_discrete_sequence=colors)
            fig2.update_layout(showlegend=False, height=400)
            st.plotly_chart(fig2, use_container_width=True)
        
        # ÏáºÌïëÎ™∞Î≥Ñ ÏùºÎ≥Ñ Ìä∏Î†åÎìú
        mall_daily = df_record.groupby([df_record['Ï£ºÎ¨∏ÏùºÏûê'].dt.date, 'ÏáºÌïëÎ™∞'])['Ïã§Í≤∞Ï†úÍ∏àÏï°'].sum().reset_index()
        
        fig3 = px.line(mall_daily, x='Ï£ºÎ¨∏ÏùºÏûê', y='Ïã§Í≤∞Ï†úÍ∏àÏï°', color='ÏáºÌïëÎ™∞',
                      title='ÏáºÌïëÎ™∞Î≥Ñ ÏùºÎ≥Ñ Îß§Ï∂ú Ìä∏Î†åÎìú',
                      color_discrete_sequence=colors)
        fig3.update_layout(height=400)
        st.plotly_chart(fig3, use_container_width=True)
    
    with tab3:
        col1, col2 = st.columns(2)
        
        with col1:
            # TOP 10 ÏÉÅÌíà (Îß§Ï∂ú Í∏∞Ï§Ä)
            product_sales = df_record.groupby('SKUÏÉÅÌíàÎ™Ö').agg({
                'Ï£ºÎ¨∏ÏàòÎüâ': 'sum',
                'Ïã§Í≤∞Ï†úÍ∏àÏï°': 'sum'
            }).reset_index()
            
            top_products = product_sales.nlargest(10, 'Ïã§Í≤∞Ï†úÍ∏àÏï°')
            
            fig = px.bar(top_products, x='Ïã§Í≤∞Ï†úÍ∏àÏï°', y='SKUÏÉÅÌíàÎ™Ö',
                         orientation='h', title='TOP 10 Î≤†Ïä§Ìä∏ÏÖÄÎü¨ (Îß§Ï∂ú Í∏∞Ï§Ä)',
                         labels={'Ïã§Í≤∞Ï†úÍ∏àÏï°': 'Îß§Ï∂ú (Ïõê)', 'SKUÏÉÅÌíàÎ™Ö': 'ÏÉÅÌíàÎ™Ö'},
                         color='Ïã§Í≤∞Ï†úÍ∏àÏï°',
                         color_continuous_scale='Blues')
            fig.update_layout(yaxis={'categoryorder':'total ascending'}, height=500)
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # TOP 10 ÏÉÅÌíà (ÏàòÎüâ Í∏∞Ï§Ä)
            top_qty = product_sales.nlargest(10, 'Ï£ºÎ¨∏ÏàòÎüâ')
            
            fig2 = px.bar(top_qty, x='Ï£ºÎ¨∏ÏàòÎüâ', y='SKUÏÉÅÌíàÎ™Ö',
                          orientation='h', title='TOP 10 Î≤†Ïä§Ìä∏ÏÖÄÎü¨ (ÏàòÎüâ Í∏∞Ï§Ä)',
                          labels={'Ï£ºÎ¨∏ÏàòÎüâ': 'ÌåêÎß§ ÏàòÎüâ', 'SKUÏÉÅÌíàÎ™Ö': 'ÏÉÅÌíàÎ™Ö'},
                          color='Ï£ºÎ¨∏ÏàòÎüâ',
                          color_continuous_scale='Greens')
            fig2.update_layout(yaxis={'categoryorder':'total ascending'}, height=500)
            st.plotly_chart(fig2, use_container_width=True)
        
        # ÏÉÅÌíàÎ≥Ñ ÌåêÎß§ Ìä∏Î¶¨Îßµ
        fig3 = px.treemap(product_sales.nlargest(20, 'Ï£ºÎ¨∏ÏàòÎüâ'), 
                         path=['SKUÏÉÅÌíàÎ™Ö'], values='Ï£ºÎ¨∏ÏàòÎüâ',
                         title='ÏÉÅÌíàÎ≥Ñ ÌåêÎß§ ÏàòÎüâ Î∂ÑÌè¨ (TOP 20)',
                         color='Ïã§Í≤∞Ï†úÍ∏àÏï°',
                         color_continuous_scale='RdYlBu')
        fig3.update_layout(height=500)
        st.plotly_chart(fig3, use_container_width=True)
    
    with tab4:
        st.subheader("ü§ñ AI Í∏∞Î∞ò Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù")
        
        col1, col2 = st.columns([1, 3])
        
        with col1:
            analysis_type = st.selectbox(
                "Î∂ÑÏÑù Ïú†Ìòï ÏÑ†ÌÉù",
                ["trend", "forecast", "anomaly"],
                format_func=lambda x: {
                    "trend": "üìà Ìä∏Î†åÎìú Î∂ÑÏÑù",
                    "forecast": "üîÆ ÌåêÎß§ ÏòàÏ∏°",
                    "anomaly": "‚ö†Ô∏è Ïù¥ÏÉÅ Ìå®ÌÑ¥ Í∞êÏßÄ"
                }[x]
            )
            
            if st.button("üöÄ AI Î∂ÑÏÑù Ïã§Ìñâ", type="primary", use_container_width=True):
                st.session_state['run_analysis'] = True
        
        with col2:
            if st.session_state.get('run_analysis', False):
                with st.spinner("AIÍ∞Ä Îç∞Ïù¥ÌÑ∞Î•º Î∂ÑÏÑù Ï§ëÏûÖÎãàÎã§... ‚è≥"):
                    analysis_result = analyze_with_gemini(df_record, analysis_type)
                    if analysis_result:
                        st.markdown("### üìä Î∂ÑÏÑù Í≤∞Í≥º")
                        st.markdown(analysis_result)
                        st.session_state['run_analysis'] = False
                    else:
                        st.error("AI Î∂ÑÏÑùÏùÑ ÏàòÌñâÌï† Ïàò ÏóÜÏäµÎãàÎã§.")
                        st.session_state['run_analysis'] = False

# --------------------------------------------------------------------------
# Í∏∞Ï°¥ Ìï®ÏàòÎì§ (ÏàòÏ†ï Î∞è Ïú†ÏßÄ)
# --------------------------------------------------------------------------
def to_excel_formatted(df, format_type=None):
    """Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏùÑ ÏÑúÏãùÏù¥ Ï†ÅÏö©Îêú ÏóëÏÖÄ ÌååÏùº ÌòïÏãùÏùò BytesIO Í∞ùÏ≤¥Î°ú Î≥ÄÌôòÌïòÎäî Ìï®Ïàò"""
    output = io.BytesIO()
    df_to_save = df.fillna('')
    
    if format_type == 'ecount_upload':
        df_to_save = df_to_save.rename(columns={'Ï†ÅÏöî_Ï†ÑÌëú': 'Ï†ÅÏöî', 'Ï†ÅÏöî_ÌíàÎ™©': 'Ï†ÅÏöî.1'})

    df_to_save.to_excel(output, index=False, sheet_name='Sheet1')
    
    workbook = openpyxl.load_workbook(output)
    sheet = workbook.active

    # Í≥µÌÜµ ÏÑúÏãù: Î™®Îì† ÏÖÄ Í∞ÄÏö¥Îç∞ Ï†ïÎ†¨
    center_alignment = Alignment(horizontal='center', vertical='center')
    for row in sheet.iter_rows():
        for cell in row:
            cell.alignment = center_alignment

    # ÌååÏùºÎ≥Ñ ÌäπÏàò ÏÑúÏãù
    for column_cells in sheet.columns:
        max_length = 0
        column = column_cells[0].column_letter
        for cell in column_cells:
            try:
                if cell.value:
                    if len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
            except:
                pass
        adjusted_width = (max_length + 2) * 1.2
        sheet.column_dimensions[column].width = adjusted_width
    
    thin_border = Border(left=Side(style='thin'), right=Side(style='thin'), top=Side(style='thin'), bottom=Side(style='thin'))
    pink_fill = PatternFill(start_color="FFEBEE", end_color="FFEBEE", fill_type="solid")

    if format_type == 'packing_list':
        for row in sheet.iter_rows(min_row=1, max_row=sheet.max_row, min_col=1, max_col=sheet.max_column):
            for cell in row:
                cell.border = thin_border
        
        bundle_start_row = 2
        for row_num in range(2, sheet.max_row + 2):
            current_bundle_cell = sheet.cell(row=row_num, column=1) if row_num <= sheet.max_row else None
            
            if (current_bundle_cell and current_bundle_cell.value) or (row_num > sheet.max_row):
                if row_num > 2:
                    bundle_end_row = row_num - 1
                    prev_bundle_num_str = str(sheet.cell(row=bundle_start_row, column=1).value)
                    
                    if prev_bundle_num_str.isdigit():
                        prev_bundle_num = int(prev_bundle_num_str)
                        if prev_bundle_num % 2 != 0:
                            for r in range(bundle_start_row, bundle_end_row + 1):
                                for c in range(1, sheet.max_column + 1):
                                    sheet.cell(row=r, column=c).fill = pink_fill
                    
                    if bundle_start_row < bundle_end_row:
                        sheet.merge_cells(start_row=bundle_start_row, start_column=1, end_row=bundle_end_row, end_column=1)
                        sheet.merge_cells(start_row=bundle_start_row, start_column=4, end_row=bundle_end_row, end_column=4)
                
                bundle_start_row = row_num

    if format_type == 'quantity_summary':
        for row_idx, row in enumerate(sheet.iter_rows(min_row=1, max_row=sheet.max_row, min_col=1, max_col=sheet.max_column)):
            for cell in row:
                cell.border = thin_border
            if row_idx > 0 and row_idx % 2 != 0:
                for cell in row:
                    cell.fill = pink_fill
    
    final_output = io.BytesIO()
    workbook.save(final_output)
    final_output.seek(0)
    
    return final_output.getvalue()

def extract_order_date(df_ecount):
    """Ïù¥Ïπ¥Ïö¥Ìä∏ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Ï£ºÎ¨∏ÏùºÏûê Ï∂îÏ∂ú"""
    try:
        # ÏùºÏûê Ïª¨ÎüºÏù¥ ÏûàÏúºÎ©¥ ÏÇ¨Ïö©, ÏóÜÏúºÎ©¥ Ïò§Îäò ÎÇ†Ïßú ÏÇ¨Ïö©
        if 'ÏùºÏûê' in df_ecount.columns:
            order_date = pd.to_datetime(df_ecount['ÏùºÏûê'].iloc[0], format='%Y%m%d', errors='coerce')
            if pd.isna(order_date):
                order_date = datetime.now().date()
            else:
                order_date = order_date.date()
        else:
            order_date = datetime.now().date()
        return order_date
    except:
        return datetime.now().date()

def process_all_files(file1, file2, file3, df_master):
    try:
        df_smartstore = pd.read_excel(file1)
        df_ecount_orig = pd.read_excel(file2)
        df_godomall = pd.read_excel(file3)

        # ÌååÏùºÎ™Ö Ï†ÄÏû•
        st.session_state['current_files'] = f"{file1.name}, {file2.name}, {file3.name}"

        # Ï£ºÎ¨∏ÏùºÏûê Ï∂îÏ∂ú
        order_date = extract_order_date(df_ecount_orig)

        df_ecount_orig['original_order'] = range(len(df_ecount_orig))
        
        # Ïª¨ÎüºÎ™Ö Ìò∏ÌôòÏÑ± Ï≤òÎ¶¨
        if 'Ìöå Ìï†Ïù∏ Í∏àÏï°' in df_godomall.columns and 'ÌöåÏõê Ìï†Ïù∏ Í∏àÏï°' not in df_godomall.columns:
            df_godomall.rename(columns={'Ìöå Ìï†Ïù∏ Í∏àÏï°': 'ÌöåÏõê Ìï†Ïù∏ Í∏àÏï°'}, inplace=True)
        if 'ÏûêÏ≤¥ÏòµÏÖòÏΩîÎìú' in df_godomall.columns:
            df_godomall.rename(columns={'ÏûêÏ≤¥ÏòµÏÖòÏΩîÎìú': 'Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú'}, inplace=True)
        
        # 1Îã®Í≥Ñ: Îç∞Ïù¥ÌÑ∞ ÌÅ¥Î¶¨Îãù Í∞ïÌôî
        cols_to_numeric = ['ÏÉÅÌíàÎ≥Ñ ÌíàÎ™©Í∏àÏï°', 'Ï¥ù Î∞∞ÏÜ° Í∏àÏï°', 'ÌöåÏõê Ìï†Ïù∏ Í∏àÏï°', 'Ïø†Ìè∞ Ìï†Ïù∏ Í∏àÏï°', 'ÏÇ¨Ïö©Îêú ÎßàÏùºÎ¶¨ÏßÄ', 'Ï¥ù Í≤∞Ï†ú Í∏àÏï°']
        for col in cols_to_numeric:
            if col in df_godomall.columns: 
                df_godomall[col] = pd.to_numeric(df_godomall[col].astype(str).str.replace('[Ïõê,]', '', regex=True), errors='coerce').fillna(0)
        
        # 2Îã®Í≥Ñ: Î∞∞ÏÜ°ÎπÑ Ï§ëÎ≥µ Í≥ÑÏÇ∞ Î∞©ÏßÄ
        df_godomall['Î≥¥Ï†ïÎêú_Î∞∞ÏÜ°ÎπÑ'] = np.where(
            df_godomall.duplicated(subset=['ÏàòÏ∑®Ïù∏ Ïù¥Î¶Ñ']), 
            0, 
            df_godomall['Ï¥ù Î∞∞ÏÜ° Í∏àÏï°']
        )
        
        df_godomall['ÏàòÏ†ïÎê†_Í∏àÏï°_Í≥†ÎèÑÎ™∞'] = (
            df_godomall['ÏÉÅÌíàÎ≥Ñ ÌíàÎ™©Í∏àÏï°'] + df_godomall['Î≥¥Ï†ïÎêú_Î∞∞ÏÜ°ÎπÑ'] - df_godomall['ÌöåÏõê Ìï†Ïù∏ Í∏àÏï°'] - 
            df_godomall['Ïø†Ìè∞ Ìï†Ïù∏ Í∏àÏï°'] - df_godomall['ÏÇ¨Ïö©Îêú ÎßàÏùºÎ¶¨ÏßÄ']
        )
        
        # 3Îã®Í≥Ñ: Í≤∞Ï†ú Í∏àÏï° Í≤ÄÏ¶ù Î∞è ÏïåÎ¶º Í∏∞Îä• Ï∂îÍ∞Ä
        godomall_warnings = []
        grouped_godomall = df_godomall.groupby('ÏàòÏ∑®Ïù∏ Ïù¥Î¶Ñ')
        
        for name, group in grouped_godomall:
            calculated_total = group['ÏàòÏ†ïÎê†_Í∏àÏï°_Í≥†ÎèÑÎ™∞'].sum()
            actual_total = group['Ï¥ù Í≤∞Ï†ú Í∏àÏï°'].iloc[0]
            discrepancy = calculated_total - actual_total
            
            if abs(discrepancy) > 1:
                warning_msg = f"- [Í≥†ÎèÑÎ™∞ Í∏àÏï° Î∂àÏùºÏπò] **{name}**ÎãòÏùò Ï£ºÎ¨∏Ïùò Í≥ÑÏÇ∞Îêú Í∏àÏï°Í≥º Ïã§Ï†ú Í≤∞Ï†ú Í∏àÏï°Ïù¥ **{discrepancy:,.0f}Ïõê** ÎßåÌÅº Ï∞®Ïù¥ÎÇ©ÎãàÎã§."
                godomall_warnings.append(warning_msg)

        # Í∏∞Ï°¥ Ï≤òÎ¶¨ Î°úÏßÅ Í≥ÑÏÜç...
        df_final = df_ecount_orig.copy().rename(columns={'Í∏àÏï°': 'Ïã§Í≤∞Ï†úÍ∏àÏï°'})
        
        # Ïä§ÎßàÌä∏Ïä§ÌÜ†Ïñ¥ Î≥ëÌï© Ï§ÄÎπÑ
        key_cols_smartstore = ['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú', 'Ï£ºÎ¨∏ÏàòÎüâ', 'ÏàòÎ†πÏûêÎ™Ö']
        smartstore_prices = df_smartstore.rename(columns={'Ïã§Í≤∞Ï†úÍ∏àÏï°': 'ÏàòÏ†ïÎê†_Í∏àÏï°_Ïä§ÌÜ†Ïñ¥'})[key_cols_smartstore + ['ÏàòÏ†ïÎê†_Í∏àÏï°_Ïä§ÌÜ†Ïñ¥']].drop_duplicates(subset=key_cols_smartstore, keep='first')
        
        # Í≥†ÎèÑÎ™∞ Î≥ëÌï©
        key_cols_godomall = ['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú', 'ÏàòÏ∑®Ïù∏ Ïù¥Î¶Ñ', 'ÏÉÅÌíàÏàòÎüâ']
        godomall_prices_for_merge = df_godomall[key_cols_godomall + ['ÏàòÏ†ïÎê†_Í∏àÏï°_Í≥†ÎèÑÎ™∞']].rename(
            columns={'ÏàòÏ∑®Ïù∏ Ïù¥Î¶Ñ': 'ÏàòÎ†πÏûêÎ™Ö', 'ÏÉÅÌíàÏàòÎüâ': 'Ï£ºÎ¨∏ÏàòÎüâ'}
        )
        godomall_prices_for_merge = godomall_prices_for_merge.drop_duplicates(
            subset=['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú', 'ÏàòÎ†πÏûêÎ™Ö', 'Ï£ºÎ¨∏ÏàòÎüâ'], keep='first'
        )
        
        # Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ ÌÜµÏùº
        for col in ['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú', 'ÏàòÎ†πÏûêÎ™Ö']:
            df_final[col] = df_final[col].astype(str).str.strip()
            smartstore_prices[col] = smartstore_prices[col].astype(str).str.strip()
            godomall_prices_for_merge[col] = godomall_prices_for_merge[col].astype(str).str.strip()
        
        for col in ['Ï£ºÎ¨∏ÏàòÎüâ']:
            df_final[col] = pd.to_numeric(df_final[col], errors='coerce').fillna(0).astype(int)
            smartstore_prices[col] = pd.to_numeric(smartstore_prices[col], errors='coerce').fillna(0).astype(int)
            godomall_prices_for_merge[col] = pd.to_numeric(godomall_prices_for_merge[col], errors='coerce').fillna(0).astype(int)

        df_final['Ïã§Í≤∞Ï†úÍ∏àÏï°'] = pd.to_numeric(df_final['Ïã§Í≤∞Ï†úÍ∏àÏï°'], errors='coerce').fillna(0).astype(int)
        
        # Îç∞Ïù¥ÌÑ∞ Î≥ëÌï©
        df_final = pd.merge(df_final, smartstore_prices, on=key_cols_smartstore, how='left')
        df_final = pd.merge(df_final, godomall_prices_for_merge, 
                            on=['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú', 'ÏàòÎ†πÏûêÎ™Ö', 'Ï£ºÎ¨∏ÏàòÎüâ'], 
                            how='left')

        # Í≤ΩÍ≥† Î©îÏãúÏßÄ ÏÉùÏÑ±
        warnings = [f"- [Í∏àÏï°Î≥¥Ï†ï Ïã§Ìå®] **{row['ÏáºÌïëÎ™∞']}** / {row['ÏàòÎ†πÏûêÎ™Ö']} / {row['SKUÏÉÅÌíàÎ™Ö']}" 
                   for _, row in df_final[(df_final['ÏáºÌïëÎ™∞'] == 'Ïä§ÎßàÌä∏Ïä§ÌÜ†Ïñ¥') & (df_final['ÏàòÏ†ïÎê†_Í∏àÏï°_Ïä§ÌÜ†Ïñ¥'].isna()) | 
                                          (df_final['ÏáºÌïëÎ™∞'] == 'Í≥†ÎèÑÎ™∞5') & (df_final['ÏàòÏ†ïÎê†_Í∏àÏï°_Í≥†ÎèÑÎ™∞'].isna())].iterrows()]
        warnings.extend(godomall_warnings)

        # ÏµúÏ¢Ö Í≤∞Ï†ú Í∏àÏï° ÏóÖÎç∞Ïù¥Ìä∏
        df_final['Ïã§Í≤∞Ï†úÍ∏àÏï°'] = np.where(df_final['ÏáºÌïëÎ™∞'] == 'Í≥†ÎèÑÎ™∞5', 
                                          df_final['ÏàòÏ†ïÎê†_Í∏àÏï°_Í≥†ÎèÑÎ™∞'].fillna(df_final['Ïã§Í≤∞Ï†úÍ∏àÏï°']), 
                                          df_final['Ïã§Í≤∞Ï†úÍ∏àÏï°'])
        df_final['Ïã§Í≤∞Ï†úÍ∏àÏï°'] = np.where(df_final['ÏáºÌïëÎ™∞'] == 'Ïä§ÎßàÌä∏Ïä§ÌÜ†Ïñ¥', 
                                          df_final['ÏàòÏ†ïÎê†_Í∏àÏï°_Ïä§ÌÜ†Ïñ¥'].fillna(df_final['Ïã§Í≤∞Ï†úÍ∏àÏï°']), 
                                          df_final['Ïã§Í≤∞Ï†úÍ∏àÏï°'])
        
        df_main_result = df_final[['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú', 'SKUÏÉÅÌíàÎ™Ö', 'Ï£ºÎ¨∏ÏàòÎüâ', 'Ïã§Í≤∞Ï†úÍ∏àÏï°', 'ÏáºÌïëÎ™∞', 'ÏàòÎ†πÏûêÎ™Ö', 'original_order']]
        
        # ÎèôÎ™ÖÏù¥Ïù∏ Í≤ΩÍ≥† Ï∂îÍ∞Ä
        homonym_warnings = []
        name_groups = df_main_result.groupby('ÏàòÎ†πÏûêÎ™Ö')['original_order'].apply(list)
        for name, orders in name_groups.items():
            if len(orders) > 1 and (max(orders) - min(orders) + 1) != len(orders):
                homonym_warnings.append(f"- [ÎèôÎ™ÖÏù¥Ïù∏ ÏùòÏã¨] **{name}** ÎãòÏùò Ï£ºÎ¨∏Ïù¥ Îñ®Ïñ¥Ï†∏ÏÑú ÏûÖÎ†•ÎêòÏóàÏäµÎãàÎã§.")
        warnings.extend(homonym_warnings)

        # ÏàòÎüâ ÏöîÏïΩ Î∞è Ìè¨Ïû• Î¶¨Ïä§Ìä∏ ÏÉùÏÑ±
        df_quantity_summary = df_main_result.groupby('SKUÏÉÅÌíàÎ™Ö', as_index=False)['Ï£ºÎ¨∏ÏàòÎüâ'].sum().rename(columns={'Ï£ºÎ¨∏ÏàòÎüâ': 'Í∞úÏàò'})
        df_packing_list = df_main_result.sort_values(by='original_order')[['SKUÏÉÅÌíàÎ™Ö', 'Ï£ºÎ¨∏ÏàòÎüâ', 'ÏàòÎ†πÏûêÎ™Ö', 'ÏáºÌïëÎ™∞']].copy()
        is_first_item = df_packing_list['ÏàòÎ†πÏûêÎ™Ö'] != df_packing_list['ÏàòÎ†πÏûêÎ™Ö'].shift(1)
        df_packing_list['Î¨∂ÏùåÎ≤àÌò∏'] = is_first_item.cumsum()
        df_packing_list_final = df_packing_list.copy()
        df_packing_list_final['Î¨∂ÏùåÎ≤àÌò∏'] = df_packing_list_final['Î¨∂ÏùåÎ≤àÌò∏'].where(is_first_item, '')
        df_packing_list_final = df_packing_list_final[['Î¨∂ÏùåÎ≤àÌò∏', 'SKUÏÉÅÌíàÎ™Ö', 'Ï£ºÎ¨∏ÏàòÎüâ', 'ÏàòÎ†πÏûêÎ™Ö', 'ÏáºÌïëÎ™∞']]

        # ÎßàÏä§ÌÑ∞ Îç∞Ïù¥ÌÑ∞ Î≥ëÌï©
        df_merged = pd.merge(df_main_result, df_master[['SKUÏΩîÎìú', 'Í≥ºÏÑ∏Ïó¨Î∂Ä', 'ÏûÖÏàòÎüâ']], 
                            left_on='Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú', right_on='SKUÏΩîÎìú', how='left')
        
        unmastered = df_merged[df_merged['SKUÏΩîÎìú'].isna()]
        for _, row in unmastered.iterrows():
            warnings.append(f"- [ÎØ∏Îì±Î°ù ÏÉÅÌíà] **{row['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú']}** / {row['SKUÏÉÅÌíàÎ™Ö']}")

        # Í±∞ÎûòÏ≤ò Îß§Ìïë
        client_map = {
            'Ïø†Ìå°': 'Ïø†Ìå° Ï£ºÏãùÌöåÏÇ¨', 
            'Í≥†ÎèÑÎ™∞5': 'Í≥†ÎûòÎØ∏ÏûêÏÇ¨Î™∞_ÌòÑÍ∏àÏòÅÏàòÏ¶ù(Í≥†ÎèÑÎ™∞)', 
            'Ïä§ÎßàÌä∏Ïä§ÌÜ†Ïñ¥': 'Ïä§ÌÜ†Ïñ¥Ìåú',
            'Î∞∞ÎØºÏÉÅÌöå': 'Ï£ºÏãùÌöåÏÇ¨ Ïö∞ÏïÑÌïúÌòïÏ†úÎì§(Î∞∞ÎØºÏÉÅÌöå)',
            'Ïù¥ÏßÄÏõ∞Î™∞': 'Ï£ºÏãùÌöåÏÇ¨ ÌòÑÎåÄÏù¥ÏßÄÏõ∞'
        }
        
        # Ïù¥Ïπ¥Ïö¥Ìä∏ ÏóÖÎ°úÎìúÏö© Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±
        df_ecount_upload = pd.DataFrame()
        
        df_ecount_upload['ÏùºÏûê'] = datetime.now().strftime("%Y%m%d")
        df_ecount_upload['Í±∞ÎûòÏ≤òÎ™Ö'] = df_merged['ÏáºÌïëÎ™∞'].map(client_map).fillna(df_merged['ÏáºÌïëÎ™∞'])
        df_ecount_upload['Ï∂úÌïòÏ∞ΩÍ≥†'] = 'Í≥†ÎûòÎØ∏'
        df_ecount_upload['Í±∞ÎûòÏú†Ìòï'] = np.where(df_merged['Í≥ºÏÑ∏Ïó¨Î∂Ä'] == 'Î©¥ÏÑ∏', 12, 11)
        df_ecount_upload['Ï†ÅÏöî_Ï†ÑÌëú'] = 'Ïò§Ï†Ñ/Ïò®ÎùºÏù∏'
        df_ecount_upload['ÌíàÎ™©ÏΩîÎìú'] = df_merged['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú']
        
        # ÏàòÎüâ Í≥ÑÏÇ∞
        is_box_order = df_merged['SKUÏÉÅÌíàÎ™Ö'].str.contains("BOX", na=False)
        ÏûÖÏàòÎüâ = pd.to_numeric(df_merged['ÏûÖÏàòÎüâ'], errors='coerce').fillna(1)
        base_quantity = np.where(is_box_order, df_merged['Ï£ºÎ¨∏ÏàòÎüâ'] * ÏûÖÏàòÎüâ, df_merged['Ï£ºÎ¨∏ÏàòÎüâ'])
        is_3_pack = df_merged['SKUÏÉÅÌíàÎ™Ö'].str.contains("3Í∞úÏûÖ|3Í∞ú", na=False)
        final_quantity = np.where(is_3_pack, base_quantity * 3, base_quantity)
        df_ecount_upload['Î∞ïÏä§'] = np.where(is_box_order, df_merged['Ï£ºÎ¨∏ÏàòÎüâ'], np.nan)
        df_ecount_upload['ÏàòÎüâ'] = final_quantity.astype(int)
        
        # Í∏àÏï° Í≥ÑÏÇ∞
        df_merged['Ïã§Í≤∞Ï†úÍ∏àÏï°'] = pd.to_numeric(df_merged['Ïã§Í≤∞Ï†úÍ∏àÏï°'], errors='coerce').fillna(0)
        Í≥µÍ∏âÍ∞ÄÏï° = np.where(df_merged['Í≥ºÏÑ∏Ïó¨Î∂Ä'] == 'Í≥ºÏÑ∏', df_merged['Ïã§Í≤∞Ï†úÍ∏àÏï°'] / 1.1, df_merged['Ïã§Í≤∞Ï†úÍ∏àÏï°'])
        df_ecount_upload['Í≥µÍ∏âÍ∞ÄÏï°'] = Í≥µÍ∏âÍ∞ÄÏï°
        df_ecount_upload['Î∂ÄÍ∞ÄÏÑ∏'] = df_merged['Ïã§Í≤∞Ï†úÍ∏àÏï°'] - df_ecount_upload['Í≥µÍ∏âÍ∞ÄÏï°']
        
        df_ecount_upload['ÏáºÌïëÎ™∞Í≥†Í∞ùÎ™Ö'] = df_merged['ÏàòÎ†πÏûêÎ™Ö']
        df_ecount_upload['original_order'] = df_merged['original_order']
        
        # Ïù¥Ïπ¥Ïö¥Ìä∏ Ïª¨Îüº Ï†ïÎ¶¨
        ecount_columns = [
            'ÏùºÏûê', 'ÏàúÎ≤à', 'Í±∞ÎûòÏ≤òÏΩîÎìú', 'Í±∞ÎûòÏ≤òÎ™Ö', 'Îã¥ÎãπÏûê', 'Ï∂úÌïòÏ∞ΩÍ≥†', 'Í±∞ÎûòÏú†Ìòï', 'ÌÜµÌôî', 'ÌôòÏú®', 
            'Ï†ÅÏöî_Ï†ÑÌëú', 'ÎØ∏ÏàòÍ∏à', 'Ï¥ùÌï©Í≥Ñ', 'Ïó∞Í≤∞Ï†ÑÌëú', 'ÌíàÎ™©ÏΩîÎìú', 'ÌíàÎ™©Î™Ö', 'Í∑úÍ≤©', 'Î∞ïÏä§', 'ÏàòÎüâ', 
            'Îã®Í∞Ä', 'Ïô∏ÌôîÍ∏àÏï°', 'Í≥µÍ∏âÍ∞ÄÏï°', 'Î∂ÄÍ∞ÄÏÑ∏', 'Ï†ÅÏöî_ÌíàÎ™©', 'ÏÉùÏÇ∞Ï†ÑÌëúÏÉùÏÑ±', 'ÏãúÎ¶¨Ïñº/Î°úÌä∏', 
            'Í¥ÄÎ¶¨Ìï≠Î™©', 'ÏáºÌïëÎ™∞Í≥†Í∞ùÎ™Ö', 'original_order'
        ]
        for col in ecount_columns:
            if col not in df_ecount_upload:
                df_ecount_upload[col] = ''
        
        for col in ['Í≥µÍ∏âÍ∞ÄÏï°', 'Î∂ÄÍ∞ÄÏÑ∏']:
            df_ecount_upload[col] = df_ecount_upload[col].round().astype('Int64')
        
        df_ecount_upload['Í±∞ÎûòÏú†Ìòï'] = pd.to_numeric(df_ecount_upload['Í±∞ÎûòÏú†Ìòï'])
        
        # Ï†ïÎ†¨
        sort_order = [
            'Í≥†ÎûòÎØ∏ÏûêÏÇ¨Î™∞_ÌòÑÍ∏àÏòÅÏàòÏ¶ù(Í≥†ÎèÑÎ™∞)', 
            'Ïä§ÌÜ†Ïñ¥Ìåú', 
            'Ïø†Ìå° Ï£ºÏãùÌöåÏÇ¨',
            'Ï£ºÏãùÌöåÏÇ¨ Ïö∞ÏïÑÌïúÌòïÏ†úÎì§(Î∞∞ÎØºÏÉÅÌöå)',
            'Ï£ºÏãùÌöåÏÇ¨ ÌòÑÎåÄÏù¥ÏßÄÏõ∞'
        ]
        
        df_ecount_upload['Í±∞ÎûòÏ≤òÎ™Ö_sort'] = pd.Categorical(df_ecount_upload['Í±∞ÎûòÏ≤òÎ™Ö'], categories=sort_order, ordered=True)
        
        df_ecount_upload = df_ecount_upload.sort_values(
            by=['Í±∞ÎûòÏ≤òÎ™Ö_sort', 'Í±∞ÎûòÏú†Ìòï', 'original_order'],
            ascending=[True, True, True]
        ).drop(columns=['Í±∞ÎûòÏ≤òÎ™Ö_sort', 'original_order'])
        
        df_ecount_upload = df_ecount_upload[ecount_columns[:-1]]

        # SharePoint Í∏∞Î°ùÏö© Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ
        df_for_record = df_merged.copy()
        df_for_record['Ï£ºÎ¨∏ÏùºÏûê'] = order_date
        df_for_record['Í±∞ÎûòÏ≤òÎ™Ö'] = df_for_record['ÏáºÌïëÎ™∞'].map(client_map).fillna(df_for_record['ÏáºÌïëÎ™∞'])
        df_for_record['Í≥µÍ∏âÍ∞ÄÏï°'] = Í≥µÍ∏âÍ∞ÄÏï°.round().astype('Int64')
        df_for_record['Î∂ÄÍ∞ÄÏÑ∏'] = (df_merged['Ïã§Í≤∞Ï†úÍ∏àÏï°'] - Í≥µÍ∏âÍ∞ÄÏï°).round().astype('Int64')
        df_for_record['Í±∞ÎûòÏú†Ìòï'] = np.where(df_merged['Í≥ºÏÑ∏Ïó¨Î∂Ä'] == 'Î©¥ÏÑ∏', 12, 11)
        df_for_record['ÌíàÎ™©ÏΩîÎìú'] = df_merged['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú']

        return (df_main_result.drop(columns=['original_order']), 
                df_quantity_summary, 
                df_packing_list_final, 
                df_ecount_upload, 
                df_for_record,
                True, 
                "Î™®Îì† ÌååÏùº Ï≤òÎ¶¨Í∞Ä ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§.", 
                warnings)

    except Exception as e:
        import traceback
        st.error(f"Ï≤òÎ¶¨ Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {e}")
        st.error(traceback.format_exc())
        return None, None, None, None, None, False, f"Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {e}", []

# --------------------------------------------------------------------------
# Streamlit Î©îÏù∏ Ïï±
# --------------------------------------------------------------------------
def main():
    # ÏÇ¨Ïù¥ÎìúÎ∞î ÏÑ§Ï†ï
    with st.sidebar:
        st.title("‚öôÔ∏è ÏÑ§Ï†ï")
        
        # ÏÇ¨Ïö©Ïûê Ïù¥Î¶Ñ ÏûÖÎ†•
        user_name = st.text_input("ÏÇ¨Ïö©Ïûê Ïù¥Î¶Ñ", value=st.session_state.get('user_name', ''))
        if user_name:
            st.session_state['user_name'] = user_name
        
        st.divider()
        
        # SharePoint Ïó∞Í≤∞ ÏÉÅÌÉú
        st.subheader("üì° Ïó∞Í≤∞ ÏÉÅÌÉú")
        ctx = get_sharepoint_context()
        if ctx:
            st.success("‚úÖ SharePoint Ïó∞Í≤∞Îê®")
        else:
            st.error("‚ùå SharePoint Ïó∞Í≤∞ Ïã§Ìå®")
        
        # AI ÏÉÅÌÉú
        if st.secrets.get("gemini", {}).get("api_key"):
            st.success("‚úÖ Gemini AI ÌôúÏÑ±Ìôî")
        else:
            st.warning("‚ö†Ô∏è AI Í∏∞Îä• ÎπÑÌôúÏÑ±Ìôî")
        
        st.divider()
        
        # Ï†ïÎ≥¥
        st.info("""
        **v2.0 ÏÉàÎ°úÏö¥ Í∏∞Îä•:**
        - SharePoint ÌÜµÌï©
        - AI Í∏∞Î∞ò Î∂ÑÏÑù
        - Ïã§ÏãúÍ∞Ñ ÎåÄÏãúÎ≥¥Îìú
        - ÏûêÎèô Îç∞Ïù¥ÌÑ∞ Í∏∞Î°ù
        
        **Ïó∞Í≤∞ Ï†ïÎ≥¥:**
        - Site: goremi.sharepoint.com
        - Îç∞Ïù¥ÌÑ∞: plto_master_data.xlsx
        - Í∏∞Î°ù: plto_record_data.xlsx
        """)
    
    # Î©îÏù∏ Ïª®ÌÖêÏ∏†
    st.title("üìë Ï£ºÎ¨∏ Ï≤òÎ¶¨ ÏûêÎèôÌôî ÏãúÏä§ÌÖú v2.0")
    st.caption("SharePoint & AI Powered | Ïã§ÏãúÍ∞Ñ Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù | ÏûêÎèô Í∏∞Î°ù ÏãúÏä§ÌÖú")
    
    # ÌÉ≠ Íµ¨ÏÑ±
    tab1, tab2, tab3 = st.tabs(["üì§ Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨", "üìä ÎåÄÏãúÎ≥¥Îìú", "üìà ÏÉÅÏÑ∏ Î∂ÑÏÑù"])
    
    with tab1:
        st.header("1. ÏõêÎ≥∏ ÏóëÏÖÄ ÌååÏùº 3Í∞ú ÏóÖÎ°úÎìú")
        col1, col2, col3 = st.columns(3)
        with col1:
            file1 = st.file_uploader("1Ô∏è‚É£ Ïä§ÎßàÌä∏Ïä§ÌÜ†Ïñ¥ (Í∏àÏï°ÌôïÏù∏Ïö©)", type=['xlsx', 'xls', 'csv'])
        with col2:
            file2 = st.file_uploader("2Ô∏è‚É£ Ïù¥Ïπ¥Ïö¥Ìä∏ Îã§Ïö¥Î°úÎìú (Ï£ºÎ¨∏Î™©Î°ù)", type=['xlsx', 'xls', 'csv'])
        with col3:
            file3 = st.file_uploader("3Ô∏è‚É£ Í≥†ÎèÑÎ™∞ (Í∏àÏï°ÌôïÏù∏Ïö©)", type=['xlsx', 'xls', 'csv'])

        st.divider()
        
        st.header("2. Ï≤òÎ¶¨ Í≤∞Í≥º ÌôïÏù∏ Î∞è Îã§Ïö¥Î°úÎìú")
        
        if st.button("üöÄ Î™®Îì† Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ Î∞è ÌååÏùº ÏÉùÏÑ± Ïã§Ìñâ", type="primary", use_container_width=True):
            if file1 and file2 and file3:
                # ÎßàÏä§ÌÑ∞ Îç∞Ïù¥ÌÑ∞ Î°úÎìú
                with st.spinner('SharePointÏóêÏÑú ÎßàÏä§ÌÑ∞ Îç∞Ïù¥ÌÑ∞Î•º Î∂àÎü¨Ïò§Îäî Ï§ë...'):
                    df_master = load_master_from_sharepoint()
                    
                if df_master is None:
                    st.error("üö® ÎßàÏä§ÌÑ∞ Îç∞Ïù¥ÌÑ∞Î•º Î∂àÎü¨Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§!")
                    return
                
                # ÌååÏùº Ï≤òÎ¶¨
                with st.spinner('Î™®Îì† ÌååÏùºÏùÑ ÏùΩÍ≥† Îç∞Ïù¥ÌÑ∞Î•º Ï≤òÎ¶¨ÌïòÎ©∞ ÏóëÏÖÄ ÏÑúÏãùÏùÑ Ï†ÅÏö© Ï§ëÏûÖÎãàÎã§...'):
                    result = process_all_files(file1, file2, file3, df_master)
                    
                if result[5]:  # success
                    df_main, df_qty, df_pack, df_ecount, df_for_record, success, message, warnings = result
                    
                    st.success(message)
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

                    # SharePointÏóê Í∏∞Î°ù Ï†ÄÏû•
                    with st.spinner('SharePointÏóê Ï≤òÎ¶¨ Í≤∞Í≥ºÎ•º Í∏∞Î°ù Ï§ë...'):
                        saved, record_count = save_record_to_sharepoint(df_for_record)
                        if saved:
                            st.success(f"‚úÖ SharePointÏóê {record_count}Í±¥Ïùò Ïã†Í∑ú Îç∞Ïù¥ÌÑ∞Í∞Ä Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§.")
                        else:
                            st.warning("‚ö†Ô∏è SharePoint Ï†ÄÏû• Ïã§Ìå® - Î°úÏª¨ Î∞±ÏóÖÏùÑ Í∂åÏû•Ìï©ÎãàÎã§.")

                    # Í≤ΩÍ≥† Î©îÏãúÏßÄ ÌëúÏãú
                    if warnings:
                        st.warning("‚ö†Ô∏è ÌôïÏù∏ ÌïÑÏöî Ìï≠Î™©")
                        with st.expander("ÏûêÏÑ∏Ìïú Î™©Î°ù Î≥¥Í∏∞..."):
                            for warning_message in warnings:
                                st.markdown(warning_message)
                    
                    # Í≤∞Í≥º ÌÉ≠
                    tab_erp, tab_pack, tab_qty, tab_main = st.tabs([
                        "üè¢ Ïù¥Ïπ¥Ïö¥Ìä∏ ÏóÖÎ°úÎìúÏö©", 
                        "üìã Ìè¨Ïû• Î¶¨Ïä§Ìä∏", 
                        "üì¶ Ï∂úÍ≥†ÏàòÎüâ ÏöîÏïΩ", 
                        "‚úÖ ÏµúÏ¢Ö Î≥¥Ï†ï Î¶¨Ïä§Ìä∏"
                    ])
                    
                    with tab_erp:
                        st.dataframe(df_ecount.astype(str), use_container_width=True)
                        st.download_button(
                            "üì• Îã§Ïö¥Î°úÎìú", 
                            to_excel_formatted(df_ecount, format_type='ecount_upload'), 
                            f"Ïù¥Ïπ¥Ïö¥Ìä∏_ÏóÖÎ°úÎìúÏö©_{timestamp}.xlsx"
                        )

                    with tab_pack:
                        st.dataframe(df_pack, use_container_width=True)
                        st.download_button(
                            "üì• Îã§Ïö¥Î°úÎìú", 
                            to_excel_formatted(df_pack, format_type='packing_list'), 
                            f"Î¨ºÎ•òÌåÄ_Ï†ÑÎã¨Ïö©_Ìè¨Ïû•Î¶¨Ïä§Ìä∏_{timestamp}.xlsx"
                        )

                    with tab_qty:
                        st.dataframe(df_qty, use_container_width=True)
                        st.download_button(
                            "üì• Îã§Ïö¥Î°úÎìú", 
                            to_excel_formatted(df_qty, format_type='quantity_summary'), 
                            f"Î¨ºÎ•òÌåÄ_Ï†ÑÎã¨Ïö©_Ï∂úÍ≥†ÏàòÎüâ_{timestamp}.xlsx"
                        )
                    
                    with tab_main:
                        st.dataframe(df_main, use_container_width=True)
                        st.download_button(
                            "üì• Îã§Ïö¥Î°úÎìú", 
                            to_excel_formatted(df_main), 
                            f"ÏµúÏ¢Ö_Ïã§Í≤∞Ï†úÍ∏àÏï°_Î≥¥Ï†ïÏôÑÎ£å_{timestamp}.xlsx"
                        )
                else:
                    st.error(result[6])  # error message
            else:
                st.warning("‚ö†Ô∏è 3Í∞úÏùò ÏóëÏÖÄ ÌååÏùºÏùÑ Î™®Îëê ÏóÖÎ°úÎìúÌï¥Ïïº Ïã§ÌñâÌï† Ïàò ÏûàÏäµÎãàÎã§.")
    
    with tab2:
        st.header("üìä Ïã§ÏãúÍ∞Ñ ÎåÄÏãúÎ≥¥Îìú")
        
        # Îç∞Ïù¥ÌÑ∞ Î°úÎìú
        with st.spinner("SharePointÏóêÏÑú Í∏∞Î°ù Îç∞Ïù¥ÌÑ∞Î•º Î∂àÎü¨Ïò§Îäî Ï§ë..."):
            df_record = load_record_data_from_sharepoint()
        
        if not df_record.empty:
            create_dashboard(df_record)
        else:
            st.info("üìä ÏïÑÏßÅ Í∏∞Î°ùÎêú Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§. Îç∞Ïù¥ÌÑ∞Î•º Ï≤òÎ¶¨ÌïòÎ©¥ ÏûêÎèôÏúºÎ°ú ÎåÄÏãúÎ≥¥ÎìúÍ∞Ä ÏÉùÏÑ±Îê©ÎãàÎã§.")
    
    with tab3:
        st.header("üìà ÏÉÅÏÑ∏ Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù")
        
        with st.spinner("SharePointÏóêÏÑú Í∏∞Î°ù Îç∞Ïù¥ÌÑ∞Î•º Î∂àÎü¨Ïò§Îäî Ï§ë..."):
            df_record = load_record_data_from_sharepoint()
        
        if not df_record.empty:
            # ÌïÑÌÑ∞ÎßÅ ÏòµÏÖò
            col1, col2, col3 = st.columns(3)
            
            with col1:
                date_range = st.date_input(
                    "ÎÇ†Ïßú Î≤îÏúÑ",
                    value=(datetime.now() - timedelta(days=30), datetime.now()),
                    format="YYYY-MM-DD"
                )
            
            with col2:
                selected_malls = st.multiselect(
                    "ÏáºÌïëÎ™∞ ÏÑ†ÌÉù",
                    options=df_record['ÏáºÌïëÎ™∞'].unique(),
                    default=df_record['ÏáºÌïëÎ™∞'].unique()
                )
            
            with col3:
                top_n = st.number_input("TOP N ÏÉÅÌíà", min_value=5, max_value=50, value=10)
            
            # ÌïÑÌÑ∞ÎßÅ Ï†ÅÏö©
            df_filtered = df_record[
                (pd.to_datetime(df_record['Ï£ºÎ¨∏ÏùºÏûê']).dt.date >= date_range[0]) &
                (pd.to_datetime(df_record['Ï£ºÎ¨∏ÏùºÏûê']).dt.date <= date_range[1]) &
                (df_record['ÏáºÌïëÎ™∞'].isin(selected_malls))
            ]
            
            if not df_filtered.empty:
                # ÏÉÅÏÑ∏ ÌÜµÍ≥Ñ
                st.subheader("üìä ÏÉÅÏÑ∏ ÌÜµÍ≥Ñ")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.metric("ÌïÑÌÑ∞ÎßÅÎêú Îß§Ï∂ú", f"‚Ç©{df_filtered['Ïã§Í≤∞Ï†úÍ∏àÏï°'].sum():,.0f}")
                    st.metric("ÌèâÍ∑† Ï£ºÎ¨∏ Í∏àÏï°", f"‚Ç©{df_filtered['Ïã§Í≤∞Ï†úÍ∏àÏï°'].mean():,.0f}")
                
                with col2:
                    st.metric("Ï¥ù Ï£ºÎ¨∏ Í±¥Ïàò", f"{len(df_filtered):,}")
                    st.metric("Í≥†Ïú† ÏÉÅÌíà Ïàò", f"{df_filtered['SKUÏÉÅÌíàÎ™Ö'].nunique():,}")
                
                # Îç∞Ïù¥ÌÑ∞ ÌÖåÏù¥Î∏î
                st.subheader("üìã ÏÉÅÏÑ∏ Îç∞Ïù¥ÌÑ∞")
                st.dataframe(
                    df_filtered.sort_values('Ï£ºÎ¨∏ÏùºÏûê', ascending=False),
                    use_container_width=True,
                    height=400
                )
                
                # Îç∞Ïù¥ÌÑ∞ Îã§Ïö¥Î°úÎìú
                csv = df_filtered.to_csv(index=False).encode('utf-8-sig')
                st.download_button(
                    "üì• CSV Îã§Ïö¥Î°úÎìú",
                    csv,
                    "filtered_data.csv",
                    "text/csv",
                    key='download-csv'
                )
            else:
                st.warning("ÏÑ†ÌÉùÌïú Ï°∞Í±¥Ïóê ÎßûÎäî Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.")
        else:
            st.info("üìä ÏïÑÏßÅ Í∏∞Î°ùÎêú Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.")

if __name__ == "__main__":
    main()
