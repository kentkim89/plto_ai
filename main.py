import streamlit as st
import pandas as pd
import io
import numpy as np
import openpyxl
from openpyxl.styles import PatternFill, Border, Side, Alignment
from datetime import datetime, timedelta
import plotly.express as px
import plotly.graph_objects as go
import hashlib
import json
import traceback
import requests
from io import BytesIO

# Office365 imports with fallback
SHAREPOINT_AVAILABLE = False
try:
    from office365.runtime.auth.client_credential import ClientCredential
    from office365.sharepoint.client_context import ClientContext
    from office365.sharepoint.files.file import File
    SHAREPOINT_AVAILABLE = True
except ImportError:
    pass

# Gemini AI import with fallback
GEMINI_AVAILABLE = False
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    pass

# --------------------------------------------------------------------------
# ÌéòÏù¥ÏßÄ ÏÑ§Ï†ï
# --------------------------------------------------------------------------
st.set_page_config(
    page_title="Ï£ºÎ¨∏ Ï≤òÎ¶¨ ÏûêÎèôÌôî Pro v2.0",
    layout="wide",
    page_icon="üìä",
    initial_sidebar_state="expanded"
)

# --------------------------------------------------------------------------
# SharePoint Ïó∞Í≤∞ Ìï®Ïàò
# --------------------------------------------------------------------------

@st.cache_resource
def init_sharepoint_context():
    """SharePoint Ïª®ÌÖçÏä§Ìä∏ Ï¥àÍ∏∞Ìôî"""
    if not SHAREPOINT_AVAILABLE:
        return None
    
    try:
        if "sharepoint" not in st.secrets:
            return None
            
        tenant_id = st.secrets["sharepoint"]["tenant_id"]
        client_id = st.secrets["sharepoint"]["client_id"]
        client_secret = st.secrets["sharepoint"]["client_secret"]
        site_url = "https://goremi.sharepoint.com/sites/data"
        
        credentials = ClientCredential(client_id, client_secret)
        ctx = ClientContext(site_url).with_credentials(credentials)
        return ctx
    except Exception as e:
        st.error(f"SharePoint Ïó∞Í≤∞ Ïã§Ìå®: {e}")
        return None

@st.cache_data(ttl=600)
def load_master_data_from_sharepoint():
    """SharePointÏóêÏÑú ÎßàÏä§ÌÑ∞ Îç∞Ïù¥ÌÑ∞ Î°úÎìú ÎòêÎäî ÏßÅÏ†ë URL Îã§Ïö¥Î°úÎìú"""
    try:
        # Î®ºÏ†Ä SharePoint API ÏãúÎèÑ
        if SHAREPOINT_AVAILABLE:
            ctx = init_sharepoint_context()
            if ctx and "sharepoint_files" in st.secrets:
                try:
                    file_url = st.secrets["sharepoint_files"]["plto_master_data_file_url"]
                    response = File.open_binary(ctx, file_url)
                    df_master = pd.read_excel(io.BytesIO(response.content))
                    df_master = df_master.drop_duplicates(subset=['SKUÏΩîÎìú'], keep='first')
                    return df_master
                except:
                    pass
        
        # API Ïã§Ìå® Ïãú ÏßÅÏ†ë Îã§Ïö¥Î°úÎìú ÏãúÎèÑ
        if "sharepoint_files" in st.secrets:
            file_url = st.secrets["sharepoint_files"]["plto_master_data_file_url"]
            
            # SharePoint Í≥µÏú† ÎßÅÌÅ¨ Î≥ÄÌôò
            if "sharepoint.com/:x:" in file_url:
                parts = file_url.split('/')
                share_id = parts[-1].split('?')[0]
                base_url = file_url.split('/:x:')[0]
                download_url = f"{base_url}/sites/data/_layouts/15/download.aspx?share={share_id}"
            else:
                download_url = file_url
            
            response = requests.get(download_url, timeout=30)
            if response.status_code == 200:
                df_master = pd.read_excel(io.BytesIO(response.content))
                df_master = df_master.drop_duplicates(subset=['SKUÏΩîÎìú'], keep='first')
                return df_master
                
    except Exception as e:
        st.warning(f"SharePoint Î°úÎìú Ïã§Ìå®: {e}")
    
    return pd.DataFrame()

def save_to_sharepoint_records(df_main_result, df_ecount_upload):
    """Ï≤òÎ¶¨ Í≤∞Í≥ºÎ•º SharePointÏùò plto_record_data.xlsxÏóê Ï†ÄÏû•"""
    try:
        if not SHAREPOINT_AVAILABLE:
            st.info("SharePoint API ÏóÜÏù¥Îäî Ï†ÄÏû•Ìï† Ïàò ÏóÜÏäµÎãàÎã§.")
            return False, "SharePoint Ï†ÄÏû• Î∂àÍ∞Ä"
            
        ctx = init_sharepoint_context()
        if not ctx:
            return False, "SharePoint Ïó∞Í≤∞ Ïã§Ìå®"
        
        if "plto_record_data_file_url" not in st.secrets.get("sharepoint_files", {}):
            st.warning("plto_record_data_file_urlÏù¥ ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")
            return False, "Î†àÏΩîÎìú ÌååÏùº URL ÎØ∏ÏÑ§Ï†ï"
        
        record_file_url = st.secrets["sharepoint_files"]["plto_record_data_file_url"]
        
        # Í∏∞Ï°¥ ÌååÏùº ÏùΩÍ∏∞
        existing_df = pd.DataFrame()
        try:
            response = File.open_binary(ctx, record_file_url)
            existing_df = pd.read_excel(io.BytesIO(response.content))
        except:
            pass  # ÌååÏùºÏù¥ ÏóÜÏúºÎ©¥ ÏÉàÎ°ú ÏÉùÏÑ±
        
        # ÏÉà Î†àÏΩîÎìú Ï§ÄÎπÑ
        new_records = pd.DataFrame()
        order_date = df_ecount_upload['ÏùºÏûê'].iloc[0] if not df_ecount_upload.empty else datetime.now().strftime("%Y%m%d")
        
        new_records['Ï£ºÎ¨∏ÏùºÏûê'] = order_date
        new_records['Ï≤òÎ¶¨ÏùºÏãú'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        new_records['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú'] = df_main_result['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú']
        new_records['SKUÏÉÅÌíàÎ™Ö'] = df_main_result['SKUÏÉÅÌíàÎ™Ö']
        new_records['Ï£ºÎ¨∏ÏàòÎüâ'] = df_main_result['Ï£ºÎ¨∏ÏàòÎüâ']
        new_records['Ïã§Í≤∞Ï†úÍ∏àÏï°'] = df_main_result['Ïã§Í≤∞Ï†úÍ∏àÏï°']
        new_records['ÏáºÌïëÎ™∞'] = df_main_result['ÏáºÌïëÎ™∞']
        new_records['ÏàòÎ†πÏûêÎ™Ö'] = df_main_result['ÏàòÎ†πÏûêÎ™Ö']
        
        # Ï§ëÎ≥µ Ï≤¥ÌÅ¨
        new_records['unique_hash'] = new_records.apply(
            lambda x: hashlib.md5(
                f"{x['Ï£ºÎ¨∏ÏùºÏûê']}_{x['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú']}_{x['ÏàòÎ†πÏûêÎ™Ö']}_{x['ÏáºÌïëÎ™∞']}".encode()
            ).hexdigest(), axis=1
        )
        
        # Í∏∞Ï°¥ Îç∞Ïù¥ÌÑ∞ÏôÄ Î≥ëÌï©
        if not existing_df.empty and 'unique_hash' in existing_df.columns:
            new_unique = new_records[~new_records['unique_hash'].isin(existing_df['unique_hash'])]
            combined_df = pd.concat([existing_df, new_unique], ignore_index=True)
            new_count = len(new_unique)
        else:
            combined_df = new_records
            new_count = len(new_records)
        
        # ExcelÎ°ú Ï†ÄÏû•
        output = BytesIO()
        combined_df.to_excel(output, index=False, sheet_name='Records')
        output.seek(0)
        
        # SharePointÏóê ÏóÖÎ°úÎìú
        target_folder = ctx.web.get_folder_by_server_relative_url("/sites/data/Shared Documents")
        target_folder.upload_file("plto_record_data.xlsx", output.read()).execute_query()
        
        return True, f"‚úÖ {new_count}Í∞ú Ïã†Í∑ú Î†àÏΩîÎìú Ï†ÄÏû• ÏôÑÎ£å"
        
    except Exception as e:
        return False, f"Ï†ÄÏû• Ïã§Ìå®: {e}"

def load_record_data_from_sharepoint():
    """SharePointÏóêÏÑú Í∏∞Î°ù Îç∞Ïù¥ÌÑ∞ Î°úÎìú"""
    try:
        if not SHAREPOINT_AVAILABLE:
            return pd.DataFrame()
            
        ctx = init_sharepoint_context()
        if not ctx:
            return pd.DataFrame()
        
        if "plto_record_data_file_url" not in st.secrets.get("sharepoint_files", {}):
            return pd.DataFrame()
            
        record_file_url = st.secrets["sharepoint_files"]["plto_record_data_file_url"]
        response = File.open_binary(ctx, record_file_url)
        df_records = pd.read_excel(io.BytesIO(response.content))
        
        if 'Ï£ºÎ¨∏ÏùºÏûê' in df_records.columns:
            df_records['Ï£ºÎ¨∏ÏùºÏûê'] = pd.to_datetime(df_records['Ï£ºÎ¨∏ÏùºÏûê'], format='%Y%m%d', errors='coerce')
        
        return df_records
    except:
        return pd.DataFrame()

# --------------------------------------------------------------------------
# AI Î∂ÑÏÑù Ìï®Ïàò
# --------------------------------------------------------------------------

@st.cache_resource
def init_gemini():
    """Gemini AI Ï¥àÍ∏∞Ìôî"""
    if not GEMINI_AVAILABLE:
        return None
    
    try:
        if "GEMINI_API_KEY" in st.secrets:
            genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
            return genai.GenerativeModel('gemini-pro')
    except Exception as e:
        st.warning(f"Gemini AI Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
    return None

def analyze_sales_with_ai(df_records):
    """AIÎ•º ÏÇ¨Ïö©Ìïú ÌåêÎß§ Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù"""
    if not GEMINI_AVAILABLE:
        return "AI Î∂ÑÏÑùÏù¥ ÎπÑÌôúÏÑ±ÌôîÎêòÏñ¥ ÏûàÏäµÎãàÎã§."
    
    try:
        model = init_gemini()
        if not model or df_records.empty:
            return None
        
        # Î∂ÑÏÑùÏùÑ ÏúÑÌïú Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ
        summary = {
            "Ï¥ù_Ï£ºÎ¨∏Ïàò": len(df_records),
            "Ï¥ù_Îß§Ï∂ú": float(df_records['Ïã§Í≤∞Ï†úÍ∏àÏï°'].sum()),
            "ÏÉÅÌíà_Ï¢ÖÎ•ò": int(df_records['SKUÏÉÅÌíàÎ™Ö'].nunique()),
            "Í≥†Í∞ùÏàò": int(df_records['ÏàòÎ†πÏûêÎ™Ö'].nunique()),
            "Í∏∞Í∞Ñ": f"{df_records['Ï£ºÎ¨∏ÏùºÏûê'].min().strftime('%Y-%m-%d')} ~ {df_records['Ï£ºÎ¨∏ÏùºÏûê'].max().strftime('%Y-%m-%d')}",
            "Î≤†Ïä§Ìä∏ÏÖÄÎü¨_TOP5": df_records.groupby('SKUÏÉÅÌíàÎ™Ö')['Ï£ºÎ¨∏ÏàòÎüâ'].sum().nlargest(5).to_dict(),
            "Ï±ÑÎÑêÎ≥Ñ_Îß§Ï∂ú": {k: float(v) for k, v in df_records.groupby('ÏáºÌïëÎ™∞')['Ïã§Í≤∞Ï†úÍ∏àÏï°'].sum().to_dict().items()},
            "ÏùºÌèâÍ∑†_Îß§Ï∂ú": float(df_records.groupby('Ï£ºÎ¨∏ÏùºÏûê')['Ïã§Í≤∞Ï†úÍ∏àÏï°'].sum().mean())
        }
        
        prompt = f"""
        Ïò®ÎùºÏù∏ ÏáºÌïëÎ™∞ ÌåêÎß§ Îç∞Ïù¥ÌÑ∞Î•º Î∂ÑÏÑùÌï¥Ï£ºÏÑ∏Ïöî:
        
        {json.dumps(summary, ensure_ascii=False, indent=2, default=str)}
        
        Îã§Ïùå ÎÇ¥Ïö©ÏùÑ Ìè¨Ìï®ÌïòÏó¨ Î∂ÑÏÑùÌï¥Ï£ºÏÑ∏Ïöî:
        1. üìà ÌåêÎß§ Ìä∏Î†åÎìú Î∂ÑÏÑù
        2. üèÜ Î≤†Ïä§Ìä∏ÏÖÄÎü¨ Ïù∏ÏÇ¨Ïù¥Ìä∏
        3. üõí Ï±ÑÎÑêÎ≥Ñ ÏÑ±Í≥º ÌèâÍ∞Ä
        4. üí° Ïã§Ìñâ Í∞ÄÎä•Ìïú Í∞úÏÑ† Ï†úÏïà
        5. ‚ö†Ô∏è Ï£ºÏùòÍ∞Ä ÌïÑÏöîÌïú Î∂ÄÎ∂Ñ
        
        Î∂ÑÏÑùÏùÄ Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏúºÎ°ú ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.
        """
        
        response = model.generate_content(prompt)
        return response.text
        
    except Exception as e:
        return f"AI Î∂ÑÏÑù Ïò§Î•ò: {e}"

# --------------------------------------------------------------------------
# Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ Ìï®Ïàò
# --------------------------------------------------------------------------

def to_excel_formatted(df, format_type=None):
    """Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏùÑ ÏÑúÏãùÏù¥ Ï†ÅÏö©Îêú ÏóëÏÖÄ ÌååÏùºÎ°ú Î≥ÄÌôò"""
    output = BytesIO()
    df_to_save = df.fillna('')
    
    if format_type == 'ecount_upload':
        df_to_save = df_to_save.rename(columns={'Ï†ÅÏöî_Ï†ÑÌëú': 'Ï†ÅÏöî', 'Ï†ÅÏöî_ÌíàÎ™©': 'Ï†ÅÏöî.1'})

    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df_to_save.to_excel(writer, index=False, sheet_name='Sheet1')
    
    output.seek(0)
    workbook = openpyxl.load_workbook(output)
    sheet = workbook.active

    # ÏÑúÏãù Ï†ÅÏö©
    center_alignment = Alignment(horizontal='center', vertical='center')
    for row in sheet.iter_rows():
        for cell in row:
            cell.alignment = center_alignment

    # Ïó¥ ÎÑàÎπÑ Ï°∞Ï†ï
    for column_cells in sheet.columns:
        max_length = 0
        column = column_cells[0].column_letter
        for cell in column_cells:
            try:
                if cell.value and len(str(cell.value)) > max_length:
                    max_length = len(str(cell.value))
            except:
                pass
        adjusted_width = min((max_length + 2) * 1.2, 50)
        sheet.column_dimensions[column].width = adjusted_width
    
    thin_border = Border(
        left=Side(style='thin'), right=Side(style='thin'),
        top=Side(style='thin'), bottom=Side(style='thin')
    )
    pink_fill = PatternFill(start_color="FFEBEE", end_color="FFEBEE", fill_type="solid")

    # ÌäπÎ≥Ñ ÏÑúÏãù
    if format_type == 'packing_list':
        for row in sheet.iter_rows(min_row=1, max_row=sheet.max_row):
            for cell in row:
                cell.border = thin_border
        
        bundle_start_row = 2
        for row_num in range(2, sheet.max_row + 2):
            current_bundle_cell = sheet.cell(row=row_num, column=1) if row_num <= sheet.max_row else None
            
            if (current_bundle_cell and current_bundle_cell.value) or (row_num > sheet.max_row):
                if row_num > 2:
                    bundle_end_row = row_num - 1
                    prev_bundle_num_str = str(sheet.cell(row=bundle_start_row, column=1).value)
                    
                    if prev_bundle_num_str.isdigit() and int(prev_bundle_num_str) % 2 != 0:
                        for r in range(bundle_start_row, bundle_end_row + 1):
                            for c in range(1, sheet.max_column + 1):
                                sheet.cell(row=r, column=c).fill = pink_fill
                    
                    if bundle_start_row < bundle_end_row:
                        sheet.merge_cells(start_row=bundle_start_row, start_column=1,
                                        end_row=bundle_end_row, end_column=1)
                        sheet.merge_cells(start_row=bundle_start_row, start_column=4,
                                        end_row=bundle_end_row, end_column=4)
                
                bundle_start_row = row_num

    elif format_type == 'quantity_summary':
        for row_idx, row in enumerate(sheet.iter_rows(min_row=1, max_row=sheet.max_row)):
            for cell in row:
                cell.border = thin_border
            if row_idx > 0 and row_idx % 2 != 0:
                for cell in row:
                    cell.fill = pink_fill
    
    final_output = BytesIO()
    workbook.save(final_output)
    final_output.seek(0)
    
    return final_output.getvalue()

def process_all_files(file1, file2, file3, df_master):
    """Î©îÏù∏ Ï≤òÎ¶¨ Ìï®Ïàò"""
    try:
        # ÌååÏùº ÏùΩÍ∏∞
        df_smartstore = pd.read_excel(file1)
        df_ecount_orig = pd.read_excel(file2)
        df_godomall = pd.read_excel(file3)

        df_ecount_orig['original_order'] = range(len(df_ecount_orig))
        
        # Ïª¨ÎüºÎ™Ö Ìò∏ÌôòÏÑ± Ï≤òÎ¶¨
        if 'Ìöå Ìï†Ïù∏ Í∏àÏï°' in df_godomall.columns:
            df_godomall.rename(columns={'Ìöå Ìï†Ïù∏ Í∏àÏï°': 'ÌöåÏõê Ìï†Ïù∏ Í∏àÏï°'}, inplace=True)
        if 'ÏûêÏ≤¥ÏòµÏÖòÏΩîÎìú' in df_godomall.columns:
            df_godomall.rename(columns={'ÏûêÏ≤¥ÏòµÏÖòÏΩîÎìú': 'Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú'}, inplace=True)
        
        # Îç∞Ïù¥ÌÑ∞ ÌÅ¥Î¶¨Îãù
        cols_to_numeric = ['ÏÉÅÌíàÎ≥Ñ ÌíàÎ™©Í∏àÏï°', 'Ï¥ù Î∞∞ÏÜ° Í∏àÏï°', 'ÌöåÏõê Ìï†Ïù∏ Í∏àÏï°',
                          'Ïø†Ìè∞ Ìï†Ïù∏ Í∏àÏï°', 'ÏÇ¨Ïö©Îêú ÎßàÏùºÎ¶¨ÏßÄ', 'Ï¥ù Í≤∞Ï†ú Í∏àÏï°']
        for col in cols_to_numeric:
            if col in df_godomall.columns:
                df_godomall[col] = pd.to_numeric(
                    df_godomall[col].astype(str).str.replace('[Ïõê,]', '', regex=True),
                    errors='coerce'
                ).fillna(0)
        
        # Î∞∞ÏÜ°ÎπÑ Ï§ëÎ≥µ Î∞©ÏßÄ
        df_godomall['Î≥¥Ï†ïÎêú_Î∞∞ÏÜ°ÎπÑ'] = np.where(
            df_godomall.duplicated(subset=['ÏàòÏ∑®Ïù∏ Ïù¥Î¶Ñ']),
            0,
            df_godomall['Ï¥ù Î∞∞ÏÜ° Í∏àÏï°']
        )
        
        df_godomall['ÏàòÏ†ïÎê†_Í∏àÏï°_Í≥†ÎèÑÎ™∞'] = (
            df_godomall['ÏÉÅÌíàÎ≥Ñ ÌíàÎ™©Í∏àÏï°'] + df_godomall['Î≥¥Ï†ïÎêú_Î∞∞ÏÜ°ÎπÑ'] -
            df_godomall['ÌöåÏõê Ìï†Ïù∏ Í∏àÏï°'] - df_godomall['Ïø†Ìè∞ Ìï†Ïù∏ Í∏àÏï°'] -
            df_godomall['ÏÇ¨Ïö©Îêú ÎßàÏùºÎ¶¨ÏßÄ']
        )
        
        # Í≤ΩÍ≥† ÏàòÏßë
        warnings = []
        
        # Í≥†ÎèÑÎ™∞ Í∏àÏï° Í≤ÄÏ¶ù
        for name, group in df_godomall.groupby('ÏàòÏ∑®Ïù∏ Ïù¥Î¶Ñ'):
            calculated = group['ÏàòÏ†ïÎê†_Í∏àÏï°_Í≥†ÎèÑÎ™∞'].sum()
            actual = group['Ï¥ù Í≤∞Ï†ú Í∏àÏï°'].iloc[0]
            diff = calculated - actual
            if abs(diff) > 1:
                warnings.append(f"- [Í∏àÏï° Î∂àÏùºÏπò] **{name}**Îãò: {diff:,.0f}Ïõê Ï∞®Ïù¥")

        # Î©îÏù∏ Ï≤òÎ¶¨
        df_final = df_ecount_orig.copy().rename(columns={'Í∏àÏï°': 'Ïã§Í≤∞Ï†úÍ∏àÏï°'})
        
        # Î≥ëÌï© Ï§ÄÎπÑ
        key_cols = ['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú', 'Ï£ºÎ¨∏ÏàòÎüâ', 'ÏàòÎ†πÏûêÎ™Ö']
        smartstore_prices = df_smartstore.rename(
            columns={'Ïã§Í≤∞Ï†úÍ∏àÏï°': 'ÏàòÏ†ïÎê†_Í∏àÏï°_Ïä§ÌÜ†Ïñ¥'}
        )[key_cols + ['ÏàòÏ†ïÎê†_Í∏àÏï°_Ïä§ÌÜ†Ïñ¥']].drop_duplicates(subset=key_cols, keep='first')
        
        godomall_prices = df_godomall.rename(
            columns={'ÏàòÏ∑®Ïù∏ Ïù¥Î¶Ñ': 'ÏàòÎ†πÏûêÎ™Ö', 'ÏÉÅÌíàÏàòÎüâ': 'Ï£ºÎ¨∏ÏàòÎüâ'}
        )[['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú', 'ÏàòÎ†πÏûêÎ™Ö', 'Ï£ºÎ¨∏ÏàòÎüâ', 'ÏàòÏ†ïÎê†_Í∏àÏï°_Í≥†ÎèÑÎ™∞']].drop_duplicates(
            subset=['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú', 'ÏàòÎ†πÏûêÎ™Ö', 'Ï£ºÎ¨∏ÏàòÎüâ'], keep='first'
        )
        
        # Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ ÌÜµÏùº
        for col in ['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú', 'ÏàòÎ†πÏûêÎ™Ö']:
            df_final[col] = df_final[col].astype(str).str.strip()
            smartstore_prices[col] = smartstore_prices[col].astype(str).str.strip()
            godomall_prices[col] = godomall_prices[col].astype(str).str.strip()
        
        for col in ['Ï£ºÎ¨∏ÏàòÎüâ']:
            df_final[col] = pd.to_numeric(df_final[col], errors='coerce').fillna(0).astype(int)
            smartstore_prices[col] = pd.to_numeric(smartstore_prices[col], errors='coerce').fillna(0).astype(int)
            godomall_prices[col] = pd.to_numeric(godomall_prices[col], errors='coerce').fillna(0).astype(int)
        
        df_final['Ïã§Í≤∞Ï†úÍ∏àÏï°'] = pd.to_numeric(df_final['Ïã§Í≤∞Ï†úÍ∏àÏï°'], errors='coerce').fillna(0).astype(int)
        
        # Î≥ëÌï©
        df_final = pd.merge(df_final, smartstore_prices, on=key_cols, how='left')
        df_final = pd.merge(df_final, godomall_prices, on=key_cols, how='left')

        # Í∏àÏï° ÏóÖÎç∞Ïù¥Ìä∏
        df_final['Ïã§Í≤∞Ï†úÍ∏àÏï°'] = np.where(
            df_final['ÏáºÌïëÎ™∞'] == 'Í≥†ÎèÑÎ™∞5',
            df_final['ÏàòÏ†ïÎê†_Í∏àÏï°_Í≥†ÎèÑÎ™∞'].fillna(df_final['Ïã§Í≤∞Ï†úÍ∏àÏï°']),
            df_final['Ïã§Í≤∞Ï†úÍ∏àÏï°']
        )
        df_final['Ïã§Í≤∞Ï†úÍ∏àÏï°'] = np.where(
            df_final['ÏáºÌïëÎ™∞'] == 'Ïä§ÎßàÌä∏Ïä§ÌÜ†Ïñ¥',
            df_final['ÏàòÏ†ïÎê†_Í∏àÏï°_Ïä§ÌÜ†Ïñ¥'].fillna(df_final['Ïã§Í≤∞Ï†úÍ∏àÏï°']),
            df_final['Ïã§Í≤∞Ï†úÍ∏àÏï°']
        )
        
        # Í≤∞Í≥º ÏÉùÏÑ±
        df_main_result = df_final[[
            'Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú', 'SKUÏÉÅÌíàÎ™Ö', 'Ï£ºÎ¨∏ÏàòÎüâ', 'Ïã§Í≤∞Ï†úÍ∏àÏï°',
            'ÏáºÌïëÎ™∞', 'ÏàòÎ†πÏûêÎ™Ö', 'original_order'
        ]]
        
        # ÎèôÎ™ÖÏù¥Ïù∏ Ï≤¥ÌÅ¨
        name_groups = df_main_result.groupby('ÏàòÎ†πÏûêÎ™Ö')['original_order'].apply(list)
        for name, orders in name_groups.items():
            if len(orders) > 1 and (max(orders) - min(orders) + 1) != len(orders):
                warnings.append(f"- [ÎèôÎ™ÖÏù¥Ïù∏ ÏùòÏã¨] **{name}**ÎãòÏùò Ï£ºÎ¨∏Ïù¥ Îñ®Ïñ¥Ï†∏ ÏûàÏäµÎãàÎã§.")
        
        # ÏöîÏïΩ ÏÉùÏÑ±
        df_quantity_summary = df_main_result.groupby('SKUÏÉÅÌíàÎ™Ö', as_index=False)['Ï£ºÎ¨∏ÏàòÎüâ'].sum()
        df_quantity_summary.columns = ['SKUÏÉÅÌíàÎ™Ö', 'Í∞úÏàò']
        
        # Ìè¨Ïû• Î¶¨Ïä§Ìä∏
        df_packing = df_main_result.sort_values('original_order')[[
            'SKUÏÉÅÌíàÎ™Ö', 'Ï£ºÎ¨∏ÏàòÎüâ', 'ÏàòÎ†πÏûêÎ™Ö', 'ÏáºÌïëÎ™∞'
        ]].copy()
        
        is_first = df_packing['ÏàòÎ†πÏûêÎ™Ö'] != df_packing['ÏàòÎ†πÏûêÎ™Ö'].shift(1)
        df_packing['Î¨∂ÏùåÎ≤àÌò∏'] = is_first.cumsum()
        df_packing_list = df_packing.copy()
        df_packing_list['Î¨∂ÏùåÎ≤àÌò∏'] = df_packing_list['Î¨∂ÏùåÎ≤àÌò∏'].where(is_first, '')
        df_packing_list = df_packing_list[[
            'Î¨∂ÏùåÎ≤àÌò∏', 'SKUÏÉÅÌíàÎ™Ö', 'Ï£ºÎ¨∏ÏàòÎüâ', 'ÏàòÎ†πÏûêÎ™Ö', 'ÏáºÌïëÎ™∞'
        ]]

        # Ïù¥Ïπ¥Ïö¥Ìä∏ Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±
        df_merged = pd.merge(
            df_main_result,
            df_master[['SKUÏΩîÎìú', 'Í≥ºÏÑ∏Ïó¨Î∂Ä', 'ÏûÖÏàòÎüâ']],
            left_on='Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú',
            right_on='SKUÏΩîÎìú',
            how='left'
        )
        
        # ÎØ∏Îì±Î°ù ÏÉÅÌíà Ï≤¥ÌÅ¨
        for _, row in df_merged[df_merged['SKUÏΩîÎìú'].isna()].iterrows():
            warnings.append(f"- [ÎØ∏Îì±Î°ù] {row['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú']}: {row['SKUÏÉÅÌíàÎ™Ö']}")

        # Í±∞ÎûòÏ≤ò Îß§Ìïë
        client_map = {
            'Ïø†Ìå°': 'Ïø†Ìå° Ï£ºÏãùÌöåÏÇ¨',
            'Í≥†ÎèÑÎ™∞5': 'Í≥†ÎûòÎØ∏ÏûêÏÇ¨Î™∞_ÌòÑÍ∏àÏòÅÏàòÏ¶ù(Í≥†ÎèÑÎ™∞)',
            'Ïä§ÎßàÌä∏Ïä§ÌÜ†Ïñ¥': 'Ïä§ÌÜ†Ïñ¥Ìåú',
            'Î∞∞ÎØºÏÉÅÌöå': 'Ï£ºÏãùÌöåÏÇ¨ Ïö∞ÏïÑÌïúÌòïÏ†úÎì§(Î∞∞ÎØºÏÉÅÌöå)',
            'Ïù¥ÏßÄÏõ∞Î™∞': 'Ï£ºÏãùÌöåÏÇ¨ ÌòÑÎåÄÏù¥ÏßÄÏõ∞'
        }
        
        # Ïù¥Ïπ¥Ïö¥Ìä∏ ÏóÖÎ°úÎìú Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±
        df_ecount = pd.DataFrame()
        df_ecount['ÏùºÏûê'] = datetime.now().strftime("%Y%m%d")
        df_ecount['Í±∞ÎûòÏ≤òÎ™Ö'] = df_merged['ÏáºÌïëÎ™∞'].map(client_map).fillna(df_merged['ÏáºÌïëÎ™∞'])
        df_ecount['Ï∂úÌïòÏ∞ΩÍ≥†'] = 'Í≥†ÎûòÎØ∏'
        df_ecount['Í±∞ÎûòÏú†Ìòï'] = np.where(df_merged['Í≥ºÏÑ∏Ïó¨Î∂Ä'] == 'Î©¥ÏÑ∏', 12, 11)
        df_ecount['Ï†ÅÏöî_Ï†ÑÌëú'] = 'Ïò§Ï†Ñ/Ïò®ÎùºÏù∏'
        df_ecount['ÌíàÎ™©ÏΩîÎìú'] = df_merged['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú']
        
        # ÏàòÎüâ Í≥ÑÏÇ∞
        is_box = df_merged['SKUÏÉÅÌíàÎ™Ö'].str.contains("BOX", na=False)
        ÏûÖÏàòÎüâ = pd.to_numeric(df_merged['ÏûÖÏàòÎüâ'], errors='coerce').fillna(1)
        base_qty = np.where(is_box, df_merged['Ï£ºÎ¨∏ÏàòÎüâ'] * ÏûÖÏàòÎüâ, df_merged['Ï£ºÎ¨∏ÏàòÎüâ'])
        is_3pack = df_merged['SKUÏÉÅÌíàÎ™Ö'].str.contains("3Í∞úÏûÖ|3Í∞ú", na=False)
        final_qty = np.where(is_3pack, base_qty * 3, base_qty)
        
        df_ecount['Î∞ïÏä§'] = np.where(is_box, df_merged['Ï£ºÎ¨∏ÏàòÎüâ'], np.nan)
        df_ecount['ÏàòÎüâ'] = final_qty.astype(int)
        
        # Í∏àÏï° Í≥ÑÏÇ∞
        df_merged['Ïã§Í≤∞Ï†úÍ∏àÏï°'] = pd.to_numeric(df_merged['Ïã§Í≤∞Ï†úÍ∏àÏï°'], errors='coerce').fillna(0)
        Í≥µÍ∏âÍ∞ÄÏï° = np.where(
            df_merged['Í≥ºÏÑ∏Ïó¨Î∂Ä'] == 'Í≥ºÏÑ∏',
            df_merged['Ïã§Í≤∞Ï†úÍ∏àÏï°'] / 1.1,
            df_merged['Ïã§Í≤∞Ï†úÍ∏àÏï°']
        )
        df_ecount['Í≥µÍ∏âÍ∞ÄÏï°'] = Í≥µÍ∏âÍ∞ÄÏï°
        df_ecount['Î∂ÄÍ∞ÄÏÑ∏'] = df_merged['Ïã§Í≤∞Ï†úÍ∏àÏï°'] - df_ecount['Í≥µÍ∏âÍ∞ÄÏï°']
        
        df_ecount['ÏáºÌïëÎ™∞Í≥†Í∞ùÎ™Ö'] = df_merged['ÏàòÎ†πÏûêÎ™Ö']
        df_ecount['original_order'] = df_merged['original_order']
        
        # Ïª¨Îüº Ï†ïÎ¶¨
        ecount_columns = [
            'ÏùºÏûê', 'ÏàúÎ≤à', 'Í±∞ÎûòÏ≤òÏΩîÎìú', 'Í±∞ÎûòÏ≤òÎ™Ö', 'Îã¥ÎãπÏûê', 'Ï∂úÌïòÏ∞ΩÍ≥†',
            'Í±∞ÎûòÏú†Ìòï', 'ÌÜµÌôî', 'ÌôòÏú®', 'Ï†ÅÏöî_Ï†ÑÌëú', 'ÎØ∏ÏàòÍ∏à', 'Ï¥ùÌï©Í≥Ñ',
            'Ïó∞Í≤∞Ï†ÑÌëú', 'ÌíàÎ™©ÏΩîÎìú', 'ÌíàÎ™©Î™Ö', 'Í∑úÍ≤©', 'Î∞ïÏä§', 'ÏàòÎüâ',
            'Îã®Í∞Ä', 'Ïô∏ÌôîÍ∏àÏï°', 'Í≥µÍ∏âÍ∞ÄÏï°', 'Î∂ÄÍ∞ÄÏÑ∏', 'Ï†ÅÏöî_ÌíàÎ™©',
            'ÏÉùÏÇ∞Ï†ÑÌëúÏÉùÏÑ±', 'ÏãúÎ¶¨Ïñº/Î°úÌä∏', 'Í¥ÄÎ¶¨Ìï≠Î™©', 'ÏáºÌïëÎ™∞Í≥†Í∞ùÎ™Ö'
        ]
        
        for col in ecount_columns:
            if col not in df_ecount:
                df_ecount[col] = ''
        
        df_ecount['Í≥µÍ∏âÍ∞ÄÏï°'] = df_ecount['Í≥µÍ∏âÍ∞ÄÏï°'].round().astype('Int64')
        df_ecount['Î∂ÄÍ∞ÄÏÑ∏'] = df_ecount['Î∂ÄÍ∞ÄÏÑ∏'].round().astype('Int64')
        df_ecount['Í±∞ÎûòÏú†Ìòï'] = pd.to_numeric(df_ecount['Í±∞ÎûòÏú†Ìòï'])
        
        # Ï†ïÎ†¨
        sort_order = [
            'Í≥†ÎûòÎØ∏ÏûêÏÇ¨Î™∞_ÌòÑÍ∏àÏòÅÏàòÏ¶ù(Í≥†ÎèÑÎ™∞)',
            'Ïä§ÌÜ†Ïñ¥Ìåú',
            'Ïø†Ìå° Ï£ºÏãùÌöåÏÇ¨',
            'Ï£ºÏãùÌöåÏÇ¨ Ïö∞ÏïÑÌïúÌòïÏ†úÎì§(Î∞∞ÎØºÏÉÅÌöå)',
            'Ï£ºÏãùÌöåÏÇ¨ ÌòÑÎåÄÏù¥ÏßÄÏõ∞'
        ]
        
        df_ecount['Í±∞ÎûòÏ≤òÎ™Ö_sort'] = pd.Categorical(
            df_ecount['Í±∞ÎûòÏ≤òÎ™Ö'],
            categories=sort_order,
            ordered=True
        )
        
        df_ecount = df_ecount.sort_values(
            by=['Í±∞ÎûòÏ≤òÎ™Ö_sort', 'Í±∞ÎûòÏú†Ìòï', 'original_order']
        ).drop(columns=['Í±∞ÎûòÏ≤òÎ™Ö_sort', 'original_order'])
        
        df_ecount_upload = df_ecount[ecount_columns]

        return (
            df_main_result.drop(columns=['original_order']),
            df_quantity_summary,
            df_packing_list,
            df_ecount_upload,
            True,
            "‚úÖ Î™®Îì† Ï≤òÎ¶¨Í∞Ä ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§!",
            warnings
        )

    except Exception as e:
        return None, None, None, None, False, f"‚ùå Ïò§Î•ò: {str(e)}", []

def create_analytics_dashboard(df_records):
    """Î∂ÑÏÑù ÎåÄÏãúÎ≥¥Îìú ÏÉùÏÑ±"""
    if df_records.empty:
        st.warning("Î∂ÑÏÑùÌï† Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.")
        return
    
    # Í∏∞Î≥∏ Î©îÌä∏Î¶≠
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        total_revenue = df_records['Ïã§Í≤∞Ï†úÍ∏àÏï°'].sum()
        st.metric("üí∞ Ï¥ù Îß§Ï∂ú", f"‚Ç©{total_revenue:,.0f}")
    
    with col2:
        total_orders = len(df_records)
        st.metric("üì¶ Ï¥ù Ï£ºÎ¨∏Ïàò", f"{total_orders:,}")
    
    with col3:
        avg_order = total_revenue / total_orders if total_orders > 0 else 0
        st.metric("üíµ ÌèâÍ∑† Ï£ºÎ¨∏Ïï°", f"‚Ç©{avg_order:,.0f}")
    
    with col4:
        unique_customers = df_records['ÏàòÎ†πÏûêÎ™Ö'].nunique()
        st.metric("üë• Í≥†Í∞ùÏàò", f"{unique_customers:,}")
    
    # Ï∞®Ìä∏
    tab1, tab2, tab3, tab4 = st.tabs(["üìà ÏùºÎ≥Ñ Ìä∏Î†åÎìú", "üèÜ Î≤†Ïä§Ìä∏ÏÖÄÎü¨", "üõí Ï±ÑÎÑê Î∂ÑÏÑù", "ü§ñ AI Ïù∏ÏÇ¨Ïù¥Ìä∏"])
    
    with tab1:
        # ÏùºÎ≥Ñ Îß§Ï∂ú Ìä∏Î†åÎìú
        daily_sales = df_records.groupby('Ï£ºÎ¨∏ÏùºÏûê').agg({
            'Ïã§Í≤∞Ï†úÍ∏àÏï°': 'sum',
            'Ï£ºÎ¨∏ÏàòÎüâ': 'sum',
            'ÏàòÎ†πÏûêÎ™Ö': 'nunique'
        }).reset_index()
        daily_sales.columns = ['ÎÇ†Ïßú', 'Îß§Ï∂úÏï°', 'ÌåêÎß§ÏàòÎüâ', 'Í≥†Í∞ùÏàò']
        
        fig = go.Figure()
        fig.add_trace(go.Scatter(
            x=daily_sales['ÎÇ†Ïßú'],
            y=daily_sales['Îß§Ï∂úÏï°'],
            mode='lines+markers',
            name='Îß§Ï∂úÏï°',
            line=dict(color='#1f77b4', width=2),
            marker=dict(size=8)
        ))
        fig.update_layout(
            title="ÏùºÎ≥Ñ Îß§Ï∂ú Ìä∏Î†åÎìú",
            xaxis_title="ÎÇ†Ïßú",
            yaxis_title="Îß§Ï∂úÏï° (Ïõê)",
            hovermode='x unified',
            height=400
        )
        st.plotly_chart(fig, use_container_width=True)
        
        # ÏöîÏùºÎ≥Ñ Î∂ÑÏÑù
        daily_sales['ÏöîÏùº'] = pd.to_datetime(daily_sales['ÎÇ†Ïßú']).dt.day_name()
        weekday_sales = daily_sales.groupby('ÏöîÏùº')['Îß§Ï∂úÏï°'].mean().reindex([
            'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'
        ])
        fig2 = px.bar(weekday_sales, title="ÏöîÏùºÎ≥Ñ ÌèâÍ∑† Îß§Ï∂ú")
        st.plotly_chart(fig2, use_container_width=True)
    
    with tab2:
        # Î≤†Ïä§Ìä∏ÏÖÄÎü¨ TOP 10
        top_products = df_records.groupby('SKUÏÉÅÌíàÎ™Ö').agg({
            'Ï£ºÎ¨∏ÏàòÎüâ': 'sum',
            'Ïã§Í≤∞Ï†úÍ∏àÏï°': 'sum'
        }).nlargest(10, 'Ï£ºÎ¨∏ÏàòÎüâ').reset_index()
        
        fig = px.bar(
            top_products,
            x='Ï£ºÎ¨∏ÏàòÎüâ',
            y='SKUÏÉÅÌíàÎ™Ö',
            orientation='h',
            title="ÏÉÅÌíàÎ≥Ñ ÌåêÎß§ ÏàòÎüâ TOP 10",
            color='Ïã§Í≤∞Ï†úÍ∏àÏï°',
            color_continuous_scale='Blues',
            labels={'Ï£ºÎ¨∏ÏàòÎüâ': 'ÌåêÎß§ ÏàòÎüâ', 'Ïã§Í≤∞Ï†úÍ∏àÏï°': 'Îß§Ï∂úÏï°'}
        )
        st.plotly_chart(fig, use_container_width=True)
        
        # ÏÉÅÌíàÎ≥Ñ ÏÉÅÏÑ∏
        st.dataframe(top_products, use_container_width=True)
    
    with tab3:
        # Ï±ÑÎÑêÎ≥Ñ Î∂ÑÏÑù
        channel_stats = df_records.groupby('ÏáºÌïëÎ™∞').agg({
            'Ïã§Í≤∞Ï†úÍ∏àÏï°': 'sum',
            'Ï£ºÎ¨∏ÏàòÎüâ': 'sum',
            'ÏàòÎ†πÏûêÎ™Ö': 'nunique'
        }).reset_index()
        channel_stats.columns = ['ÏáºÌïëÎ™∞', 'Îß§Ï∂úÏï°', 'ÌåêÎß§ÏàòÎüâ', 'Í≥†Í∞ùÏàò']
        
        fig = px.pie(
            channel_stats,
            values='Îß§Ï∂úÏï°',
            names='ÏáºÌïëÎ™∞',
            title="Ï±ÑÎÑêÎ≥Ñ Îß§Ï∂ú ÎπÑÏ§ë"
        )
        st.plotly_chart(fig, use_container_width=True)
        
        # Ï±ÑÎÑêÎ≥Ñ ÏÑ±Í≥º ÏßÄÌëú
        st.dataframe(channel_stats, use_container_width=True)
    
    with tab4:
        if GEMINI_AVAILABLE:
            with st.spinner("ü§ñ AIÍ∞Ä Îç∞Ïù¥ÌÑ∞Î•º Î∂ÑÏÑù Ï§ëÏûÖÎãàÎã§..."):
                ai_insights = analyze_sales_with_ai(df_records)
                if ai_insights:
                    st.markdown("### ü§ñ AI ÌåêÎß§ Î∂ÑÏÑù Î¶¨Ìè¨Ìä∏")
                    st.markdown(ai_insights)
                else:
                    st.info("AI Î∂ÑÏÑùÏùÑ ÏÉùÏÑ±Ìï† Ïàò ÏóÜÏäµÎãàÎã§.")
        else:
            st.warning("AI Î∂ÑÏÑù Í∏∞Îä•ÏùÑ ÏÇ¨Ïö©ÌïòÎ†§Î©¥ google-generativeaiÎ•º ÏÑ§ÏπòÌïòÏÑ∏Ïöî.")
            st.code("pip install google-generativeai")

# --------------------------------------------------------------------------
# Î©îÏù∏ Ïï±
# --------------------------------------------------------------------------

# ÏÇ¨Ïù¥ÎìúÎ∞î
with st.sidebar:
    st.title("üìä Order Pro v2.0")
    st.markdown("---")
    
    menu = st.radio(
        "Î©îÎâ¥ ÏÑ†ÌÉù",
        ["üìë Ï£ºÎ¨∏ Ï≤òÎ¶¨", "üìà ÌåêÎß§ Î∂ÑÏÑù", "‚öôÔ∏è ÏÑ§Ï†ï"],
        index=0
    )
    
    st.markdown("---")
    st.caption("üìå ÏãúÏä§ÌÖú ÏÉÅÌÉú")
    
    # SharePoint ÏÉÅÌÉú
    if SHAREPOINT_AVAILABLE:
        if init_sharepoint_context():
            st.success("‚úÖ SharePoint Ïó∞Í≤∞")
        else:
            st.warning("‚ö†Ô∏è SharePoint ÏÑ§Ï†ï ÌïÑÏöî")
    else:
        st.info("üíæ Î°úÏª¨ Î™®Îìú")
    
    # AI ÏÉÅÌÉú
    if GEMINI_AVAILABLE:
        st.success("‚úÖ AI ÌôúÏÑ±Ìôî")
    else:
        st.info("ü§ñ AI ÎπÑÌôúÏÑ±Ìôî")
    
    # Ï∫êÏãú Ï¥àÍ∏∞Ìôî
    if st.button("üîÑ Ï∫êÏãú Ï¥àÍ∏∞Ìôî"):
        st.cache_data.clear()
        st.cache_resource.clear()
        st.success("Ï∫êÏãú Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
        st.rerun()

# Î©îÏù∏ ÏΩòÌÖêÏ∏†
if menu == "üìë Ï£ºÎ¨∏ Ï≤òÎ¶¨":
    st.title("üìë Ï£ºÎ¨∏ Ï≤òÎ¶¨ ÏûêÎèôÌôî")
    st.info("üí° SharePoint Ïó∞Îèô Î∞è ÏûêÎèô Ï†ÄÏû• Í∏∞Îä•Ïù¥ ÌôúÏÑ±ÌôîÎêòÏñ¥ ÏûàÏäµÎãàÎã§.")
    
    # ÎßàÏä§ÌÑ∞ Îç∞Ïù¥ÌÑ∞ ÏÑπÏÖò
    with st.expander("üìä ÎßàÏä§ÌÑ∞ Îç∞Ïù¥ÌÑ∞ ÏÉÅÌÉú", expanded=True):
        df_master = load_master_data_from_sharepoint()
        
        if not df_master.empty:
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Ï¥ù SKU", f"{len(df_master):,}Í∞ú")
            with col2:
                st.metric("Í≥ºÏÑ∏ ÏÉÅÌíà", f"{(df_master['Í≥ºÏÑ∏Ïó¨Î∂Ä']=='Í≥ºÏÑ∏').sum():,}Í∞ú")
            with col3:
                st.metric("Î©¥ÏÑ∏ ÏÉÅÌíà", f"{(df_master['Í≥ºÏÑ∏Ïó¨Î∂Ä']=='Î©¥ÏÑ∏').sum():,}Í∞ú")
        else:
            st.warning("ÎßàÏä§ÌÑ∞ Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§. ÏàòÎèô ÏóÖÎ°úÎìúÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.")
            uploaded_master = st.file_uploader("ÎßàÏä§ÌÑ∞ Îç∞Ïù¥ÌÑ∞ ÏóÖÎ°úÎìú", type=['xlsx', 'xls', 'csv'])
            if uploaded_master:
                try:
                    if uploaded_master.name.endswith('.csv'):
                        df_master = pd.read_csv(uploaded_master)
                    else:
                        df_master = pd.read_excel(uploaded_master)
                    df_master = df_master.drop_duplicates(subset=['SKUÏΩîÎìú'], keep='first')
                    st.success(f"‚úÖ {len(df_master)}Í∞ú SKU Î°úÎìú ÏôÑÎ£å")
                except Exception as e:
                    st.error(f"ÌååÏùº ÏùΩÍ∏∞ Ïã§Ìå®: {e}")
    
    st.markdown("---")
    
    # ÌååÏùº ÏóÖÎ°úÎìú
    st.header("1Ô∏è‚É£ ÏõêÎ≥∏ ÌååÏùº ÏóÖÎ°úÎìú")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        file1 = st.file_uploader("Ïä§ÎßàÌä∏Ïä§ÌÜ†Ïñ¥", type=['xlsx', 'xls'])
    with col2:
        file2 = st.file_uploader("Ïù¥Ïπ¥Ïö¥Ìä∏", type=['xlsx', 'xls'])
    with col3:
        file3 = st.file_uploader("Í≥†ÎèÑÎ™∞", type=['xlsx', 'xls'])
    
    # Ï≤òÎ¶¨ Ïã§Ìñâ
    st.header("2Ô∏è‚É£ Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨")
    
    if st.button("üöÄ Ï≤òÎ¶¨ ÏãúÏûë", type="primary", disabled=not(file1 and file2 and file3)):
        if file1 and file2 and file3:
            if not df_master.empty:
                with st.spinner('Ï≤òÎ¶¨ Ï§ë...'):
                    result = process_all_files(file1, file2, file3, df_master)
                    df_main, df_qty, df_pack, df_ecount, success, message, warnings = result
                
                if success:
                    st.balloons()
                    st.success(message)
                    
                    # SharePoint Ï†ÄÏû•
                    if SHAREPOINT_AVAILABLE:
                        with st.spinner('SharePointÏóê Í∏∞Î°ù Ï†ÄÏû• Ï§ë...'):
                            save_success, save_msg = save_to_sharepoint_records(df_main, df_ecount)
                            if save_success:
                                st.success(save_msg)
                            else:
                                st.warning(save_msg)
                    
                    # Í≤ΩÍ≥† ÌëúÏãú
                    if warnings:
                        with st.expander(f"‚ö†Ô∏è ÌôïÏù∏ ÌïÑÏöî ({len(warnings)}Í±¥)"):
                            for w in warnings:
                                st.markdown(w)
                    
                    # ÏÑ∏ÏÖò Ï†ÄÏû•
                    st.session_state['last_result'] = df_main
                    
                    # Í≤∞Í≥º Îã§Ïö¥Î°úÎìú
                    st.markdown("---")
                    st.header("3Ô∏è‚É£ Í≤∞Í≥º Îã§Ïö¥Î°úÎìú")
                    
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    
                    tabs = st.tabs(["üè¢ Ïù¥Ïπ¥Ïö¥Ìä∏", "üìã Ìè¨Ïû•Î¶¨Ïä§Ìä∏", "üì¶ ÏàòÎüâÏöîÏïΩ", "‚úÖ ÏµúÏ¢ÖÍ≤∞Í≥º"])
                    
                    with tabs[0]:
                        st.dataframe(df_ecount.head(20), use_container_width=True)
                        st.download_button(
                            "üì• Ïù¥Ïπ¥Ïö¥Ìä∏ ÏóÖÎ°úÎìúÏö© Îã§Ïö¥Î°úÎìú",
                            to_excel_formatted(df_ecount, 'ecount_upload'),
                            f"Ïù¥Ïπ¥Ïö¥Ìä∏_{timestamp}.xlsx",
                            mime="application/vnd.ms-excel"
                        )
                    
                    with tabs[1]:
                        st.dataframe(df_pack.head(20), use_container_width=True)
                        st.download_button(
                            "üì• Ìè¨Ïû•Î¶¨Ïä§Ìä∏ Îã§Ïö¥Î°úÎìú",
                            to_excel_formatted(df_pack, 'packing_list'),
                            f"Ìè¨Ïû•Î¶¨Ïä§Ìä∏_{timestamp}.xlsx",
                            mime="application/vnd.ms-excel"
                        )
                    
                    with tabs[2]:
                        st.dataframe(df_qty, use_container_width=True)
                        st.download_button(
                            "üì• ÏàòÎüâÏöîÏïΩ Îã§Ïö¥Î°úÎìú",
                            to_excel_formatted(df_qty, 'quantity_summary'),
                            f"ÏàòÎüâÏöîÏïΩ_{timestamp}.xlsx",
                            mime="application/vnd.ms-excel"
                        )
                    
                    with tabs[3]:
                        st.dataframe(df_main.head(20), use_container_width=True)
                        st.download_button(
                            "üì• ÏµúÏ¢ÖÍ≤∞Í≥º Îã§Ïö¥Î°úÎìú",
                            to_excel_formatted(df_main),
                            f"ÏµúÏ¢ÖÍ≤∞Í≥º_{timestamp}.xlsx",
                            mime="application/vnd.ms-excel"
                        )
                else:
                    st.error(message)
            else:
                st.error("ÎßàÏä§ÌÑ∞ Îç∞Ïù¥ÌÑ∞Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§!")
        else:
            st.warning("3Í∞ú ÌååÏùºÏùÑ Î™®Îëê ÏóÖÎ°úÎìúÌï¥Ï£ºÏÑ∏Ïöî!")

elif menu == "üìà ÌåêÎß§ Î∂ÑÏÑù":
    st.title("üìà AI Í∏∞Î∞ò ÌåêÎß§ Î∂ÑÏÑù")
    
    # Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§ ÏÑ†ÌÉù
    data_source = st.radio(
        "Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§",
        ["SharePoint Í∏∞Î°ù", "ÏµúÍ∑º Ï≤òÎ¶¨ Í≤∞Í≥º"],
        horizontal=True
    )
    
    if data_source == "SharePoint Í∏∞Î°ù":
        # Í∏∞Í∞Ñ ÏÑ†ÌÉù
        col1, col2 = st.columns(2)
        with col1:
            period = st.selectbox(
                "Î∂ÑÏÑù Í∏∞Í∞Ñ",
                ["ÏµúÍ∑º 7Ïùº", "ÏµúÍ∑º 30Ïùº", "ÏµúÍ∑º 90Ïùº", "Ï†ÑÏ≤¥", "ÏÇ¨Ïö©Ïûê ÏßÄÏ†ï"]
            )
        
        with col2:
            if period == "ÏÇ¨Ïö©Ïûê ÏßÄÏ†ï":
                date_range = st.date_input(
                    "ÎÇ†Ïßú Î≤îÏúÑ",
                    value=(datetime.now() - timedelta(days=30), datetime.now())
                )
        
        if st.button("üìä Î∂ÑÏÑù ÏãúÏûë", type="primary"):
            with st.spinner("Îç∞Ïù¥ÌÑ∞ Î°úÎìú Ï§ë..."):
                df_records = load_record_data_from_sharepoint()
                
                if not df_records.empty:
                    # Í∏∞Í∞Ñ ÌïÑÌÑ∞ÎßÅ
                    if period != "Ï†ÑÏ≤¥":
                        today = pd.Timestamp.now()
                        if period == "ÏµúÍ∑º 7Ïùº":
                            start_date = today - timedelta(days=7)
                        elif period == "ÏµúÍ∑º 30Ïùº":
                            start_date = today - timedelta(days=30)
                        elif period == "ÏµúÍ∑º 90Ïùº":
                            start_date = today - timedelta(days=90)
                        elif period == "ÏÇ¨Ïö©Ïûê ÏßÄÏ†ï":
                            start_date = pd.Timestamp(date_range[0])
                            today = pd.Timestamp(date_range[1])
                        
                        df_records = df_records[
                            (df_records['Ï£ºÎ¨∏ÏùºÏûê'] >= start_date) &
                            (df_records['Ï£ºÎ¨∏ÏùºÏûê'] <= today)
                        ]
                    
                    if not df_records.empty:
                        create_analytics_dashboard(df_records)
                    else:
                        st.warning("ÏÑ†ÌÉùÌïú Í∏∞Í∞ÑÏóê Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.")
                else:
                    st.warning("SharePointÏóêÏÑú Îç∞Ïù¥ÌÑ∞Î•º Î∂àÎü¨Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§.")
    
    else:  # ÏµúÍ∑º Ï≤òÎ¶¨ Í≤∞Í≥º
        if 'last_result' in st.session_state:
            df_records = st.session_state['last_result'].copy()
            df_records['Ï£ºÎ¨∏ÏùºÏûê'] = datetime.now()
            create_analytics_dashboard(df_records)
        else:
            st.info("Î®ºÏ†Ä Ï£ºÎ¨∏ Ï≤òÎ¶¨Î•º Ïã§ÌñâÌï¥Ï£ºÏÑ∏Ïöî.")

elif menu == "‚öôÔ∏è ÏÑ§Ï†ï":
    st.title("‚öôÔ∏è ÏãúÏä§ÌÖú ÏÑ§Ï†ï")
    
    # SharePoint ÏÑ§Ï†ï
    st.header("üìÅ SharePoint ÏÑ§Ï†ï")
    
    if "sharepoint" in st.secrets:
        col1, col2 = st.columns(2)
        with col1:
            st.text_input("Tenant ID", value=st.secrets["sharepoint"]["tenant_id"][:20]+"...", disabled=True)
            st.text_input("Client ID", value=st.secrets["sharepoint"]["client_id"][:20]+"...", disabled=True)
        with col2:
            st.text_input("Site Name", value=st.secrets["sharepoint_files"]["site_name"], disabled=True)
            st.text_input("File Name", value=st.secrets["sharepoint_files"]["file_name"], disabled=True)
        
        if st.button("üîÑ SharePoint Ïó∞Í≤∞ ÌÖåÏä§Ìä∏"):
            with st.spinner("ÌÖåÏä§Ìä∏ Ï§ë..."):
                ctx = init_sharepoint_context()
                if ctx:
                    st.success("‚úÖ SharePoint Ïó∞Í≤∞ ÏÑ±Í≥µ!")
                else:
                    st.error("‚ùå SharePoint Ïó∞Í≤∞ Ïã§Ìå®")
    else:
        st.warning("SharePoint ÏÑ§Ï†ïÏù¥ ÏóÜÏäµÎãàÎã§.")
        st.code("""
# secrets.toml ÏòàÏãú
[sharepoint]
tenant_id = "your-tenant-id"
client_id = "your-client-id"
client_secret = "your-secret"

[sharepoint_files]
plto_master_data_file_url = "sharepoint-file-url"
plto_record_data_file_url = "record-file-url"
site_name = "data"
file_name = "plto_master_data.xlsx"
        """)
    
    # AI ÏÑ§Ï†ï
    st.header("ü§ñ AI ÏÑ§Ï†ï")
    
    if "GEMINI_API_KEY" in st.secrets:
        st.text_input("Gemini API Key", value=st.secrets["GEMINI_API_KEY"][:10]+"...", disabled=True)
        
        if st.button("üîÑ AI Ïó∞Í≤∞ ÌÖåÏä§Ìä∏"):
            with st.spinner("ÌÖåÏä§Ìä∏ Ï§ë..."):
                model = init_gemini()
                if model:
                    st.success("‚úÖ Gemini AI Ïó∞Í≤∞ ÏÑ±Í≥µ!")
                else:
                    st.error("‚ùå AI Ïó∞Í≤∞ Ïã§Ìå®")
    else:
        st.warning("Gemini API ÌÇ§Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")
    
    # ÏãúÏä§ÌÖú Ï†ïÎ≥¥
    st.header("‚ÑπÔ∏è ÏãúÏä§ÌÖú Ï†ïÎ≥¥")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("SharePoint", "ÌôúÏÑ±Ìôî" if SHAREPOINT_AVAILABLE else "ÎπÑÌôúÏÑ±Ìôî")
    with col2:
        st.metric("AI Î∂ÑÏÑù", "ÌôúÏÑ±Ìôî" if GEMINI_AVAILABLE else "ÎπÑÌôúÏÑ±Ìôî")
    with col3:
        st.metric("Î≤ÑÏ†Ñ", "v2.0")
