import streamlit as st
import pandas as pd
import io
import numpy as np
import openpyxl
from openpyxl.styles import PatternFill, Border, Side, Alignment
from openpyxl.utils import get_column_letter
from datetime import datetime, timedelta
import plotly.express as px
import plotly.graph_objects as go
from typing import Optional, Tuple, List, Dict
import hashlib
import json

# SharePoint ê´€ë ¨ importsëŠ” try-exceptë¡œ ì²˜ë¦¬
try:
    from office365.runtime.auth.client_credential import ClientCredential
    from office365.sharepoint.client_context import ClientContext
    from office365.sharepoint.files.file import File
    SHAREPOINT_AVAILABLE = True
except ImportError:
    SHAREPOINT_AVAILABLE = False
    st.warning("SharePoint ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¡œì»¬ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

# Gemini AI importë„ optionalë¡œ ì²˜ë¦¬
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

# --------------------------------------------------------------------------
# SharePoint ì—°ê²° ì„¤ì • (Optional)
# --------------------------------------------------------------------------

@st.cache_resource
def init_sharepoint_context():
    """SharePoint ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”"""
    if not SHAREPOINT_AVAILABLE:
        return None
    
    try:
        # secrets ì²´í¬
        if "sharepoint" not in st.secrets:
            return None
            
        tenant_id = st.secrets["sharepoint"]["tenant_id"]
        client_id = st.secrets["sharepoint"]["client_id"]
        client_secret = st.secrets["sharepoint"]["client_secret"]
        site_url = "https://goremi.sharepoint.com/sites/data"
        
        credentials = ClientCredential(client_id, client_secret)
        ctx = ClientContext(site_url).with_credentials(credentials)
        return ctx
    except Exception as e:
        st.warning(f"SharePoint ì—°ê²° ì‹¤íŒ¨: {e}")
        return None

@st.cache_data(ttl=600)
def load_master_data_from_sharepoint():
    """SharePointì—ì„œ ë§ˆìŠ¤í„° ë°ì´í„° ë¡œë“œ ë˜ëŠ” ë¡œì»¬ íŒŒì¼ ì‚¬ìš©"""
    if SHAREPOINT_AVAILABLE:
        try:
            ctx = init_sharepoint_context()
            if ctx and "sharepoint_files" in st.secrets:
                file_url = st.secrets["sharepoint_files"]["plto_master_data_file_url"]
                response = File.open_binary(ctx, file_url)
                df_master = pd.read_excel(io.BytesIO(response.content))
                df_master = df_master.drop_duplicates(subset=['SKUì½”ë“œ'], keep='first')
                return df_master
        except Exception as e:
            st.info(f"SharePoint ì ‘ì† ì‹¤íŒ¨, ë¡œì»¬ íŒŒì¼ ì‚¬ìš©: {e}")
    
    # ë¡œì»¬ íŒŒì¼ ë¡œë“œ
    return load_local_master_data("master_data.csv")

def load_local_master_data(file_path="master_data.csv"):
    """ë¡œì»¬ ë°±ì—… ë§ˆìŠ¤í„° ë°ì´í„° ë¡œë“œ"""
    try:
        df_master = pd.read_csv(file_path)
        df_master = df_master.drop_duplicates(subset=['SKUì½”ë“œ'], keep='first')
        return df_master
    except Exception as e:
        st.error(f"ë§ˆìŠ¤í„° ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return pd.DataFrame()

def save_to_sharepoint_records(df_main_result, df_ecount_upload):
    """ì²˜ë¦¬ ê²°ê³¼ë¥¼ SharePointì— ì €ì¥ (Optional)"""
    if not SHAREPOINT_AVAILABLE:
        return False, "SharePoint ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
    
    try:
        ctx = init_sharepoint_context()
        if not ctx:
            return False, "SharePoint ì—°ê²° ì‹¤íŒ¨"
        
        if "sharepoint_files" not in st.secrets or "plto_record_data_file_url" not in st.secrets["sharepoint_files"]:
            return False, "ë ˆì½”ë“œ íŒŒì¼ URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        record_file_url = st.secrets["sharepoint_files"]["plto_record_data_file_url"]
        
        # ê¸°ì¡´ ë ˆì½”ë“œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œë„
        try:
            response = File.open_binary(ctx, record_file_url)
            existing_df = pd.read_excel(io.BytesIO(response.content))
        except:
            existing_df = pd.DataFrame()
        
        # ìƒˆ ë°ì´í„° ì¤€ë¹„
        new_records = pd.DataFrame()
        order_date = df_ecount_upload['ì¼ì'].iloc[0] if not df_ecount_upload.empty else datetime.now().strftime("%Y%m%d")
        
        new_records['ì£¼ë¬¸ì¼ì'] = order_date
        new_records['ì²˜ë¦¬ì¼ì‹œ'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        new_records['ì¬ê³ ê´€ë¦¬ì½”ë“œ'] = df_main_result['ì¬ê³ ê´€ë¦¬ì½”ë“œ']
        new_records['SKUìƒí’ˆëª…'] = df_main_result['SKUìƒí’ˆëª…']
        new_records['ì£¼ë¬¸ìˆ˜ëŸ‰'] = df_main_result['ì£¼ë¬¸ìˆ˜ëŸ‰']
        new_records['ì‹¤ê²°ì œê¸ˆì•¡'] = df_main_result['ì‹¤ê²°ì œê¸ˆì•¡']
        new_records['ì‡¼í•‘ëª°'] = df_main_result['ì‡¼í•‘ëª°']
        new_records['ìˆ˜ë ¹ìëª…'] = df_main_result['ìˆ˜ë ¹ìëª…']
        
        # ì¤‘ë³µ ì²´í¬ìš© í•´ì‹œ ìƒì„±
        new_records['unique_hash'] = new_records.apply(
            lambda x: hashlib.md5(
                f"{x['ì£¼ë¬¸ì¼ì']}_{x['ì¬ê³ ê´€ë¦¬ì½”ë“œ']}_{x['ìˆ˜ë ¹ìëª…']}_{x['ì‡¼í•‘ëª°']}".encode()
            ).hexdigest(), axis=1
        )
        
        # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©
        if not existing_df.empty and 'unique_hash' in existing_df.columns:
            new_unique_records = new_records[~new_records['unique_hash'].isin(existing_df['unique_hash'])]
            combined_df = pd.concat([existing_df, new_unique_records], ignore_index=True)
        else:
            combined_df = new_records
        
        # Excel íŒŒì¼ë¡œ ì €ì¥
        output = io.BytesIO()
        combined_df.to_excel(output, index=False, sheet_name='Records')
        output.seek(0)
        
        # SharePointì— ì—…ë¡œë“œ
        target_folder = ctx.web.get_folder_by_server_relative_url("/sites/data/Shared Documents")
        target_folder.upload_file("plto_record_data.xlsx", output.read()).execute_query()
        
        return True, f"ì„±ê³µì ìœ¼ë¡œ {len(new_records)}ê°œì˜ ë ˆì½”ë“œë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤."
        
    except Exception as e:
        return False, f"SharePoint ì €ì¥ ì‹¤íŒ¨: {e}"

# --------------------------------------------------------------------------
# AI ë¶„ì„ ê¸°ëŠ¥ (Optional)
# --------------------------------------------------------------------------

@st.cache_resource
def init_gemini():
    """Gemini AI ì´ˆê¸°í™”"""
    if not GEMINI_AVAILABLE:
        return None
    
    try:
        if "GEMINI_API_KEY" in st.secrets:
            genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
            model = genai.GenerativeModel('gemini-pro')
            return model
    except Exception as e:
        st.warning(f"Gemini AI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    return None

def analyze_sales_data_with_ai(df_records):
    """AIë¥¼ ì‚¬ìš©í•œ íŒë§¤ ë°ì´í„° ë¶„ì„"""
    if not GEMINI_AVAILABLE:
        return "AI ë¶„ì„ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
    
    try:
        model = init_gemini()
        if not model or df_records.empty:
            return None
        
        # ë°ì´í„° ìš”ì•½ ì¤€ë¹„
        summary = {
            "total_orders": len(df_records),
            "total_revenue": float(df_records['ì‹¤ê²°ì œê¸ˆì•¡'].sum()),
            "unique_products": int(df_records['SKUìƒí’ˆëª…'].nunique()),
            "unique_customers": int(df_records['ìˆ˜ë ¹ìëª…'].nunique()),
            "date_range": f"{df_records['ì£¼ë¬¸ì¼ì'].min()} ~ {df_records['ì£¼ë¬¸ì¼ì'].max()}",
            "top_products": df_records.groupby('SKUìƒí’ˆëª…')['ì£¼ë¬¸ìˆ˜ëŸ‰'].sum().nlargest(5).to_dict(),
            "channel_distribution": {k: float(v) for k, v in df_records.groupby('ì‡¼í•‘ëª°')['ì‹¤ê²°ì œê¸ˆì•¡'].sum().to_dict().items()}
        }
        
        prompt = f"""
        ë‹¤ìŒ ì˜¨ë¼ì¸ ì‡¼í•‘ëª° íŒë§¤ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
        
        {json.dumps(summary, ensure_ascii=False, indent=2, default=str)}
        
        ë‹¤ìŒ í•­ëª©ë“¤ì„ í¬í•¨í•´ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”:
        1. ì „ì²´ì ì¸ íŒë§¤ íŠ¸ë Œë“œ
        2. ë² ìŠ¤íŠ¸ì…€ëŸ¬ ìƒí’ˆ ë¶„ì„
        3. ì±„ë„ë³„ íŒë§¤ ì„±ê³¼
        4. ê°œì„  ì œì•ˆì‚¬í•­
        
        ê°„ê²°í•˜ê³  ì‹¤ìš©ì ì¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
        """
        
        response = model.generate_content(prompt)
        return response.text
        
    except Exception as e:
        return f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

def load_record_data_from_sharepoint():
    """SharePointì—ì„œ ê¸°ë¡ ë°ì´í„° ë¡œë“œ"""
    if not SHAREPOINT_AVAILABLE:
        return pd.DataFrame()
    
    try:
        ctx = init_sharepoint_context()
        if not ctx or "sharepoint_files" not in st.secrets:
            return pd.DataFrame()
        
        if "plto_record_data_file_url" not in st.secrets["sharepoint_files"]:
            return pd.DataFrame()
            
        record_file_url = st.secrets["sharepoint_files"]["plto_record_data_file_url"]
        response = File.open_binary(ctx, record_file_url)
        df_records = pd.read_excel(io.BytesIO(response.content))
        
        if 'ì£¼ë¬¸ì¼ì' in df_records.columns:
            df_records['ì£¼ë¬¸ì¼ì'] = pd.to_datetime(df_records['ì£¼ë¬¸ì¼ì'], format='%Y%m%d', errors='coerce')
        
        return df_records
    except:
        return pd.DataFrame()

def create_analytics_dashboard(df_records):
    """ë¶„ì„ ëŒ€ì‹œë³´ë“œ ìƒì„±"""
    if df_records.empty:
        st.warning("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë‚ ì§œë³„ ì§‘ê³„
    df_daily = df_records.groupby('ì£¼ë¬¸ì¼ì').agg({
        'ì‹¤ê²°ì œê¸ˆì•¡': 'sum',
        'ì£¼ë¬¸ìˆ˜ëŸ‰': 'sum',
        'ìˆ˜ë ¹ìëª…': 'nunique'
    }).reset_index()
    df_daily.columns = ['ë‚ ì§œ', 'ë§¤ì¶œì•¡', 'íŒë§¤ìˆ˜ëŸ‰', 'ê³ ê°ìˆ˜']
    
    # ìƒí’ˆë³„ íŒë§¤ TOP 10
    df_product_top = df_records.groupby('SKUìƒí’ˆëª…').agg({
        'ì£¼ë¬¸ìˆ˜ëŸ‰': 'sum',
        'ì‹¤ê²°ì œê¸ˆì•¡': 'sum'
    }).nlargest(10, 'ì£¼ë¬¸ìˆ˜ëŸ‰').reset_index()
    
    # ì±„ë„ë³„ ë§¤ì¶œ
    df_channel = df_records.groupby('ì‡¼í•‘ëª°')['ì‹¤ê²°ì œê¸ˆì•¡'].sum().reset_index()
    
    # ëŒ€ì‹œë³´ë“œ ë ˆì´ì•„ì›ƒ
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        total_revenue = df_records['ì‹¤ê²°ì œê¸ˆì•¡'].sum()
        st.metric("ì´ ë§¤ì¶œ", f"â‚©{total_revenue:,.0f}")
    
    with col2:
        total_orders = len(df_records)
        st.metric("ì´ ì£¼ë¬¸ìˆ˜", f"{total_orders:,}")
    
    with col3:
        avg_order_value = total_revenue / total_orders if total_orders > 0 else 0
        st.metric("í‰ê·  ì£¼ë¬¸ ê¸ˆì•¡", f"â‚©{avg_order_value:,.0f}")
    
    with col4:
        unique_customers = df_records['ìˆ˜ë ¹ìëª…'].nunique()
        st.metric("ê³ ê°ìˆ˜", f"{unique_customers:,}")
    
    # ì°¨íŠ¸ ìƒì„±
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“ˆ ì¼ë³„ íŠ¸ë Œë“œ", "ğŸ† ë² ìŠ¤íŠ¸ì…€ëŸ¬", "ğŸ›’ ì±„ë„ ë¶„ì„", "ğŸ¤– AI ì¸ì‚¬ì´íŠ¸"])
    
    with tab1:
        if not df_daily.empty:
            fig_trend = go.Figure()
            fig_trend.add_trace(go.Scatter(
                x=df_daily['ë‚ ì§œ'], 
                y=df_daily['ë§¤ì¶œì•¡'],
                mode='lines+markers',
                name='ë§¤ì¶œì•¡',
                line=dict(color='#1f77b4', width=2)
            ))
            fig_trend.update_layout(
                title="ì¼ë³„ ë§¤ì¶œ íŠ¸ë Œë“œ",
                xaxis_title="ë‚ ì§œ",
                yaxis_title="ë§¤ì¶œì•¡ (ì›)",
                hovermode='x unified'
            )
            st.plotly_chart(fig_trend, use_container_width=True)
    
    with tab2:
        if not df_product_top.empty:
            fig_products = px.bar(
                df_product_top, 
                x='ì£¼ë¬¸ìˆ˜ëŸ‰', 
                y='SKUìƒí’ˆëª…',
                orientation='h',
                title="ìƒí’ˆë³„ íŒë§¤ ìˆ˜ëŸ‰ TOP 10",
                color='ì‹¤ê²°ì œê¸ˆì•¡',
                color_continuous_scale='Blues'
            )
            st.plotly_chart(fig_products, use_container_width=True)
    
    with tab3:
        if not df_channel.empty:
            fig_channel = px.pie(
                df_channel, 
                values='ì‹¤ê²°ì œê¸ˆì•¡', 
                names='ì‡¼í•‘ëª°',
                title="ì±„ë„ë³„ ë§¤ì¶œ ë¹„ì¤‘"
            )
            st.plotly_chart(fig_channel, use_container_width=True)
    
    with tab4:
        if GEMINI_AVAILABLE:
            with st.spinner("AIê°€ ë°ì´í„°ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                ai_insights = analyze_sales_data_with_ai(df_records)
                if ai_insights:
                    st.markdown("### ğŸ¤– AI íŒë§¤ ë¶„ì„ ë¦¬í¬íŠ¸")
                    st.markdown(ai_insights)
                else:
                    st.info("AI ë¶„ì„ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("AI ë¶„ì„ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ google-generativeaië¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.")

# --------------------------------------------------------------------------
# ê¸°ì¡´ í•¨ìˆ˜ë“¤
# --------------------------------------------------------------------------

def to_excel_formatted(df, format_type=None):
    """ë°ì´í„°í”„ë ˆì„ì„ ì„œì‹ì´ ì ìš©ëœ ì—‘ì…€ íŒŒì¼ í˜•ì‹ì˜ BytesIO ê°ì²´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜"""
    output = io.BytesIO()
    df_to_save = df.fillna('')
    
    if format_type == 'ecount_upload':
        df_to_save = df_to_save.rename(columns={'ì ìš”_ì „í‘œ': 'ì ìš”', 'ì ìš”_í’ˆëª©': 'ì ìš”.1'})

    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df_to_save.to_excel(writer, index=False, sheet_name='Sheet1')
    
    output.seek(0)
    workbook = openpyxl.load_workbook(output)
    sheet = workbook.active

    # ê³µí†µ ì„œì‹: ëª¨ë“  ì…€ ê°€ìš´ë° ì •ë ¬
    center_alignment = Alignment(horizontal='center', vertical='center')
    for row in sheet.iter_rows():
        for cell in row:
            cell.alignment = center_alignment

    # íŒŒì¼ë³„ íŠ¹ìˆ˜ ì„œì‹
    for column_cells in sheet.columns:
        max_length = 0
        column = column_cells[0].column_letter
        for cell in column_cells:
            try:
                if cell.value:
                    if len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
            except:
                pass
        adjusted_width = min((max_length + 2) * 1.2, 50)  # ìµœëŒ€ ë„ˆë¹„ ì œí•œ
        sheet.column_dimensions[column].width = adjusted_width
    
    thin_border = Border(left=Side(style='thin'), right=Side(style='thin'), 
                         top=Side(style='thin'), bottom=Side(style='thin'))
    pink_fill = PatternFill(start_color="FFEBEE", end_color="FFEBEE", fill_type="solid")

    if format_type == 'packing_list':
        for row in sheet.iter_rows(min_row=1, max_row=sheet.max_row, min_col=1, max_col=sheet.max_column):
            for cell in row:
                cell.border = thin_border
        
        bundle_start_row = 2
        for row_num in range(2, sheet.max_row + 2):
            current_bundle_cell = sheet.cell(row=row_num, column=1) if row_num <= sheet.max_row else None
            
            if (current_bundle_cell and current_bundle_cell.value) or (row_num > sheet.max_row):
                if row_num > 2:
                    bundle_end_row = row_num - 1
                    prev_bundle_num_str = str(sheet.cell(row=bundle_start_row, column=1).value)
                    
                    if prev_bundle_num_str.isdigit():
                        prev_bundle_num = int(prev_bundle_num_str)
                        if prev_bundle_num % 2 != 0:
                            for r in range(bundle_start_row, bundle_end_row + 1):
                                for c in range(1, sheet.max_column + 1):
                                    sheet.cell(row=r, column=c).fill = pink_fill
                    
                    if bundle_start_row < bundle_end_row:
                        sheet.merge_cells(start_row=bundle_start_row, start_column=1, 
                                        end_row=bundle_end_row, end_column=1)
                        sheet.merge_cells(start_row=bundle_start_row, start_column=4, 
                                        end_row=bundle_end_row, end_column=4)
                
                bundle_start_row = row_num

    if format_type == 'quantity_summary':
        for row_idx, row in enumerate(sheet.iter_rows(min_row=1, max_row=sheet.max_row, 
                                                     min_col=1, max_col=sheet.max_column)):
            for cell in row:
                cell.border = thin_border
            if row_idx > 0 and row_idx % 2 != 0:
                for cell in row:
                    cell.fill = pink_fill
    
    final_output = io.BytesIO()
    workbook.save(final_output)
    final_output.seek(0)
    
    return final_output.getvalue()

def process_all_files(file1, file2, file3, df_master):
    """ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜"""
    try:
        df_smartstore = pd.read_excel(file1)
        df_ecount_orig = pd.read_excel(file2)
        df_godomall = pd.read_excel(file3)

        df_ecount_orig['original_order'] = range(len(df_ecount_orig))
        
        # ì»¬ëŸ¼ëª… í˜¸í™˜ì„± ì²˜ë¦¬
        if 'íšŒ í• ì¸ ê¸ˆì•¡' in df_godomall.columns and 'íšŒì› í• ì¸ ê¸ˆì•¡' not in df_godomall.columns:
            df_godomall.rename(columns={'íšŒ í• ì¸ ê¸ˆì•¡': 'íšŒì› í• ì¸ ê¸ˆì•¡'}, inplace=True)
        if 'ìì²´ì˜µì…˜ì½”ë“œ' in df_godomall.columns:
            df_godomall.rename(columns={'ìì²´ì˜µì…˜ì½”ë“œ': 'ì¬ê³ ê´€ë¦¬ì½”ë“œ'}, inplace=True)
        
        # ë°ì´í„° í´ë¦¬ë‹
        cols_to_numeric = ['ìƒí’ˆë³„ í’ˆëª©ê¸ˆì•¡', 'ì´ ë°°ì†¡ ê¸ˆì•¡', 'íšŒì› í• ì¸ ê¸ˆì•¡', 
                          'ì¿ í° í• ì¸ ê¸ˆì•¡', 'ì‚¬ìš©ëœ ë§ˆì¼ë¦¬ì§€', 'ì´ ê²°ì œ ê¸ˆì•¡']
        for col in cols_to_numeric:
            if col in df_godomall.columns: 
                df_godomall[col] = pd.to_numeric(
                    df_godomall[col].astype(str).str.replace('[ì›,]', '', regex=True), 
                    errors='coerce'
                ).fillna(0)
        
        # ë°°ì†¡ë¹„ ì¤‘ë³µ ê³„ì‚° ë°©ì§€
        df_godomall['ë³´ì •ëœ_ë°°ì†¡ë¹„'] = np.where(
            df_godomall.duplicated(subset=['ìˆ˜ì·¨ì¸ ì´ë¦„']), 
            0, 
            df_godomall['ì´ ë°°ì†¡ ê¸ˆì•¡']
        )
        
        df_godomall['ìˆ˜ì •ë _ê¸ˆì•¡_ê³ ë„ëª°'] = (
            df_godomall['ìƒí’ˆë³„ í’ˆëª©ê¸ˆì•¡'] + df_godomall['ë³´ì •ëœ_ë°°ì†¡ë¹„'] - 
            df_godomall['íšŒì› í• ì¸ ê¸ˆì•¡'] - df_godomall['ì¿ í° í• ì¸ ê¸ˆì•¡'] - 
            df_godomall['ì‚¬ìš©ëœ ë§ˆì¼ë¦¬ì§€']
        )
        
        # ê²°ì œ ê¸ˆì•¡ ê²€ì¦
        godomall_warnings = []
        grouped_godomall = df_godomall.groupby('ìˆ˜ì·¨ì¸ ì´ë¦„')
        
        for name, group in grouped_godomall:
            calculated_total = group['ìˆ˜ì •ë _ê¸ˆì•¡_ê³ ë„ëª°'].sum()
            actual_total = group['ì´ ê²°ì œ ê¸ˆì•¡'].iloc[0]
            discrepancy = calculated_total - actual_total
            
            if abs(discrepancy) > 1:
                warning_msg = f"- [ê³ ë„ëª° ê¸ˆì•¡ ë¶ˆì¼ì¹˜] **{name}**ë‹˜ì˜ ì£¼ë¬¸ ê¸ˆì•¡ ì°¨ì´: **{discrepancy:,.0f}ì›**"
                godomall_warnings.append(warning_msg)

        # ê¸°ì¡´ ì²˜ë¦¬ ë¡œì§
        df_final = df_ecount_orig.copy().rename(columns={'ê¸ˆì•¡': 'ì‹¤ê²°ì œê¸ˆì•¡'})
        
        # ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ë³‘í•©
        key_cols_smartstore = ['ì¬ê³ ê´€ë¦¬ì½”ë“œ', 'ì£¼ë¬¸ìˆ˜ëŸ‰', 'ìˆ˜ë ¹ìëª…']
        smartstore_prices = df_smartstore.rename(columns={'ì‹¤ê²°ì œê¸ˆì•¡': 'ìˆ˜ì •ë _ê¸ˆì•¡_ìŠ¤í† ì–´'})[
            key_cols_smartstore + ['ìˆ˜ì •ë _ê¸ˆì•¡_ìŠ¤í† ì–´']
        ].drop_duplicates(subset=key_cols_smartstore, keep='first')
        
        # ê³ ë„ëª° ë³‘í•©
        key_cols_godomall = ['ì¬ê³ ê´€ë¦¬ì½”ë“œ', 'ìˆ˜ì·¨ì¸ ì´ë¦„', 'ìƒí’ˆìˆ˜ëŸ‰']
        godomall_prices_for_merge = df_godomall[key_cols_godomall + ['ìˆ˜ì •ë _ê¸ˆì•¡_ê³ ë„ëª°']].rename(
            columns={'ìˆ˜ì·¨ì¸ ì´ë¦„': 'ìˆ˜ë ¹ìëª…', 'ìƒí’ˆìˆ˜ëŸ‰': 'ì£¼ë¬¸ìˆ˜ëŸ‰'}
        )
        godomall_prices_for_merge = godomall_prices_for_merge.drop_duplicates(
            subset=['ì¬ê³ ê´€ë¦¬ì½”ë“œ', 'ìˆ˜ë ¹ìëª…', 'ì£¼ë¬¸ìˆ˜ëŸ‰'], keep='first'
        )
        
        # ë°ì´í„° íƒ€ì… í†µì¼
        for col in ['ì¬ê³ ê´€ë¦¬ì½”ë“œ', 'ìˆ˜ë ¹ìëª…']:
            df_final[col] = df_final[col].astype(str).str.strip()
            smartstore_prices[col] = smartstore_prices[col].astype(str).str.strip()
            godomall_prices_for_merge[col] = godomall_prices_for_merge[col].astype(str).str.strip()
        
        for col in ['ì£¼ë¬¸ìˆ˜ëŸ‰']:
            df_final[col] = pd.to_numeric(df_final[col], errors='coerce').fillna(0).astype(int)
            smartstore_prices[col] = pd.to_numeric(smartstore_prices[col], errors='coerce').fillna(0).astype(int)
            godomall_prices_for_merge[col] = pd.to_numeric(godomall_prices_for_merge[col], errors='coerce').fillna(0).astype(int)

        df_final['ì‹¤ê²°ì œê¸ˆì•¡'] = pd.to_numeric(df_final['ì‹¤ê²°ì œê¸ˆì•¡'], errors='coerce').fillna(0).astype(int)
        
        # ë°ì´í„° ë³‘í•©
        df_final = pd.merge(df_final, smartstore_prices, on=key_cols_smartstore, how='left')
        df_final = pd.merge(df_final, godomall_prices_for_merge, 
                            on=['ì¬ê³ ê´€ë¦¬ì½”ë“œ', 'ìˆ˜ë ¹ìëª…', 'ì£¼ë¬¸ìˆ˜ëŸ‰'], how='left')

        # ê²½ê³  ë©”ì‹œì§€ ìƒì„±
        warnings = []
        failed_corrections = df_final[
            ((df_final['ì‡¼í•‘ëª°'] == 'ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´') & (df_final['ìˆ˜ì •ë _ê¸ˆì•¡_ìŠ¤í† ì–´'].isna())) |
            ((df_final['ì‡¼í•‘ëª°'] == 'ê³ ë„ëª°5') & (df_final['ìˆ˜ì •ë _ê¸ˆì•¡_ê³ ë„ëª°'].isna()))
        ]
        
        for _, row in failed_corrections.iterrows():
            warnings.append(f"- [ê¸ˆì•¡ë³´ì • ì‹¤íŒ¨] **{row['ì‡¼í•‘ëª°']}** / {row['ìˆ˜ë ¹ìëª…']} / {row['SKUìƒí’ˆëª…']}")
        
        warnings.extend(godomall_warnings)

        # ìµœì¢… ê²°ì œ ê¸ˆì•¡ ì—…ë°ì´íŠ¸
        df_final['ì‹¤ê²°ì œê¸ˆì•¡'] = np.where(
            df_final['ì‡¼í•‘ëª°'] == 'ê³ ë„ëª°5',
            df_final['ìˆ˜ì •ë _ê¸ˆì•¡_ê³ ë„ëª°'].fillna(df_final['ì‹¤ê²°ì œê¸ˆì•¡']),
            df_final['ì‹¤ê²°ì œê¸ˆì•¡']
        )
        df_final['ì‹¤ê²°ì œê¸ˆì•¡'] = np.where(
            df_final['ì‡¼í•‘ëª°'] == 'ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´',
            df_final['ìˆ˜ì •ë _ê¸ˆì•¡_ìŠ¤í† ì–´'].fillna(df_final['ì‹¤ê²°ì œê¸ˆì•¡']),
            df_final['ì‹¤ê²°ì œê¸ˆì•¡']
        )
        
        df_main_result = df_final[['ì¬ê³ ê´€ë¦¬ì½”ë“œ', 'SKUìƒí’ˆëª…', 'ì£¼ë¬¸ìˆ˜ëŸ‰', 'ì‹¤ê²°ì œê¸ˆì•¡', 'ì‡¼í•‘ëª°', 'ìˆ˜ë ¹ìëª…', 'original_order']]
        
        # ë™ëª…ì´ì¸ ê²½ê³ 
        name_groups = df_main_result.groupby('ìˆ˜ë ¹ìëª…')['original_order'].apply(list)
        for name, orders in name_groups.items():
            if len(orders) > 1 and (max(orders) - min(orders) + 1) != len(orders):
                warnings.append(f"- [ë™ëª…ì´ì¸ ì˜ì‹¬] **{name}** ë‹˜ì˜ ì£¼ë¬¸ì´ ë–¨ì–´ì ¸ì„œ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # ìš”ì•½ ë°ì´í„° ìƒì„±
        df_quantity_summary = df_main_result.groupby('SKUìƒí’ˆëª…', as_index=False)['ì£¼ë¬¸ìˆ˜ëŸ‰'].sum().rename(
            columns={'ì£¼ë¬¸ìˆ˜ëŸ‰': 'ê°œìˆ˜'}
        )
        
        df_packing_list = df_main_result.sort_values(by='original_order')[
            ['SKUìƒí’ˆëª…', 'ì£¼ë¬¸ìˆ˜ëŸ‰', 'ìˆ˜ë ¹ìëª…', 'ì‡¼í•‘ëª°']
        ].copy()
        
        is_first_item = df_packing_list['ìˆ˜ë ¹ìëª…'] != df_packing_list['ìˆ˜ë ¹ìëª…'].shift(1)
        df_packing_list['ë¬¶ìŒë²ˆí˜¸'] = is_first_item.cumsum()
        df_packing_list_final = df_packing_list.copy()
        df_packing_list_final['ë¬¶ìŒë²ˆí˜¸'] = df_packing_list_final['ë¬¶ìŒë²ˆí˜¸'].where(is_first_item, '')
        df_packing_list_final = df_packing_list_final[['ë¬¶ìŒë²ˆí˜¸', 'SKUìƒí’ˆëª…', 'ì£¼ë¬¸ìˆ˜ëŸ‰', 'ìˆ˜ë ¹ìëª…', 'ì‡¼í•‘ëª°']]

        # ì´ì¹´ìš´íŠ¸ ì—…ë¡œë“œ ë°ì´í„° ìƒì„±
        df_merged = pd.merge(
            df_main_result, 
            df_master[['SKUì½”ë“œ', 'ê³¼ì„¸ì—¬ë¶€', 'ì…ìˆ˜ëŸ‰']], 
            left_on='ì¬ê³ ê´€ë¦¬ì½”ë“œ', 
            right_on='SKUì½”ë“œ', 
            how='left'
        )
        
        # ë¯¸ë“±ë¡ ìƒí’ˆ ê²½ê³ 
        unmastered = df_merged[df_merged['SKUì½”ë“œ'].isna()]
        for _, row in unmastered.iterrows():
            warnings.append(f"- [ë¯¸ë“±ë¡ ìƒí’ˆ] **{row['ì¬ê³ ê´€ë¦¬ì½”ë“œ']}** / {row['SKUìƒí’ˆëª…']}")

        # ê±°ë˜ì²˜ ë§¤í•‘
        client_map = {
            'ì¿ íŒ¡': 'ì¿ íŒ¡ ì£¼ì‹íšŒì‚¬', 
            'ê³ ë„ëª°5': 'ê³ ë˜ë¯¸ìì‚¬ëª°_í˜„ê¸ˆì˜ìˆ˜ì¦(ê³ ë„ëª°)', 
            'ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´': 'ìŠ¤í† ì–´íŒœ',
            'ë°°ë¯¼ìƒíšŒ': 'ì£¼ì‹íšŒì‚¬ ìš°ì•„í•œí˜•ì œë“¤(ë°°ë¯¼ìƒíšŒ)',
            'ì´ì§€ì›°ëª°': 'ì£¼ì‹íšŒì‚¬ í˜„ëŒ€ì´ì§€ì›°'
        }
        
        # ì´ì¹´ìš´íŠ¸ ë°ì´í„° ìƒì„±
        df_ecount_upload = pd.DataFrame()
        
        df_ecount_upload['ì¼ì'] = datetime.now().strftime("%Y%m%d")
        df_ecount_upload['ê±°ë˜ì²˜ëª…'] = df_merged['ì‡¼í•‘ëª°'].map(client_map).fillna(df_merged['ì‡¼í•‘ëª°'])
        df_ecount_upload['ì¶œí•˜ì°½ê³ '] = 'ê³ ë˜ë¯¸'
        df_ecount_upload['ê±°ë˜ìœ í˜•'] = np.where(df_merged['ê³¼ì„¸ì—¬ë¶€'] == 'ë©´ì„¸', 12, 11)
        df_ecount_upload['ì ìš”_ì „í‘œ'] = 'ì˜¤ì „/ì˜¨ë¼ì¸'
        df_ecount_upload['í’ˆëª©ì½”ë“œ'] = df_merged['ì¬ê³ ê´€ë¦¬ì½”ë“œ']
        
        # ìˆ˜ëŸ‰ ê³„ì‚°
        is_box_order = df_merged['SKUìƒí’ˆëª…'].str.contains("BOX", na=False)
        ì…ìˆ˜ëŸ‰ = pd.to_numeric(df_merged['ì…ìˆ˜ëŸ‰'], errors='coerce').fillna(1)
        base_quantity = np.where(is_box_order, df_merged['ì£¼ë¬¸ìˆ˜ëŸ‰'] * ì…ìˆ˜ëŸ‰, df_merged['ì£¼ë¬¸ìˆ˜ëŸ‰'])
        is_3_pack = df_merged['SKUìƒí’ˆëª…'].str.contains("3ê°œì…|3ê°œ", na=False)
        final_quantity = np.where(is_3_pack, base_quantity * 3, base_quantity)
        
        df_ecount_upload['ë°•ìŠ¤'] = np.where(is_box_order, df_merged['ì£¼ë¬¸ìˆ˜ëŸ‰'], np.nan)
        df_ecount_upload['ìˆ˜ëŸ‰'] = final_quantity.astype(int)
        
        # ê¸ˆì•¡ ê³„ì‚°
        df_merged['ì‹¤ê²°ì œê¸ˆì•¡'] = pd.to_numeric(df_merged['ì‹¤ê²°ì œê¸ˆì•¡'], errors='coerce').fillna(0)
        ê³µê¸‰ê°€ì•¡ = np.where(df_merged['ê³¼ì„¸ì—¬ë¶€'] == 'ê³¼ì„¸', df_merged['ì‹¤ê²°ì œê¸ˆì•¡'] / 1.1, df_merged['ì‹¤ê²°ì œê¸ˆì•¡'])
        df_ecount_upload['ê³µê¸‰ê°€ì•¡'] = ê³µê¸‰ê°€ì•¡
        df_ecount_upload['ë¶€ê°€ì„¸'] = df_merged['ì‹¤ê²°ì œê¸ˆì•¡'] - df_ecount_upload['ê³µê¸‰ê°€ì•¡']
        
        df_ecount_upload['ì‡¼í•‘ëª°ê³ ê°ëª…'] = df_merged['ìˆ˜ë ¹ìëª…']
        df_ecount_upload['original_order'] = df_merged['original_order']
        
        # ì´ì¹´ìš´íŠ¸ ì»¬ëŸ¼ ì •ë¦¬
        ecount_columns = [
            'ì¼ì', 'ìˆœë²ˆ', 'ê±°ë˜ì²˜ì½”ë“œ', 'ê±°ë˜ì²˜ëª…', 'ë‹´ë‹¹ì', 'ì¶œí•˜ì°½ê³ ', 'ê±°ë˜ìœ í˜•', 'í†µí™”', 'í™˜ìœ¨', 
            'ì ìš”_ì „í‘œ', 'ë¯¸ìˆ˜ê¸ˆ', 'ì´í•©ê³„', 'ì—°ê²°ì „í‘œ', 'í’ˆëª©ì½”ë“œ', 'í’ˆëª©ëª…', 'ê·œê²©', 'ë°•ìŠ¤', 'ìˆ˜ëŸ‰', 
            'ë‹¨ê°€', 'ì™¸í™”ê¸ˆì•¡', 'ê³µê¸‰ê°€ì•¡', 'ë¶€ê°€ì„¸', 'ì ìš”_í’ˆëª©', 'ìƒì‚°ì „í‘œìƒì„±', 'ì‹œë¦¬ì–¼/ë¡œíŠ¸', 
            'ê´€ë¦¬í•­ëª©', 'ì‡¼í•‘ëª°ê³ ê°ëª…', 'original_order'
        ]
        
        for col in ecount_columns:
            if col not in df_ecount_upload:
                df_ecount_upload[col] = ''
        
        for col in ['ê³µê¸‰ê°€ì•¡', 'ë¶€ê°€ì„¸']:
            df_ecount_upload[col] = df_ecount_upload[col].round().astype('Int64')
        
        df_ecount_upload['ê±°ë˜ìœ í˜•'] = pd.to_numeric(df_ecount_upload['ê±°ë˜ìœ í˜•'])
        
        # ì •ë ¬
        sort_order = [
            'ê³ ë˜ë¯¸ìì‚¬ëª°_í˜„ê¸ˆì˜ìˆ˜ì¦(ê³ ë„ëª°)', 
            'ìŠ¤í† ì–´íŒœ', 
            'ì¿ íŒ¡ ì£¼ì‹íšŒì‚¬',
            'ì£¼ì‹íšŒì‚¬ ìš°ì•„í•œí˜•ì œë“¤(ë°°ë¯¼ìƒíšŒ)',
            'ì£¼ì‹íšŒì‚¬ í˜„ëŒ€ì´ì§€ì›°'
        ]
        
        df_ecount_upload['ê±°ë˜ì²˜ëª…_sort'] = pd.Categorical(
            df_ecount_upload['ê±°ë˜ì²˜ëª…'], 
            categories=sort_order, 
            ordered=True
        )
        
        df_ecount_upload = df_ecount_upload.sort_values(
            by=['ê±°ë˜ì²˜ëª…_sort', 'ê±°ë˜ìœ í˜•', 'original_order'],
            ascending=[True, True, True]
        ).drop(columns=['ê±°ë˜ì²˜ëª…_sort', 'original_order'])
        
        df_ecount_upload = df_ecount_upload[ecount_columns[:-1]]

        return (df_main_result.drop(columns=['original_order']), 
                df_quantity_summary, 
                df_packing_list_final, 
                df_ecount_upload, 
                True, 
                "ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.", 
                warnings)

    except Exception as e:
        import traceback
        error_msg = f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\n{traceback.format_exc()}"
        return None, None, None, None, False, error_msg, []

# --------------------------------------------------------------------------
# Streamlit ì•± UI êµ¬ì„±
# --------------------------------------------------------------------------

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì£¼ë¬¸ ì²˜ë¦¬ ìë™í™” v2.0",
    layout="wide",
    page_icon="ğŸ“Š"
)

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.title("ğŸ“Š Order Pro v2.0")
    st.markdown("---")
    
    menu_option = st.radio(
        "ë©”ë‰´ ì„ íƒ",
        ["ğŸ“‘ ì£¼ë¬¸ ì²˜ë¦¬", "ğŸ“ˆ íŒë§¤ ë¶„ì„", "âš™ï¸ ì„¤ì •"],
        index=0
    )
    
    st.markdown("---")
    st.caption("ì—°ê²° ìƒíƒœ")
    
    # SharePoint ìƒíƒœ
    if SHAREPOINT_AVAILABLE:
        ctx = init_sharepoint_context()
        if ctx:
            st.success("âœ… SharePoint ì—°ê²°")
        else:
            st.warning("âš ï¸ SharePoint ì˜¤í”„ë¼ì¸")
    else:
        st.info("ğŸ’¾ ë¡œì»¬ ëª¨ë“œ")
    
    # AI ìƒíƒœ
    if GEMINI_AVAILABLE:
        if "GEMINI_API_KEY" in st.secrets:
            st.success("âœ… AI í™œì„±í™”")
        else:
            st.warning("âš ï¸ AI í‚¤ í•„ìš”")
    else:
        st.info("ğŸ¤– AI ë¹„í™œì„±í™”")

# ë©”ì¸ í™”ë©´
if menu_option == "ğŸ“‘ ì£¼ë¬¸ ì²˜ë¦¬":
    st.title("ğŸ“‘ ì£¼ë¬¸ ì²˜ë¦¬ ìë™í™”")
    
    if SHAREPOINT_AVAILABLE and init_sharepoint_context():
        st.info("ğŸ’¡ SharePointì™€ ì—°ë™í•˜ì—¬ ìë™ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
    else:
        st.info("ğŸ’¡ ë¡œì»¬ ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. master_data.csv íŒŒì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    st.write("---")
    st.header("1. ì›ë³¸ ì—‘ì…€ íŒŒì¼ 3ê°œ ì—…ë¡œë“œ")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        file1 = st.file_uploader("1ï¸âƒ£ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´", type=['xlsx', 'xls'])
    with col2:
        file2 = st.file_uploader("2ï¸âƒ£ ì´ì¹´ìš´íŠ¸", type=['xlsx', 'xls'])
    with col3:
        file3 = st.file_uploader("3ï¸âƒ£ ê³ ë„ëª°", type=['xlsx', 'xls'])
    
    st.write("---")
    st.header("2. ì²˜ë¦¬ ì‹¤í–‰")
    
    if st.button("ğŸš€ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘", type="primary", disabled=not (file1 and file2 and file3)):
        if file1 and file2 and file3:
            try:
                # ë§ˆìŠ¤í„° ë°ì´í„° ë¡œë“œ
                with st.spinner('ë§ˆìŠ¤í„° ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...'):
                    df_master = load_master_data_from_sharepoint()
                
                if df_master.empty:
                    st.error("ë§ˆìŠ¤í„° ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    # íŒŒì¼ ì²˜ë¦¬
                    with st.spinner('íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘...'):
                        result = process_all_files(file1, file2, file3, df_master)
                        df_main, df_qty, df_pack, df_ecount, success, message, warnings = result
                    
                    if success:
                        st.success(message)
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        
                        # SharePoint ì €ì¥ (ì˜µì…˜)
                        if SHAREPOINT_AVAILABLE and init_sharepoint_context():
                            with st.spinner('SharePointì— ì €ì¥ ì¤‘...'):
                                save_success, save_msg = save_to_sharepoint_records(df_main, df_ecount)
                                if save_success:
                                    st.success(f"âœ… {save_msg}")
                                else:
                                    st.info(f"â„¹ï¸ {save_msg}")
                        
                        # ê²½ê³  í‘œì‹œ
                        if warnings:
                            with st.expander("âš ï¸ í™•ì¸ í•„ìš” í•­ëª©"):
                                for w in warnings:
                                    st.markdown(w)
                        
                        # ê²°ê³¼ í‘œì‹œ
                        tabs = st.tabs(["ğŸ¢ ì´ì¹´ìš´íŠ¸", "ğŸ“‹ í¬ì¥ë¦¬ìŠ¤íŠ¸", "ğŸ“¦ ìˆ˜ëŸ‰ìš”ì•½", "âœ… ìµœì¢…ê²°ê³¼"])
                        
                        with tabs[0]:
                            st.dataframe(df_ecount.astype(str), use_container_width=True)
                            st.download_button(
                                "ğŸ“¥ ë‹¤ìš´ë¡œë“œ",
                                to_excel_formatted(df_ecount, 'ecount_upload'),
                                f"ì´ì¹´ìš´íŠ¸_{timestamp}.xlsx"
                            )
                        
                        with tabs[1]:
                            st.dataframe(df_pack, use_container_width=True)
                            st.download_button(
                                "ğŸ“¥ ë‹¤ìš´ë¡œë“œ",
                                to_excel_formatted(df_pack, 'packing_list'),
                                f"í¬ì¥ë¦¬ìŠ¤íŠ¸_{timestamp}.xlsx"
                            )
                        
                        with tabs[2]:
                            st.dataframe(df_qty, use_container_width=True)
                            st.download_button(
                                "ğŸ“¥ ë‹¤ìš´ë¡œë“œ",
                                to_excel_formatted(df_qty, 'quantity_summary'),
                                f"ìˆ˜ëŸ‰ìš”ì•½_{timestamp}.xlsx"
                            )
                        
                        with tabs[3]:
                            st.dataframe(df_main, use_container_width=True)
                            st.download_button(
                                "ğŸ“¥ ë‹¤ìš´ë¡œë“œ",
                                to_excel_formatted(df_main),
                                f"ìµœì¢…ê²°ê³¼_{timestamp}.xlsx"
                            )
                    else:
                        st.error(message)
                        
            except Exception as e:
                st.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        else:
            st.warning("3ê°œ íŒŒì¼ì„ ëª¨ë‘ ì—…ë¡œë“œí•˜ì„¸ìš”.")

elif menu_option == "ğŸ“ˆ íŒë§¤ ë¶„ì„":
    st.title("ğŸ“ˆ íŒë§¤ ë°ì´í„° ë¶„ì„")
    
    if not SHAREPOINT_AVAILABLE:
        st.warning("SharePointê°€ ì—°ê²°ë˜ì§€ ì•Šì•„ ë¶„ì„ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        col1, col2 = st.columns(2)
        with col1:
            period = st.selectbox("ë¶„ì„ ê¸°ê°„", ["ìµœê·¼ 7ì¼", "ìµœê·¼ 30ì¼", "ì „ì²´"])
        
        if st.button("ğŸ“Š ë¶„ì„ ì‹œì‘", type="primary"):
            with st.spinner("ë°ì´í„° ë¡œë“œ ì¤‘..."):
                df_records = load_record_data_from_sharepoint()
                
                if not df_records.empty:
                    # ê¸°ê°„ í•„í„°
                    if period != "ì „ì²´":
                        days = 7 if period == "ìµœê·¼ 7ì¼" else 30
                        cutoff = datetime.now() - timedelta(days=days)
                        df_records = df_records[df_records['ì£¼ë¬¸ì¼ì'] >= cutoff]
                    
                    if not df_records.empty:
                        create_analytics_dashboard(df_records)
                    else:
                        st.info("ì„ íƒ ê¸°ê°„ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.info("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

elif menu_option == "âš™ï¸ ì„¤ì •":
    st.title("âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
    
    st.header("ì—°ê²° ì •ë³´")
    
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("SharePoint")
        if SHAREPOINT_AVAILABLE:
            if "sharepoint" in st.secrets:
                st.text_input("Tenant ID", value=st.secrets["sharepoint"]["tenant_id"][:10] + "...", disabled=True)
                st.text_input("Client ID", value=st.secrets["sharepoint"]["client_id"][:10] + "...", disabled=True)
            else:
                st.info("SharePoint ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("SharePoint ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    with col2:
        st.subheader("AI")
        if GEMINI_AVAILABLE:
            if "GEMINI_API_KEY" in st.secrets:
                st.text_input("API Key", value=st.secrets["GEMINI_API_KEY"][:10] + "...", disabled=True)
            else:
                st.info("AI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        else:
            st.info("AI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    if st.button("ğŸ”„ ì—°ê²° í…ŒìŠ¤íŠ¸"):
        with st.spinner("í…ŒìŠ¤íŠ¸ ì¤‘..."):
            # SharePoint í…ŒìŠ¤íŠ¸
            if SHAREPOINT_AVAILABLE:
                ctx = init_sharepoint_context()
                if ctx:
                    st.success("âœ… SharePoint ì—°ê²° ì„±ê³µ")
                else:
                    st.error("âŒ SharePoint ì—°ê²° ì‹¤íŒ¨")
            
            # AI í…ŒìŠ¤íŠ¸
            if GEMINI_AVAILABLE:
                model = init_gemini()
                if model:
                    st.success("âœ… AI ì—°ê²° ì„±ê³µ")
                else:
                    st.error("âŒ AI ì—°ê²° ì‹¤íŒ¨")
    
    st.header("ìºì‹œ ê´€ë¦¬")
    if st.button("ğŸ—‘ï¸ ìºì‹œ ì´ˆê¸°í™”"):
        st.cache_data.clear()
        st.cache_resource.clear()
        st.success("ìºì‹œë¥¼ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")
        st.rerun()
