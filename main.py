import streamlit as st
import pandas as pd
import io
import numpy as np
import openpyxl
from openpyxl.styles import PatternFill, Border, Side, Alignment
from openpyxl.utils import get_column_letter
from datetime import datetime, timedelta
import requests
from office365.runtime.auth.client_credential import ClientCredential
from office365.sharepoint.client_context import ClientContext
from office365.sharepoint.files.file import File
import plotly.express as px
import plotly.graph_objects as go
import google.generativeai as genai
from typing import Optional, Tuple, List, Dict
import hashlib
import json

# --------------------------------------------------------------------------
# SharePoint Ïó∞Í≤∞ ÏÑ§Ï†ï
# --------------------------------------------------------------------------

@st.cache_resource
def init_sharepoint_context():
    """SharePoint Ïª®ÌÖçÏä§Ìä∏ Ï¥àÍ∏∞Ìôî"""
    try:
        tenant_id = st.secrets["sharepoint"]["tenant_id"]
        client_id = st.secrets["sharepoint"]["client_id"]
        client_secret = st.secrets["sharepoint"]["client_secret"]
        site_url = "https://goremi.sharepoint.com/sites/data"
        
        credentials = ClientCredential(client_id, client_secret)
        ctx = ClientContext(site_url).with_credentials(credentials)
        return ctx
    except Exception as e:
        st.error(f"SharePoint Ïó∞Í≤∞ Ïã§Ìå®: {e}")
        return None

@st.cache_data(ttl=600)  # 10Î∂Ñ Ï∫êÏãú
def load_master_data_from_sharepoint():
    """SharePointÏóêÏÑú ÎßàÏä§ÌÑ∞ Îç∞Ïù¥ÌÑ∞ Î°úÎìú"""
    try:
        ctx = init_sharepoint_context()
        if not ctx:
            return load_local_master_data("master_data.csv")
        
        file_url = st.secrets["sharepoint_files"]["plto_master_data_file_url"]
        
        # SharePointÏóêÏÑú ÌååÏùº Îã§Ïö¥Î°úÎìú
        response = File.open_binary(ctx, file_url)
        
        # BytesIOÎ°ú Î≥ÄÌôò ÌõÑ pandasÎ°ú ÏùΩÍ∏∞
        df_master = pd.read_excel(io.BytesIO(response.content))
        df_master = df_master.drop_duplicates(subset=['SKUÏΩîÎìú'], keep='first')
        
        return df_master
    except Exception as e:
        st.warning(f"SharePoint Ïó∞Í≤∞ Ïã§Ìå®, Î°úÏª¨ ÌååÏùº ÏÇ¨Ïö©: {e}")
        return load_local_master_data("master_data.csv")

def load_local_master_data(file_path="master_data.csv"):
    """Î°úÏª¨ Î∞±ÏóÖ ÎßàÏä§ÌÑ∞ Îç∞Ïù¥ÌÑ∞ Î°úÎìú"""
    try:
        df_master = pd.read_csv(file_path)
        df_master = df_master.drop_duplicates(subset=['SKUÏΩîÎìú'], keep='first')
        return df_master
    except:
        st.error("Î°úÏª¨ ÎßàÏä§ÌÑ∞ Îç∞Ïù¥ÌÑ∞ÎèÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§!")
        return pd.DataFrame()

def save_to_sharepoint_records(df_main_result, df_ecount_upload):
    """Ï≤òÎ¶¨ Í≤∞Í≥ºÎ•º SharePointÏùò plto_record_data.xlsxÏóê Ï†ÄÏû•"""
    try:
        ctx = init_sharepoint_context()
        if not ctx:
            return False, "SharePoint Ïó∞Í≤∞ Ïã§Ìå®"
        
        record_file_url = st.secrets["sharepoint_files"]["plto_record_data_file_url"]
        
        # Í∏∞Ï°¥ Î†àÏΩîÎìú ÌååÏùº Îã§Ïö¥Î°úÎìú ÏãúÎèÑ
        try:
            response = File.open_binary(ctx, record_file_url)
            existing_df = pd.read_excel(io.BytesIO(response.content))
        except:
            # ÌååÏùºÏù¥ ÏóÜÏúºÎ©¥ ÏÉàÎ°ú ÏÉùÏÑ±
            existing_df = pd.DataFrame()
        
        # ÏÉà Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ
        new_records = pd.DataFrame()
        
        # Ï£ºÎ¨∏ ÎÇ†Ïßú Ï∂îÏ∂ú (Ïù¥Ïπ¥Ïö¥Ìä∏ ÏóÖÎ°úÎìú Îç∞Ïù¥ÌÑ∞Ïùò ÏùºÏûê ÏÇ¨Ïö©)
        order_date = df_ecount_upload['ÏùºÏûê'].iloc[0] if not df_ecount_upload.empty else datetime.now().strftime("%Y%m%d")
        
        # Í∏∞Î°ùÌï† Îç∞Ïù¥ÌÑ∞ Íµ¨ÏÑ±
        new_records['Ï£ºÎ¨∏ÏùºÏûê'] = order_date
        new_records['Ï≤òÎ¶¨ÏùºÏãú'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        new_records['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú'] = df_main_result['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú']
        new_records['SKUÏÉÅÌíàÎ™Ö'] = df_main_result['SKUÏÉÅÌíàÎ™Ö']
        new_records['Ï£ºÎ¨∏ÏàòÎüâ'] = df_main_result['Ï£ºÎ¨∏ÏàòÎüâ']
        new_records['Ïã§Í≤∞Ï†úÍ∏àÏï°'] = df_main_result['Ïã§Í≤∞Ï†úÍ∏àÏï°']
        new_records['ÏáºÌïëÎ™∞'] = df_main_result['ÏáºÌïëÎ™∞']
        new_records['ÏàòÎ†πÏûêÎ™Ö'] = df_main_result['ÏàòÎ†πÏûêÎ™Ö']
        
        # Ï§ëÎ≥µ Ï≤¥ÌÅ¨Ïö© Ìï¥Ïãú ÏÉùÏÑ±
        new_records['unique_hash'] = new_records.apply(
            lambda x: hashlib.md5(
                f"{x['Ï£ºÎ¨∏ÏùºÏûê']}_{x['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú']}_{x['ÏàòÎ†πÏûêÎ™Ö']}_{x['ÏáºÌïëÎ™∞']}".encode()
            ).hexdigest(), axis=1
        )
        
        # Í∏∞Ï°¥ Îç∞Ïù¥ÌÑ∞ÏôÄ Î≥ëÌï© (Ï§ëÎ≥µ Ï†úÍ±∞)
        if not existing_df.empty and 'unique_hash' in existing_df.columns:
            # Ï§ëÎ≥µÎêòÏßÄ ÏïäÎäî ÏÉà Î†àÏΩîÎìúÎßå Ï∂îÍ∞Ä
            new_unique_records = new_records[~new_records['unique_hash'].isin(existing_df['unique_hash'])]
            combined_df = pd.concat([existing_df, new_unique_records], ignore_index=True)
        else:
            combined_df = new_records
        
        # Excel ÌååÏùºÎ°ú Ï†ÄÏû•
        output = io.BytesIO()
        combined_df.to_excel(output, index=False, sheet_name='Records')
        output.seek(0)
        
        # SharePointÏóê ÏóÖÎ°úÎìú
        target_folder = ctx.web.get_folder_by_server_relative_url("/sites/data/Shared Documents")
        target_folder.upload_file("plto_record_data.xlsx", output.read()).execute_query()
        
        return True, f"ÏÑ±Í≥µÏ†ÅÏúºÎ°ú {len(new_records)}Í∞úÏùò Î†àÏΩîÎìúÎ•º Ï†ÄÏû•ÌñàÏäµÎãàÎã§."
        
    except Exception as e:
        return False, f"SharePoint Ï†ÄÏû• Ïã§Ìå®: {e}"

# --------------------------------------------------------------------------
# AI Î∂ÑÏÑù Í∏∞Îä•
# --------------------------------------------------------------------------

@st.cache_resource
def init_gemini():
    """Gemini AI Ï¥àÍ∏∞Ìôî"""
    try:
        genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
        model = genai.GenerativeModel('gemini-pro')
        return model
    except Exception as e:
        st.error(f"Gemini AI Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
        return None

def analyze_sales_data_with_ai(df_records):
    """AIÎ•º ÏÇ¨Ïö©Ìïú ÌåêÎß§ Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù"""
    try:
        model = init_gemini()
        if not model or df_records.empty:
            return None
        
        # Îç∞Ïù¥ÌÑ∞ ÏöîÏïΩ Ï§ÄÎπÑ
        summary = {
            "total_orders": len(df_records),
            "total_revenue": df_records['Ïã§Í≤∞Ï†úÍ∏àÏï°'].sum(),
            "unique_products": df_records['SKUÏÉÅÌíàÎ™Ö'].nunique(),
            "unique_customers": df_records['ÏàòÎ†πÏûêÎ™Ö'].nunique(),
            "date_range": f"{df_records['Ï£ºÎ¨∏ÏùºÏûê'].min()} ~ {df_records['Ï£ºÎ¨∏ÏùºÏûê'].max()}",
            "top_products": df_records.groupby('SKUÏÉÅÌíàÎ™Ö')['Ï£ºÎ¨∏ÏàòÎüâ'].sum().nlargest(5).to_dict(),
            "channel_distribution": df_records.groupby('ÏáºÌïëÎ™∞')['Ïã§Í≤∞Ï†úÍ∏àÏï°'].sum().to_dict()
        }
        
        prompt = f"""
        Îã§Ïùå Ïò®ÎùºÏù∏ ÏáºÌïëÎ™∞ ÌåêÎß§ Îç∞Ïù¥ÌÑ∞Î•º Î∂ÑÏÑùÌïòÍ≥† Ïù∏ÏÇ¨Ïù¥Ìä∏Î•º Ï†úÍ≥µÌï¥Ï£ºÏÑ∏Ïöî:
        
        {json.dumps(summary, ensure_ascii=False, indent=2)}
        
        Îã§Ïùå Ìï≠Î™©Îì§ÏùÑ Ìè¨Ìï®Ìï¥ÏÑú Î∂ÑÏÑùÌï¥Ï£ºÏÑ∏Ïöî:
        1. Ï†ÑÏ≤¥Ï†ÅÏù∏ ÌåêÎß§ Ìä∏Î†åÎìú
        2. Î≤†Ïä§Ìä∏ÏÖÄÎü¨ ÏÉÅÌíà Î∂ÑÏÑù
        3. Ï±ÑÎÑêÎ≥Ñ ÌåêÎß§ ÏÑ±Í≥º
        4. Í∞úÏÑ† Ï†úÏïàÏÇ¨Ìï≠
        
        Í∞ÑÍ≤∞ÌïòÍ≥† Ïã§Ïö©Ï†ÅÏù∏ Ïù∏ÏÇ¨Ïù¥Ìä∏Î•º Ï†úÍ≥µÌï¥Ï£ºÏÑ∏Ïöî.
        """
        
        response = model.generate_content(prompt)
        return response.text
        
    except Exception as e:
        return f"AI Î∂ÑÏÑù Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}"

def load_record_data_from_sharepoint():
    """SharePointÏóêÏÑú Í∏∞Î°ù Îç∞Ïù¥ÌÑ∞ Î°úÎìú"""
    try:
        ctx = init_sharepoint_context()
        if not ctx:
            return pd.DataFrame()
        
        record_file_url = st.secrets["sharepoint_files"]["plto_record_data_file_url"]
        response = File.open_binary(ctx, record_file_url)
        df_records = pd.read_excel(io.BytesIO(response.content))
        
        # ÎÇ†Ïßú ÌòïÏãù Ï†ïÍ∑úÌôî
        if 'Ï£ºÎ¨∏ÏùºÏûê' in df_records.columns:
            df_records['Ï£ºÎ¨∏ÏùºÏûê'] = pd.to_datetime(df_records['Ï£ºÎ¨∏ÏùºÏûê'], format='%Y%m%d', errors='coerce')
        
        return df_records
    except:
        return pd.DataFrame()

def create_analytics_dashboard(df_records):
    """Î∂ÑÏÑù ÎåÄÏãúÎ≥¥Îìú ÏÉùÏÑ±"""
    if df_records.empty:
        st.warning("Î∂ÑÏÑùÌï† Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.")
        return
    
    # ÎÇ†ÏßúÎ≥Ñ ÏßëÍ≥Ñ
    df_daily = df_records.groupby('Ï£ºÎ¨∏ÏùºÏûê').agg({
        'Ïã§Í≤∞Ï†úÍ∏àÏï°': 'sum',
        'Ï£ºÎ¨∏ÏàòÎüâ': 'sum',
        'ÏàòÎ†πÏûêÎ™Ö': 'nunique'
    }).reset_index()
    df_daily.columns = ['ÎÇ†Ïßú', 'Îß§Ï∂úÏï°', 'ÌåêÎß§ÏàòÎüâ', 'Í≥†Í∞ùÏàò']
    
    # ÏÉÅÌíàÎ≥Ñ ÌåêÎß§ TOP 10
    df_product_top = df_records.groupby('SKUÏÉÅÌíàÎ™Ö').agg({
        'Ï£ºÎ¨∏ÏàòÎüâ': 'sum',
        'Ïã§Í≤∞Ï†úÍ∏àÏï°': 'sum'
    }).nlargest(10, 'Ï£ºÎ¨∏ÏàòÎüâ').reset_index()
    
    # Ï±ÑÎÑêÎ≥Ñ Îß§Ï∂ú
    df_channel = df_records.groupby('ÏáºÌïëÎ™∞')['Ïã§Í≤∞Ï†úÍ∏àÏï°'].sum().reset_index()
    
    # ÎåÄÏãúÎ≥¥Îìú Î†àÏù¥ÏïÑÏõÉ
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        total_revenue = df_records['Ïã§Í≤∞Ï†úÍ∏àÏï°'].sum()
        st.metric("Ï¥ù Îß§Ï∂ú", f"‚Ç©{total_revenue:,.0f}")
    
    with col2:
        total_orders = len(df_records)
        st.metric("Ï¥ù Ï£ºÎ¨∏Ïàò", f"{total_orders:,}")
    
    with col3:
        avg_order_value = total_revenue / total_orders if total_orders > 0 else 0
        st.metric("ÌèâÍ∑† Ï£ºÎ¨∏ Í∏àÏï°", f"‚Ç©{avg_order_value:,.0f}")
    
    with col4:
        unique_customers = df_records['ÏàòÎ†πÏûêÎ™Ö'].nunique()
        st.metric("Í≥†Í∞ùÏàò", f"{unique_customers:,}")
    
    # Ï∞®Ìä∏ ÏÉùÏÑ±
    tab1, tab2, tab3, tab4 = st.tabs(["üìà ÏùºÎ≥Ñ Ìä∏Î†åÎìú", "üèÜ Î≤†Ïä§Ìä∏ÏÖÄÎü¨", "üõí Ï±ÑÎÑê Î∂ÑÏÑù", "ü§ñ AI Ïù∏ÏÇ¨Ïù¥Ìä∏"])
    
    with tab1:
        if not df_daily.empty:
            fig_trend = go.Figure()
            fig_trend.add_trace(go.Scatter(
                x=df_daily['ÎÇ†Ïßú'], 
                y=df_daily['Îß§Ï∂úÏï°'],
                mode='lines+markers',
                name='Îß§Ï∂úÏï°',
                line=dict(color='#1f77b4', width=2)
            ))
            fig_trend.update_layout(
                title="ÏùºÎ≥Ñ Îß§Ï∂ú Ìä∏Î†åÎìú",
                xaxis_title="ÎÇ†Ïßú",
                yaxis_title="Îß§Ï∂úÏï° (Ïõê)",
                hovermode='x unified'
            )
            st.plotly_chart(fig_trend, use_container_width=True)
    
    with tab2:
        if not df_product_top.empty:
            fig_products = px.bar(
                df_product_top, 
                x='Ï£ºÎ¨∏ÏàòÎüâ', 
                y='SKUÏÉÅÌíàÎ™Ö',
                orientation='h',
                title="ÏÉÅÌíàÎ≥Ñ ÌåêÎß§ ÏàòÎüâ TOP 10",
                color='Ïã§Í≤∞Ï†úÍ∏àÏï°',
                color_continuous_scale='Blues'
            )
            st.plotly_chart(fig_products, use_container_width=True)
    
    with tab3:
        if not df_channel.empty:
            fig_channel = px.pie(
                df_channel, 
                values='Ïã§Í≤∞Ï†úÍ∏àÏï°', 
                names='ÏáºÌïëÎ™∞',
                title="Ï±ÑÎÑêÎ≥Ñ Îß§Ï∂ú ÎπÑÏ§ë"
            )
            st.plotly_chart(fig_channel, use_container_width=True)
    
    with tab4:
        with st.spinner("AIÍ∞Ä Îç∞Ïù¥ÌÑ∞Î•º Î∂ÑÏÑù Ï§ëÏûÖÎãàÎã§..."):
            ai_insights = analyze_sales_data_with_ai(df_records)
            if ai_insights:
                st.markdown("### ü§ñ AI ÌåêÎß§ Î∂ÑÏÑù Î¶¨Ìè¨Ìä∏")
                st.markdown(ai_insights)
            else:
                st.info("AI Î∂ÑÏÑùÏùÑ ÏÇ¨Ïö©Ìï† Ïàò ÏóÜÏäµÎãàÎã§.")

# --------------------------------------------------------------------------
# Í∏∞Ï°¥ Ìï®ÏàòÎì§ (ÏàòÏ†ï ÏóÜÏùå)
# --------------------------------------------------------------------------

def to_excel_formatted(df, format_type=None):
    """Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏùÑ ÏÑúÏãùÏù¥ Ï†ÅÏö©Îêú ÏóëÏÖÄ ÌååÏùº ÌòïÏãùÏùò BytesIO Í∞ùÏ≤¥Î°ú Î≥ÄÌôòÌïòÎäî Ìï®Ïàò"""
    output = io.BytesIO()
    df_to_save = df.fillna('')
    
    if format_type == 'ecount_upload':
        df_to_save = df_to_save.rename(columns={'Ï†ÅÏöî_Ï†ÑÌëú': 'Ï†ÅÏöî', 'Ï†ÅÏöî_ÌíàÎ™©': 'Ï†ÅÏöî.1'})

    df_to_save.to_excel(output, index=False, sheet_name='Sheet1')
    
    workbook = openpyxl.load_workbook(output)
    sheet = workbook.active

    # Í≥µÌÜµ ÏÑúÏãù: Î™®Îì† ÏÖÄ Í∞ÄÏö¥Îç∞ Ï†ïÎ†¨
    center_alignment = Alignment(horizontal='center', vertical='center')
    for row in sheet.iter_rows():
        for cell in row:
            cell.alignment = center_alignment

    # ÌååÏùºÎ≥Ñ ÌäπÏàò ÏÑúÏãù
    for column_cells in sheet.columns:
        max_length = 0
        column = column_cells[0].column_letter
        for cell in column_cells:
            try:
                if cell.value:
                    if len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
            except:
                pass
        adjusted_width = (max_length + 2) * 1.2
        sheet.column_dimensions[column].width = adjusted_width
    
    thin_border = Border(left=Side(style='thin'), right=Side(style='thin'), top=Side(style='thin'), bottom=Side(style='thin'))
    pink_fill = PatternFill(start_color="FFEBEE", end_color="FFEBEE", fill_type="solid")

    if format_type == 'packing_list':
        for row in sheet.iter_rows(min_row=1, max_row=sheet.max_row, min_col=1, max_col=sheet.max_column):
            for cell in row:
                cell.border = thin_border
        
        bundle_start_row = 2
        for row_num in range(2, sheet.max_row + 2):
            current_bundle_cell = sheet.cell(row=row_num, column=1) if row_num <= sheet.max_row else None
            
            if (current_bundle_cell and current_bundle_cell.value) or (row_num > sheet.max_row):
                if row_num > 2:
                    bundle_end_row = row_num - 1
                    prev_bundle_num_str = str(sheet.cell(row=bundle_start_row, column=1).value)
                    
                    if prev_bundle_num_str.isdigit():
                        prev_bundle_num = int(prev_bundle_num_str)
                        if prev_bundle_num % 2 != 0:
                            for r in range(bundle_start_row, bundle_end_row + 1):
                                for c in range(1, sheet.max_column + 1):
                                    sheet.cell(row=r, column=c).fill = pink_fill
                    
                    if bundle_start_row < bundle_end_row:
                        sheet.merge_cells(start_row=bundle_start_row, start_column=1, end_row=bundle_end_row, end_column=1)
                        sheet.merge_cells(start_row=bundle_start_row, start_column=4, end_row=bundle_end_row, end_column=4)
                
                bundle_start_row = row_num

    if format_type == 'quantity_summary':
        for row_idx, row in enumerate(sheet.iter_rows(min_row=1, max_row=sheet.max_row, min_col=1, max_col=sheet.max_column)):
            for cell in row:
                cell.border = thin_border
            if row_idx > 0 and row_idx % 2 != 0:
                for cell in row:
                    cell.fill = pink_fill
    
    final_output = io.BytesIO()
    workbook.save(final_output)
    final_output.seek(0)
    
    return final_output.getvalue()

def process_all_files(file1, file2, file3, df_master):
    try:
        df_smartstore = pd.read_excel(file1)
        df_ecount_orig = pd.read_excel(file2)
        df_godomall = pd.read_excel(file3)

        df_ecount_orig['original_order'] = range(len(df_ecount_orig))
        
        # Ïª¨ÎüºÎ™Ö Ìò∏ÌôòÏÑ± Ï≤òÎ¶¨
        if 'Ìöå Ìï†Ïù∏ Í∏àÏï°' in df_godomall.columns and 'ÌöåÏõê Ìï†Ïù∏ Í∏àÏï°' not in df_godomall.columns:
            df_godomall.rename(columns={'Ìöå Ìï†Ïù∏ Í∏àÏï°': 'ÌöåÏõê Ìï†Ïù∏ Í∏àÏï°'}, inplace=True)
        if 'ÏûêÏ≤¥ÏòµÏÖòÏΩîÎìú' in df_godomall.columns:
            df_godomall.rename(columns={'ÏûêÏ≤¥ÏòµÏÖòÏΩîÎìú': 'Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú'}, inplace=True)
        
        # 1Îã®Í≥Ñ: Îç∞Ïù¥ÌÑ∞ ÌÅ¥Î¶¨Îãù Í∞ïÌôî
        cols_to_numeric = ['ÏÉÅÌíàÎ≥Ñ ÌíàÎ™©Í∏àÏï°', 'Ï¥ù Î∞∞ÏÜ° Í∏àÏï°', 'ÌöåÏõê Ìï†Ïù∏ Í∏àÏï°', 'Ïø†Ìè∞ Ìï†Ïù∏ Í∏àÏï°', 'ÏÇ¨Ïö©Îêú ÎßàÏùºÎ¶¨ÏßÄ', 'Ï¥ù Í≤∞Ï†ú Í∏àÏï°']
        for col in cols_to_numeric:
            if col in df_godomall.columns: 
                df_godomall[col] = pd.to_numeric(df_godomall[col].astype(str).str.replace('[Ïõê,]', '', regex=True), errors='coerce').fillna(0)
        
        # 2Îã®Í≥Ñ: Î∞∞ÏÜ°ÎπÑ Ï§ëÎ≥µ Í≥ÑÏÇ∞ Î∞©ÏßÄ
        df_godomall['Î≥¥Ï†ïÎêú_Î∞∞ÏÜ°ÎπÑ'] = np.where(
            df_godomall.duplicated(subset=['ÏàòÏ∑®Ïù∏ Ïù¥Î¶Ñ']), 
            0, 
            df_godomall['Ï¥ù Î∞∞ÏÜ° Í∏àÏï°']
        )
        
        df_godomall['ÏàòÏ†ïÎê†_Í∏àÏï°_Í≥†ÎèÑÎ™∞'] = (
            df_godomall['ÏÉÅÌíàÎ≥Ñ ÌíàÎ™©Í∏àÏï°'] + df_godomall['Î≥¥Ï†ïÎêú_Î∞∞ÏÜ°ÎπÑ'] - df_godomall['ÌöåÏõê Ìï†Ïù∏ Í∏àÏï°'] - 
            df_godomall['Ïø†Ìè∞ Ìï†Ïù∏ Í∏àÏï°'] - df_godomall['ÏÇ¨Ïö©Îêú ÎßàÏùºÎ¶¨ÏßÄ']
        )
        
        # 3Îã®Í≥Ñ: Í≤∞Ï†ú Í∏àÏï° Í≤ÄÏ¶ù Î∞è ÏïåÎ¶º Í∏∞Îä• Ï∂îÍ∞Ä
        godomall_warnings = []
        grouped_godomall = df_godomall.groupby('ÏàòÏ∑®Ïù∏ Ïù¥Î¶Ñ')
        
        for name, group in grouped_godomall:
            calculated_total = group['ÏàòÏ†ïÎê†_Í∏àÏï°_Í≥†ÎèÑÎ™∞'].sum()
            actual_total = group['Ï¥ù Í≤∞Ï†ú Í∏àÏï°'].iloc[0]
            discrepancy = calculated_total - actual_total
            
            if abs(discrepancy) > 1:
                warning_msg = f"- [Í≥†ÎèÑÎ™∞ Í∏àÏï° Î∂àÏùºÏπò] **{name}**ÎãòÏùò Ï£ºÎ¨∏Ïùò Í≥ÑÏÇ∞Îêú Í∏àÏï°Í≥º Ïã§Ï†ú Í≤∞Ï†ú Í∏àÏï°Ïù¥ **{discrepancy:,.0f}Ïõê** ÎßåÌÅº Ï∞®Ïù¥ÎÇ©ÎãàÎã§."
                godomall_warnings.append(warning_msg)

        # Í∏∞Ï°¥ Ï≤òÎ¶¨ Î°úÏßÅ
        df_final = df_ecount_orig.copy().rename(columns={'Í∏àÏï°': 'Ïã§Í≤∞Ï†úÍ∏àÏï°'})
        
        # Ïä§ÎßàÌä∏Ïä§ÌÜ†Ïñ¥ Î≥ëÌï© Ï§ÄÎπÑ
        key_cols_smartstore = ['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú', 'Ï£ºÎ¨∏ÏàòÎüâ', 'ÏàòÎ†πÏûêÎ™Ö']
        smartstore_prices = df_smartstore.rename(columns={'Ïã§Í≤∞Ï†úÍ∏àÏï°': 'ÏàòÏ†ïÎê†_Í∏àÏï°_Ïä§ÌÜ†Ïñ¥'})[key_cols_smartstore + ['ÏàòÏ†ïÎê†_Í∏àÏï°_Ïä§ÌÜ†Ïñ¥']].drop_duplicates(subset=key_cols_smartstore, keep='first')
        
        key_cols_godomall = ['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú', 'ÏàòÏ∑®Ïù∏ Ïù¥Î¶Ñ', 'ÏÉÅÌíàÏàòÎüâ']
        godomall_prices_for_merge = df_godomall[key_cols_godomall + ['ÏàòÏ†ïÎê†_Í∏àÏï°_Í≥†ÎèÑÎ™∞']].rename(
            columns={'ÏàòÏ∑®Ïù∏ Ïù¥Î¶Ñ': 'ÏàòÎ†πÏûêÎ™Ö', 'ÏÉÅÌíàÏàòÎüâ': 'Ï£ºÎ¨∏ÏàòÎüâ'}
        )
        godomall_prices_for_merge = godomall_prices_for_merge.drop_duplicates(
            subset=['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú', 'ÏàòÎ†πÏûêÎ™Ö', 'Ï£ºÎ¨∏ÏàòÎüâ'], keep='first'
        )
        
        # Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ ÌÜµÏùº
        for col in ['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú', 'ÏàòÎ†πÏûêÎ™Ö']:
            df_final[col] = df_final[col].astype(str).str.strip()
            smartstore_prices[col] = smartstore_prices[col].astype(str).str.strip()
            godomall_prices_for_merge[col] = godomall_prices_for_merge[col].astype(str).str.strip()
        
        for col in ['Ï£ºÎ¨∏ÏàòÎüâ']:
            df_final[col] = pd.to_numeric(df_final[col], errors='coerce').fillna(0).astype(int)
            smartstore_prices[col] = pd.to_numeric(smartstore_prices[col], errors='coerce').fillna(0).astype(int)
            godomall_prices_for_merge[col] = pd.to_numeric(godomall_prices_for_merge[col], errors='coerce').fillna(0).astype(int)

        df_final['Ïã§Í≤∞Ï†úÍ∏àÏï°'] = pd.to_numeric(df_final['Ïã§Í≤∞Ï†úÍ∏àÏï°'], errors='coerce').fillna(0).astype(int)
        
        # Îç∞Ïù¥ÌÑ∞ Î≥ëÌï©
        df_final = pd.merge(df_final, smartstore_prices, on=key_cols_smartstore, how='left')
        df_final = pd.merge(df_final, godomall_prices_for_merge, 
                            on=['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú', 'ÏàòÎ†πÏûêÎ™Ö', 'Ï£ºÎ¨∏ÏàòÎüâ'], 
                            how='left')

        # Í≤ΩÍ≥† Î©îÏãúÏßÄ ÏÉùÏÑ±
        warnings = [f"- [Í∏àÏï°Î≥¥Ï†ï Ïã§Ìå®] **{row['ÏáºÌïëÎ™∞']}** / {row['ÏàòÎ†πÏûêÎ™Ö']} / {row['SKUÏÉÅÌíàÎ™Ö']}" 
                   for _, row in df_final[(df_final['ÏáºÌïëÎ™∞'] == 'Ïä§ÎßàÌä∏Ïä§ÌÜ†Ïñ¥') & (df_final['ÏàòÏ†ïÎê†_Í∏àÏï°_Ïä§ÌÜ†Ïñ¥'].isna()) | 
                                          (df_final['ÏáºÌïëÎ™∞'] == 'Í≥†ÎèÑÎ™∞5') & (df_final['ÏàòÏ†ïÎê†_Í∏àÏï°_Í≥†ÎèÑÎ™∞'].isna())].iterrows()]
        warnings.extend(godomall_warnings)

        # ÏµúÏ¢Ö Í≤∞Ï†ú Í∏àÏï° ÏóÖÎç∞Ïù¥Ìä∏
        df_final['Ïã§Í≤∞Ï†úÍ∏àÏï°'] = np.where(df_final['ÏáºÌïëÎ™∞'] == 'Í≥†ÎèÑÎ™∞5', 
                                          df_final['ÏàòÏ†ïÎê†_Í∏àÏï°_Í≥†ÎèÑÎ™∞'].fillna(df_final['Ïã§Í≤∞Ï†úÍ∏àÏï°']), 
                                          df_final['Ïã§Í≤∞Ï†úÍ∏àÏï°'])
        df_final['Ïã§Í≤∞Ï†úÍ∏àÏï°'] = np.where(df_final['ÏáºÌïëÎ™∞'] == 'Ïä§ÎßàÌä∏Ïä§ÌÜ†Ïñ¥', 
                                          df_final['ÏàòÏ†ïÎê†_Í∏àÏï°_Ïä§ÌÜ†Ïñ¥'].fillna(df_final['Ïã§Í≤∞Ï†úÍ∏àÏï°']), 
                                          df_final['Ïã§Í≤∞Ï†úÍ∏àÏï°'])
        
        df_main_result = df_final[['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú', 'SKUÏÉÅÌíàÎ™Ö', 'Ï£ºÎ¨∏ÏàòÎüâ', 'Ïã§Í≤∞Ï†úÍ∏àÏï°', 'ÏáºÌïëÎ™∞', 'ÏàòÎ†πÏûêÎ™Ö', 'original_order']]
        
        # ÎèôÎ™ÖÏù¥Ïù∏ Í≤ΩÍ≥† Ï∂îÍ∞Ä
        homonym_warnings = []
        name_groups = df_main_result.groupby('ÏàòÎ†πÏûêÎ™Ö')['original_order'].apply(list)
        for name, orders in name_groups.items():
            if len(orders) > 1 and (max(orders) - min(orders) + 1) != len(orders):
                homonym_warnings.append(f"- [ÎèôÎ™ÖÏù¥Ïù∏ ÏùòÏã¨] **{name}** ÎãòÏùò Ï£ºÎ¨∏Ïù¥ Îñ®Ïñ¥Ï†∏ÏÑú ÏûÖÎ†•ÎêòÏóàÏäµÎãàÎã§.")
        warnings.extend(homonym_warnings)

        # ÏàòÎüâ ÏöîÏïΩ Î∞è Ìè¨Ïû• Î¶¨Ïä§Ìä∏ ÏÉùÏÑ±
        df_quantity_summary = df_main_result.groupby('SKUÏÉÅÌíàÎ™Ö', as_index=False)['Ï£ºÎ¨∏ÏàòÎüâ'].sum().rename(columns={'Ï£ºÎ¨∏ÏàòÎüâ': 'Í∞úÏàò'})
        df_packing_list = df_main_result.sort_values(by='original_order')[['SKUÏÉÅÌíàÎ™Ö', 'Ï£ºÎ¨∏ÏàòÎüâ', 'ÏàòÎ†πÏûêÎ™Ö', 'ÏáºÌïëÎ™∞']].copy()
        is_first_item = df_packing_list['ÏàòÎ†πÏûêÎ™Ö'] != df_packing_list['ÏàòÎ†πÏûêÎ™Ö'].shift(1)
        df_packing_list['Î¨∂ÏùåÎ≤àÌò∏'] = is_first_item.cumsum()
        df_packing_list_final = df_packing_list.copy()
        df_packing_list_final['Î¨∂ÏùåÎ≤àÌò∏'] = df_packing_list_final['Î¨∂ÏùåÎ≤àÌò∏'].where(is_first_item, '')
        df_packing_list_final = df_packing_list_final[['Î¨∂ÏùåÎ≤àÌò∏', 'SKUÏÉÅÌíàÎ™Ö', 'Ï£ºÎ¨∏ÏàòÎüâ', 'ÏàòÎ†πÏûêÎ™Ö', 'ÏáºÌïëÎ™∞']]

        # Ïù¥Ïπ¥Ïö¥Ìä∏ ÏóÖÎ°úÎìú Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±
        df_merged = pd.merge(df_main_result, df_master[['SKUÏΩîÎìú', 'Í≥ºÏÑ∏Ïó¨Î∂Ä', 'ÏûÖÏàòÎüâ']], 
                            left_on='Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú', right_on='SKUÏΩîÎìú', how='left')
        
        unmastered = df_merged[df_merged['SKUÏΩîÎìú'].isna()]
        for _, row in unmastered.iterrows():
            warnings.append(f"- [ÎØ∏Îì±Î°ù ÏÉÅÌíà] **{row['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú']}** / {row['SKUÏÉÅÌíàÎ™Ö']}")

        client_map = {
            'Ïø†Ìå°': 'Ïø†Ìå° Ï£ºÏãùÌöåÏÇ¨', 
            'Í≥†ÎèÑÎ™∞5': 'Í≥†ÎûòÎØ∏ÏûêÏÇ¨Î™∞_ÌòÑÍ∏àÏòÅÏàòÏ¶ù(Í≥†ÎèÑÎ™∞)', 
            'Ïä§ÎßàÌä∏Ïä§ÌÜ†Ïñ¥': 'Ïä§ÌÜ†Ïñ¥Ìåú',
            'Î∞∞ÎØºÏÉÅÌöå': 'Ï£ºÏãùÌöåÏÇ¨ Ïö∞ÏïÑÌïúÌòïÏ†úÎì§(Î∞∞ÎØºÏÉÅÌöå)',
            'Ïù¥ÏßÄÏõ∞Î™∞': 'Ï£ºÏãùÌöåÏÇ¨ ÌòÑÎåÄÏù¥ÏßÄÏõ∞'
        }
        
        df_ecount_upload = pd.DataFrame()
        
        df_ecount_upload['ÏùºÏûê'] = datetime.now().strftime("%Y%m%d")
        df_ecount_upload['Í±∞ÎûòÏ≤òÎ™Ö'] = df_merged['ÏáºÌïëÎ™∞'].map(client_map).fillna(df_merged['ÏáºÌïëÎ™∞'])
        df_ecount_upload['Ï∂úÌïòÏ∞ΩÍ≥†'] = 'Í≥†ÎûòÎØ∏'
        df_ecount_upload['Í±∞ÎûòÏú†Ìòï'] = np.where(df_merged['Í≥ºÏÑ∏Ïó¨Î∂Ä'] == 'Î©¥ÏÑ∏', 12, 11)
        df_ecount_upload['Ï†ÅÏöî_Ï†ÑÌëú'] = 'Ïò§Ï†Ñ/Ïò®ÎùºÏù∏'
        df_ecount_upload['ÌíàÎ™©ÏΩîÎìú'] = df_merged['Ïû¨Í≥†Í¥ÄÎ¶¨ÏΩîÎìú']
        
        is_box_order = df_merged['SKUÏÉÅÌíàÎ™Ö'].str.contains("BOX", na=False)
        ÏûÖÏàòÎüâ = pd.to_numeric(df_merged['ÏûÖÏàòÎüâ'], errors='coerce').fillna(1)
        base_quantity = np.where(is_box_order, df_merged['Ï£ºÎ¨∏ÏàòÎüâ'] * ÏûÖÏàòÎüâ, df_merged['Ï£ºÎ¨∏ÏàòÎüâ'])
        is_3_pack = df_merged['SKUÏÉÅÌíàÎ™Ö'].str.contains("3Í∞úÏûÖ|3Í∞ú", na=False)
        final_quantity = np.where(is_3_pack, base_quantity * 3, base_quantity)
        df_ecount_upload['Î∞ïÏä§'] = np.where(is_box_order, df_merged['Ï£ºÎ¨∏ÏàòÎüâ'], np.nan)
        df_ecount_upload['ÏàòÎüâ'] = final_quantity.astype(int)
        
        df_merged['Ïã§Í≤∞Ï†úÍ∏àÏï°'] = pd.to_numeric(df_merged['Ïã§Í≤∞Ï†úÍ∏àÏï°'], errors='coerce').fillna(0)
        Í≥µÍ∏âÍ∞ÄÏï° = np.where(df_merged['Í≥ºÏÑ∏Ïó¨Î∂Ä'] == 'Í≥ºÏÑ∏', df_merged['Ïã§Í≤∞Ï†úÍ∏àÏï°'] / 1.1, df_merged['Ïã§Í≤∞Ï†úÍ∏àÏï°'])
        df_ecount_upload['Í≥µÍ∏âÍ∞ÄÏï°'] = Í≥µÍ∏âÍ∞ÄÏï°
        df_ecount_upload['Î∂ÄÍ∞ÄÏÑ∏'] = df_merged['Ïã§Í≤∞Ï†úÍ∏àÏï°'] - df_ecount_upload['Í≥µÍ∏âÍ∞ÄÏï°']
        
        df_ecount_upload['ÏáºÌïëÎ™∞Í≥†Í∞ùÎ™Ö'] = df_merged['ÏàòÎ†πÏûêÎ™Ö']
        df_ecount_upload['original_order'] = df_merged['original_order']
        
        ecount_columns = [
            'ÏùºÏûê', 'ÏàúÎ≤à', 'Í±∞ÎûòÏ≤òÏΩîÎìú', 'Í±∞ÎûòÏ≤òÎ™Ö', 'Îã¥ÎãπÏûê', 'Ï∂úÌïòÏ∞ΩÍ≥†', 'Í±∞ÎûòÏú†Ìòï', 'ÌÜµÌôî', 'ÌôòÏú®', 
            'Ï†ÅÏöî_Ï†ÑÌëú', 'ÎØ∏ÏàòÍ∏à', 'Ï¥ùÌï©Í≥Ñ', 'Ïó∞Í≤∞Ï†ÑÌëú', 'ÌíàÎ™©ÏΩîÎìú', 'ÌíàÎ™©Î™Ö', 'Í∑úÍ≤©', 'Î∞ïÏä§', 'ÏàòÎüâ', 
            'Îã®Í∞Ä', 'Ïô∏ÌôîÍ∏àÏï°', 'Í≥µÍ∏âÍ∞ÄÏï°', 'Î∂ÄÍ∞ÄÏÑ∏', 'Ï†ÅÏöî_ÌíàÎ™©', 'ÏÉùÏÇ∞Ï†ÑÌëúÏÉùÏÑ±', 'ÏãúÎ¶¨Ïñº/Î°úÌä∏', 
            'Í¥ÄÎ¶¨Ìï≠Î™©', 'ÏáºÌïëÎ™∞Í≥†Í∞ùÎ™Ö', 'original_order'
        ]
        for col in ecount_columns:
            if col not in df_ecount_upload:
                df_ecount_upload[col] = ''
        
        for col in ['Í≥µÍ∏âÍ∞ÄÏï°', 'Î∂ÄÍ∞ÄÏÑ∏']:
            df_ecount_upload[col] = df_ecount_upload[col].round().astype('Int64')
        
        df_ecount_upload['Í±∞ÎûòÏú†Ìòï'] = pd.to_numeric(df_ecount_upload['Í±∞ÎûòÏú†Ìòï'])
        
        sort_order = [
            'Í≥†ÎûòÎØ∏ÏûêÏÇ¨Î™∞_ÌòÑÍ∏àÏòÅÏàòÏ¶ù(Í≥†ÎèÑÎ™∞)', 
            'Ïä§ÌÜ†Ïñ¥Ìåú', 
            'Ïø†Ìå° Ï£ºÏãùÌöåÏÇ¨',
            'Ï£ºÏãùÌöåÏÇ¨ Ïö∞ÏïÑÌïúÌòïÏ†úÎì§(Î∞∞ÎØºÏÉÅÌöå)',
            'Ï£ºÏãùÌöåÏÇ¨ ÌòÑÎåÄÏù¥ÏßÄÏõ∞'
        ]
        
        df_ecount_upload['Í±∞ÎûòÏ≤òÎ™Ö_sort'] = pd.Categorical(df_ecount_upload['Í±∞ÎûòÏ≤òÎ™Ö'], categories=sort_order, ordered=True)
        
        df_ecount_upload = df_ecount_upload.sort_values(
            by=['Í±∞ÎûòÏ≤òÎ™Ö_sort', 'Í±∞ÎûòÏú†Ìòï', 'original_order'],
            ascending=[True, True, True]
        ).drop(columns=['Í±∞ÎûòÏ≤òÎ™Ö_sort', 'original_order'])
        
        df_ecount_upload = df_ecount_upload[ecount_columns[:-1]]

        return df_main_result.drop(columns=['original_order']), df_quantity_summary, df_packing_list_final, df_ecount_upload, True, "Î™®Îì† ÌååÏùº Ï≤òÎ¶¨Í∞Ä ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§.", warnings

    except Exception as e:
        import traceback
        st.error(f"Ï≤òÎ¶¨ Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {e}")
        st.error(traceback.format_exc())
        return None, None, None, None, False, f"Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {e}", []

# --------------------------------------------------------------------------
# Streamlit Ïï± UI Íµ¨ÏÑ±
# --------------------------------------------------------------------------
st.set_page_config(
    page_title="Ï£ºÎ¨∏ Ï≤òÎ¶¨ ÏûêÎèôÌôî Pro v2.0", 
    layout="wide",
    page_icon="üìä",
    initial_sidebar_state="expanded"
)

# ÏÇ¨Ïù¥ÎìúÎ∞î Î©îÎâ¥
with st.sidebar:
    st.title("üìä Order Pro v2.0")
    st.markdown("---")
    
    menu_option = st.radio(
        "Î©îÎâ¥ ÏÑ†ÌÉù",
        ["üìë Ï£ºÎ¨∏ Ï≤òÎ¶¨", "üìà ÌåêÎß§ Î∂ÑÏÑù", "‚öôÔ∏è ÏÑ§Ï†ï"],
        index=0
    )
    
    st.markdown("---")
    st.caption("SharePoint Ïó∞Îèô ÏÉÅÌÉú")
    try:
        ctx = init_sharepoint_context()
        if ctx:
            st.success("‚úÖ Ïó∞Í≤∞Îê®")
        else:
            st.error("‚ùå Ïó∞Í≤∞ Ïã§Ìå®")
    except:
        st.warning("‚ö†Ô∏è ÌôïÏù∏ ÌïÑÏöî")

# Î©îÏù∏ ÏΩòÌÖêÏ∏†
if menu_option == "üìë Ï£ºÎ¨∏ Ï≤òÎ¶¨":
    st.title("üìë Ï£ºÎ¨∏ Ï≤òÎ¶¨ ÏûêÎèôÌôî")
    st.info("üí° SharePointÏôÄ Ïó∞ÎèôÌïòÏó¨ ÎßàÏä§ÌÑ∞ Îç∞Ïù¥ÌÑ∞Î•º ÏûêÎèôÏúºÎ°ú Î∂àÎü¨Ïò§Í≥†, Ï≤òÎ¶¨ Í≤∞Í≥ºÎ•º ÏûêÎèô Ï†ÄÏû•Ìï©ÎãàÎã§.")
    
    st.write("---")
    st.header("1. ÏõêÎ≥∏ ÏóëÏÖÄ ÌååÏùº 3Í∞ú ÏóÖÎ°úÎìú")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        file1 = st.file_uploader("1Ô∏è‚É£ Ïä§ÎßàÌä∏Ïä§ÌÜ†Ïñ¥ (Í∏àÏï°ÌôïÏù∏Ïö©)", type=['xlsx', 'xls', 'csv'])
    with col2:
        file2 = st.file_uploader("2Ô∏è‚É£ Ïù¥Ïπ¥Ïö¥Ìä∏ Îã§Ïö¥Î°úÎìú (Ï£ºÎ¨∏Î™©Î°ù)", type=['xlsx', 'xls', 'csv'])
    with col3:
        file3 = st.file_uploader("3Ô∏è‚É£ Í≥†ÎèÑÎ™∞ (Í∏àÏï°ÌôïÏù∏Ïö©)", type=['xlsx', 'xls', 'csv'])
    
    st.write("---")
    st.header("2. Ï≤òÎ¶¨ Í≤∞Í≥º ÌôïÏù∏ Î∞è Îã§Ïö¥Î°úÎìú")
    
    if st.button("üöÄ Î™®Îì† Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ Î∞è ÌååÏùº ÏÉùÏÑ± Ïã§Ìñâ", type="primary"):
        if file1 and file2 and file3:
            try:
                # SharePointÏóêÏÑú ÎßàÏä§ÌÑ∞ Îç∞Ïù¥ÌÑ∞ Î°úÎìú
                with st.spinner('SharePointÏóêÏÑú ÎßàÏä§ÌÑ∞ Îç∞Ïù¥ÌÑ∞Î•º Î∂àÎü¨Ïò§Îäî Ï§ë...'):
                    df_master = load_master_data_from_sharepoint()
                
                if df_master.empty:
                    st.error("ÎßàÏä§ÌÑ∞ Îç∞Ïù¥ÌÑ∞Î•º Î∂àÎü¨Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§.")
                else:
                    with st.spinner('Î™®Îì† ÌååÏùºÏùÑ Ï≤òÎ¶¨ÌïòÎäî Ï§ëÏûÖÎãàÎã§...'):
                        df_main, df_qty, df_pack, df_ecount, success, message, warnings = process_all_files(
                            file1, file2, file3, df_master
                        )
                    
                    if success:
                        st.success(message)
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        
                        # SharePointÏóê Í∏∞Î°ù Ï†ÄÏû•
                        with st.spinner('SharePointÏóê Ï≤òÎ¶¨ Í≤∞Í≥ºÎ•º Ï†ÄÏû•ÌïòÎäî Ï§ë...'):
                            save_success, save_message = save_to_sharepoint_records(df_main, df_ecount)
                            if save_success:
                                st.success(f"‚úÖ {save_message}")
                            else:
                                st.warning(f"‚ö†Ô∏è {save_message}")
                        
                        # Í≤ΩÍ≥† Î©îÏãúÏßÄ ÌëúÏãú
                        if warnings:
                            st.warning("‚ö†Ô∏è ÌôïÏù∏ ÌïÑÏöî Ìï≠Î™©")
                            with st.expander("ÏûêÏÑ∏Ìïú Î™©Î°ù Î≥¥Í∏∞..."):
                                for warning_message in warnings:
                                    st.markdown(warning_message)
                        
                        # Í≤∞Í≥º ÌÉ≠ ÌëúÏãú
                        tab_erp, tab_pack, tab_qty, tab_main = st.tabs([
                            "üè¢ **Ïù¥Ïπ¥Ïö¥Ìä∏ ÏóÖÎ°úÎìúÏö©**", 
                            "üìã Ìè¨Ïû• Î¶¨Ïä§Ìä∏", 
                            "üì¶ Ï∂úÍ≥†ÏàòÎüâ ÏöîÏïΩ", 
                            "‚úÖ ÏµúÏ¢Ö Î≥¥Ï†ï Î¶¨Ïä§Ìä∏"
                        ])
                        
                        with tab_erp:
                            st.dataframe(df_ecount.astype(str), use_container_width=True)
                            st.download_button(
                                "üì• Îã§Ïö¥Î°úÎìú", 
                                to_excel_formatted(df_ecount, format_type='ecount_upload'), 
                                f"Ïù¥Ïπ¥Ïö¥Ìä∏_ÏóÖÎ°úÎìúÏö©_{timestamp}.xlsx"
                            )
                        
                        with tab_pack:
                            st.dataframe(df_pack, use_container_width=True)
                            st.download_button(
                                "üì• Îã§Ïö¥Î°úÎìú", 
                                to_excel_formatted(df_pack, format_type='packing_list'), 
                                f"Î¨ºÎ•òÌåÄ_Ï†ÑÎã¨Ïö©_Ìè¨Ïû•Î¶¨Ïä§Ìä∏_{timestamp}.xlsx"
                            )
                        
                        with tab_qty:
                            st.dataframe(df_qty, use_container_width=True)
                            st.download_button(
                                "üì• Îã§Ïö¥Î°úÎìú", 
                                to_excel_formatted(df_qty, format_type='quantity_summary'), 
                                f"Î¨ºÎ•òÌåÄ_Ï†ÑÎã¨Ïö©_Ï∂úÍ≥†ÏàòÎüâ_{timestamp}.xlsx"
                            )
                        
                        with tab_main:
                            st.dataframe(df_main, use_container_width=True)
                            st.download_button(
                                "üì• Îã§Ïö¥Î°úÎìú", 
                                to_excel_formatted(df_main), 
                                f"ÏµúÏ¢Ö_Ïã§Í≤∞Ï†úÍ∏àÏï°_Î≥¥Ï†ïÏôÑÎ£å_{timestamp}.xlsx"
                            )
                    else:
                        st.error(message)
                        
            except Exception as e:
                st.error(f"üö® Ï≤òÎ¶¨ Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {e}")
        else:
            st.warning("‚ö†Ô∏è 3Í∞úÏùò ÏóëÏÖÄ ÌååÏùºÏùÑ Î™®Îëê ÏóÖÎ°úÎìúÌï¥Ïïº Ïã§ÌñâÌï† Ïàò ÏûàÏäµÎãàÎã§.")

elif menu_option == "üìà ÌåêÎß§ Î∂ÑÏÑù":
    st.title("üìà AI Í∏∞Î∞ò ÌåêÎß§ Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù")
    st.info("üí° SharePointÏóê Ï†ÄÏû•Îêú ÌåêÎß§ Í∏∞Î°ùÏùÑ Î∂ÑÏÑùÌïòÍ≥† AI Ïù∏ÏÇ¨Ïù¥Ìä∏Î•º Ï†úÍ≥µÌï©ÎãàÎã§.")
    
    # Î∂ÑÏÑù Í∏∞Í∞Ñ ÏÑ†ÌÉù
    col1, col2 = st.columns(2)
    with col1:
        analysis_period = st.selectbox(
            "Î∂ÑÏÑù Í∏∞Í∞Ñ",
            ["ÏµúÍ∑º 7Ïùº", "ÏµúÍ∑º 30Ïùº", "ÏµúÍ∑º 90Ïùº", "Ï†ÑÏ≤¥ Í∏∞Í∞Ñ", "ÏÇ¨Ïö©Ïûê ÏßÄÏ†ï"],
            index=1
        )
    
    with col2:
        if analysis_period == "ÏÇ¨Ïö©Ïûê ÏßÄÏ†ï":
            date_range = st.date_input(
                "ÎÇ†Ïßú Î≤îÏúÑ",
                value=(datetime.now() - timedelta(days=30), datetime.now()),
                max_value=datetime.now()
            )
    
    if st.button("üìä Î∂ÑÏÑù ÏãúÏûë", type="primary"):
        with st.spinner("SharePointÏóêÏÑú Îç∞Ïù¥ÌÑ∞Î•º Î∂àÎü¨Ïò§Îäî Ï§ë..."):
            df_records = load_record_data_from_sharepoint()
            
            if not df_records.empty:
                # Í∏∞Í∞Ñ ÌïÑÌÑ∞ÎßÅ
                if analysis_period != "Ï†ÑÏ≤¥ Í∏∞Í∞Ñ":
                    today = pd.Timestamp.now()
                    if analysis_period == "ÏµúÍ∑º 7Ïùº":
                        start_date = today - timedelta(days=7)
                    elif analysis_period == "ÏµúÍ∑º 30Ïùº":
                        start_date = today - timedelta(days=30)
                    elif analysis_period == "ÏµúÍ∑º 90Ïùº":
                        start_date = today - timedelta(days=90)
                    elif analysis_period == "ÏÇ¨Ïö©Ïûê ÏßÄÏ†ï":
                        start_date = pd.Timestamp(date_range[0])
                        today = pd.Timestamp(date_range[1])
                    
                    df_records = df_records[
                        (df_records['Ï£ºÎ¨∏ÏùºÏûê'] >= start_date) & 
                        (df_records['Ï£ºÎ¨∏ÏùºÏûê'] <= today)
                    ]
                
                if not df_records.empty:
                    create_analytics_dashboard(df_records)
                else:
                    st.warning("ÏÑ†ÌÉùÌïú Í∏∞Í∞ÑÏóê Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.")
            else:
                st.warning("Î∂ÑÏÑùÌï† Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§. Î®ºÏ†Ä Ï£ºÎ¨∏ Ï≤òÎ¶¨Î•º Ïã§ÌñâÌï¥Ï£ºÏÑ∏Ïöî.")

elif menu_option == "‚öôÔ∏è ÏÑ§Ï†ï":
    st.title("‚öôÔ∏è ÏãúÏä§ÌÖú ÏÑ§Ï†ï")
    
    st.header("SharePoint Ïó∞Í≤∞ Ï†ïÎ≥¥")
    col1, col2 = st.columns(2)
    
    with col1:
        st.text_input("Tenant ID", value=st.secrets["sharepoint"]["tenant_id"], disabled=True)
        st.text_input("Client ID", value=st.secrets["sharepoint"]["client_id"], disabled=True)
    
    with col2:
        st.text_input("Site Name", value=st.secrets["sharepoint_files"]["site_name"], disabled=True)
        st.text_input("Master File", value=st.secrets["sharepoint_files"]["file_name"], disabled=True)
    
    st.header("AI ÏÑ§Ï†ï")
    st.text_input("Gemini API Key", value=st.secrets["GEMINI_API_KEY"][:10] + "...", disabled=True)
    
    if st.button("üîÑ Ïó∞Í≤∞ ÌÖåÏä§Ìä∏"):
        with st.spinner("ÌÖåÏä§Ìä∏ Ï§ë..."):
            # SharePoint ÌÖåÏä§Ìä∏
            ctx = init_sharepoint_context()
            if ctx:
                st.success("‚úÖ SharePoint Ïó∞Í≤∞ ÏÑ±Í≥µ")
            else:
                st.error("‚ùå SharePoint Ïó∞Í≤∞ Ïã§Ìå®")
            
            # AI ÌÖåÏä§Ìä∏
            model = init_gemini()
            if model:
                st.success("‚úÖ Gemini AI Ïó∞Í≤∞ ÏÑ±Í≥µ")
            else:
                st.error("‚ùå Gemini AI Ïó∞Í≤∞ Ïã§Ìå®")
    
    st.header("Ï∫êÏãú Í¥ÄÎ¶¨")
    if st.button("üóëÔ∏è Ï∫êÏãú Ï¥àÍ∏∞Ìôî"):
        st.cache_data.clear()
        st.cache_resource.clear()
        st.success("Ï∫êÏãúÍ∞Ä Ï¥àÍ∏∞ÌôîÎêòÏóàÏäµÎãàÎã§.")
        st.rerun()
