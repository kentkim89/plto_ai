import streamlit as st
import pandas as pd
import io
import numpy as np
import openpyxl
from openpyxl.styles import PatternFill, Border, Side, Alignment
from datetime import datetime
import json
import traceback
import requests
from io import BytesIO
import base64

# --------------------------------------------------------------------------
# í˜ì´ì§€ ì„¤ì • (ê°€ì¥ ë¨¼ì € ì‹¤í–‰)
# --------------------------------------------------------------------------
st.set_page_config(
    page_title="ì£¼ë¬¸ ì²˜ë¦¬ ìë™í™” Pro v2.3",
    layout="wide",
    page_icon="ğŸ“Š",
    initial_sidebar_state="expanded"
)

# --------------------------------------------------------------------------
# ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ìš©ì„± ì²´í¬
# --------------------------------------------------------------------------

# Microsoft Graph API ì‚¬ìš©
GRAPH_AVAILABLE = False
try:
    import msal
    GRAPH_AVAILABLE = True
except ImportError:
    pass

# Plotly ì‚¬ìš©
PLOTLY_AVAILABLE = False
try:
    import plotly.express as px
    import plotly.graph_objects as go
    PLOTLY_AVAILABLE = True
except ImportError:
    pass

# Gemini AI ì‚¬ìš©
GEMINI_AVAILABLE = False
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    pass

# --------------------------------------------------------------------------
# Microsoft Graph API ì—°ê²° í•¨ìˆ˜
# --------------------------------------------------------------------------

@st.cache_resource
def get_graph_token():
    """Microsoft Graph API í† í° íšë“"""
    if not GRAPH_AVAILABLE:
        return None
    
    try:
        if "sharepoint" not in st.secrets:
            return None
        
        tenant_id = st.secrets["sharepoint"]["tenant_id"]
        client_id = st.secrets["sharepoint"]["client_id"]
        client_secret = st.secrets["sharepoint"]["client_secret"]
        
        authority = f"https://login.microsoftonline.com/{tenant_id}"
        app = msal.ConfidentialClientApplication(
            client_id,
            authority=authority,
            client_credential=client_secret
        )
        
        result = app.acquire_token_silent(["https://graph.microsoft.com/.default"], account=None)
        if not result:
            result = app.acquire_token_for_client(scopes=["https://graph.microsoft.com/.default"])
        
        if "access_token" in result:
            return result["access_token"]
        else:
            st.error(f"í† í° íšë“ ì‹¤íŒ¨: {result.get('error_description', 'Unknown error')}")
            return None
            
    except Exception as e:
        st.error(f"Graph API ì—°ê²° ì‹¤íŒ¨: {e}")
        return None

@st.cache_data(ttl=600)
def load_master_data_from_sharepoint():
    """Microsoft Graph APIë¥¼ í†µí•´ SharePointì—ì„œ ë§ˆìŠ¤í„° ë°ì´í„° ë¡œë“œ"""
    if not GRAPH_AVAILABLE:
        return pd.DataFrame()

    try:
        token = get_graph_token()
        if not token:
            return pd.DataFrame()
        
        headers = {'Authorization': f'Bearer {token}', 'Accept': 'application/json'}
        site_url = "https://graph.microsoft.com/v1.0/sites/goremi.sharepoint.com:/sites/data"
        site_response = requests.get(site_url, headers=headers)
        
        if site_response.status_code == 200:
            site_id = site_response.json()['id']
            drives_url = f"https://graph.microsoft.com/v1.0/sites/{site_id}/drives"
            drives_response = requests.get(drives_url, headers=headers)
            
            if drives_response.status_code == 200:
                drives = drives_response.json()['value']
                for drive in drives:
                    drive_id = drive['id']
                    search_url = f"https://graph.microsoft.com/v1.0/drives/{drive_id}/root/search(q='plto_master_data.xlsx')"
                    search_response = requests.get(search_url, headers=headers)
                    
                    if search_response.status_code == 200:
                        items = search_response.json().get('value', [])
                        for item in items:
                            if item['name'] == 'plto_master_data.xlsx':
                                download_url = f"https://graph.microsoft.com/v1.0/drives/{drive_id}/items/{item['id']}/content"
                                file_response = requests.get(download_url, headers=headers)
                                if file_response.status_code == 200:
                                    st.session_state['sharepoint_site_id'] = site_id
                                    st.session_state['sharepoint_drive_id'] = drive_id
                                    df_master = pd.read_excel(io.BytesIO(file_response.content))
                                    df_master = df_master.drop_duplicates(subset=['SKUì½”ë“œ'], keep='first')
                                    st.success("âœ… Microsoft Graph APIë¡œ ë§ˆìŠ¤í„° ë°ì´í„° ë¡œë“œ ì„±ê³µ!")
                                    return df_master
        st.error("Graph APIë¥¼ í†µí•´ ë§ˆìŠ¤í„° ë°ì´í„°ë¥¼ ì°¾ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        st.error(f"âŒ ë§ˆìŠ¤í„° ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return pd.DataFrame()

def save_to_sharepoint_records(df_main_result, df_ecount_upload):
    """Microsoft Graph APIë¥¼ í†µí•´ ì²˜ë¦¬ ê²°ê³¼ë¥¼ SharePointì— ê¸°ë¡/ëˆ„ì í•©ë‹ˆë‹¤."""
    if not GRAPH_AVAILABLE:
        st.info("Graph APIê°€ í™œì„±í™”ë˜ì§€ ì•Šì•„ SharePointì— ìë™ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False, "Graph API ë¹„í™œì„±í™”"
    
    token = get_graph_token()
    if not token:
        return False, "SharePoint ì¸ì¦ í† í° íšë“ ì‹¤íŒ¨"
        
    if "sharepoint_drive_id" not in st.session_state:
        st.warning("SharePoint ë“œë¼ì´ë¸Œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆìŠ¤í„° ë°ì´í„°ë¥¼ ë¨¼ì € ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.")
        load_master_data_from_sharepoint()
        if "sharepoint_drive_id" not in st.session_state:
            return False, "SharePoint ë“œë¼ì´ë¸Œ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨"

    drive_id = st.session_state['sharepoint_drive_id']
    file_name = "plto_record_data.xlsx"
    file_path_url = f"https://graph.microsoft.com/v1.0/drives/{drive_id}/root:/{file_name}"

    headers = {'Authorization': f'Bearer {token}'}
    
    existing_df = pd.DataFrame()
    try:
        download_response = requests.get(f"{file_path_url}:/content", headers=headers)
        
        if download_response.status_code == 200:
            if download_response.content and len(download_response.content) > 0:
                try:
                    existing_df = pd.read_excel(io.BytesIO(download_response.content))
                    st.info(f"ê¸°ì¡´ ë ˆì½”ë“œ '{file_name}'ì—ì„œ {len(existing_df)}ê°œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
                except Exception as e:
                    st.warning(f"'{file_name}' íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ìƒˆ ë°ì´í„°ë¡œ ë®ì–´ì”ë‹ˆë‹¤.")
            else:
                st.info(f"ê¸°ì¡´ ë ˆì½”ë“œ '{file_name}' íŒŒì¼ì´ ë¹„ì–´ ìˆì–´ ìƒˆë¡œ ë°ì´í„°ë¥¼ ì…ë ¥í•©ë‹ˆë‹¤.")
        elif download_response.status_code == 404:
            st.info(f"ê¸°ì¡´ ë ˆì½”ë“œ '{file_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        else:
            error_details = download_response.json()
            return False, f"ê¸°ì¡´ ë ˆì½”ë“œ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({download_response.status_code}): {error_details.get('error', {}).get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
    except Exception as e:
        return False, f"ê¸°ì¡´ ë ˆì½”ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}"

    try:
        order_date_str = None
        if not df_ecount_upload.empty:
            first_date_val = df_ecount_upload['ì¼ì'].iloc[0]
            if pd.notna(first_date_val) and str(first_date_val).strip():
                order_date_str = str(first_date_val)

        if not order_date_str:
            order_date_str = datetime.now().strftime("%Y%m%d")
            st.info("ì´ì¹´ìš´íŠ¸ ë°ì´í„°ì—ì„œ ìœ íš¨í•œ 'ì¼ì'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì˜¤ëŠ˜ ë‚ ì§œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        try:
            order_date = pd.to_datetime(order_date_str, format='%Y%m%d').strftime('%Y-%m-%d')
        except ValueError:
            st.warning(f"ë‚ ì§œ í˜•ì‹('{order_date_str}')ì´ ì˜¬ë°”ë¥´ì§€ ì•Šì•„ ì˜¤ëŠ˜ ë‚ ì§œë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            order_date = datetime.now().strftime('%Y-%m-%d')

        new_records = pd.DataFrame({
            'ì£¼ë¬¸ì¼ì': order_date,
            'ì²˜ë¦¬ì¼ì‹œ': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'ì¬ê³ ê´€ë¦¬ì½”ë“œ': df_main_result['ì¬ê³ ê´€ë¦¬ì½”ë“œ'],
            'SKUìƒí’ˆëª…': df_main_result['SKUìƒí’ˆëª…'],
            'ì£¼ë¬¸ìˆ˜ëŸ‰': df_main_result['ì£¼ë¬¸ìˆ˜ëŸ‰'],
            'ì‹¤ê²°ì œê¸ˆì•¡': df_main_result['ì‹¤ê²°ì œê¸ˆì•¡'],
            'ì‡¼í•‘ëª°': df_main_result['ì‡¼í•‘ëª°'],
            'ìˆ˜ë ¹ìëª…': df_main_result['ìˆ˜ë ¹ìëª…']
        })
    
        combined_df = pd.concat([existing_df, new_records], ignore_index=True)
    
        output = BytesIO()
        combined_df.to_excel(output, index=False, sheet_name='Records')
        file_content = output.getvalue()
    
        upload_headers = {'Authorization': f'Bearer {token}', 'Content-Type': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'}
        upload_response = requests.put(f"{file_path_url}:/content", headers=upload_headers, data=file_content)
        
        if upload_response.status_code in [200, 201]:
            return True, f"âœ… SharePointì— {len(new_records)}ê°œ ì‹ ê·œ ë ˆì½”ë“œ ì €ì¥ ì™„ë£Œ (ì´ {len(combined_df)}ê°œ)"
        else:
            error_details = upload_response.json()
            return False, f"SharePoint ì—…ë¡œë“œ ì‹¤íŒ¨ ({upload_response.status_code}): {error_details.get('error', {}).get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
    except Exception as e:
        st.error(traceback.format_exc())
        return False, f"ë ˆì½”ë“œ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

# --------------------------------------------------------------------------
# AI ë¶„ì„ í•¨ìˆ˜
# --------------------------------------------------------------------------

@st.cache_resource
def init_gemini():
    """Gemini AI ì´ˆê¸°í™”. ì—¬ëŸ¬ ëª¨ë¸ì„ ì‹œë„í•˜ì—¬ ì•ˆì •ì„±ì„ ë†’ì…ë‹ˆë‹¤."""
    if not GEMINI_AVAILABLE:
        return None
    
    try:
        api_key = st.secrets.get("GEMINI_API_KEY")
        if not api_key:
            st.warning("Gemini API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        genai.configure(api_key=api_key)
        
        model_candidates = ['gemini-1.5-flash-latest', 'gemini-1.0-pro', 'gemini-pro']
        for model_name in model_candidates:
            try:
                model = genai.GenerativeModel(model_name)
                st.session_state['gemini_model_name'] = model_name
                return model
            except Exception:
                st.info(f"'{model_name}' ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨. ë‹¤ìŒ ëª¨ë¸ì„ ì‹œë„í•©ë‹ˆë‹¤.")
        
        st.error("ì‚¬ìš© ê°€ëŠ¥í•œ Gemini ëª¨ë¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return None
    except Exception as e:
        st.error(f"Gemini AI ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def analyze_sales_with_ai(df_records):
    """AIë¥¼ ì‚¬ìš©í•œ íŒë§¤ ë°ì´í„° ë¶„ì„"""
    if not GEMINI_AVAILABLE:
        return "AI ë¶„ì„ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
    
    model = init_gemini()
    if not model or df_records.empty:
        return "AI ëª¨ë¸ì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ê±°ë‚˜ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
    try:
        summary = {
            "ì´_ì£¼ë¬¸ìˆ˜": len(df_records),
            "ì´_ë§¤ì¶œ": float(df_records['ì‹¤ê²°ì œê¸ˆì•¡'].sum()),
            "ìƒí’ˆ_ì¢…ë¥˜": int(df_records['SKUìƒí’ˆëª…'].nunique()),
            "ê³ ê°ìˆ˜": int(df_records['ìˆ˜ë ¹ìëª…'].nunique()),
            "ë² ìŠ¤íŠ¸ì…€ëŸ¬_TOP5": df_records.groupby('SKUìƒí’ˆëª…')['ì£¼ë¬¸ìˆ˜ëŸ‰'].sum().nlargest(5).to_dict(),
            "ì±„ë„ë³„_ë§¤ì¶œ": {k: float(v) for k, v in df_records.groupby('ì‡¼í•‘ëª°')['ì‹¤ê²°ì œê¸ˆì•¡'].sum().to_dict().items()}
        }
        
        prompt = f"""
        ì˜¨ë¼ì¸ ì‡¼í•‘ëª° íŒë§¤ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”: {json.dumps(summary, ensure_ascii=False, indent=2, default=str)}
        ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”:
        1. ğŸ“ˆ íŒë§¤ íŠ¸ë Œë“œ ë¶„ì„
        2. ğŸ† ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì¸ì‚¬ì´íŠ¸
        3. ğŸ›’ ì±„ë„ë³„ ì„±ê³¼ í‰ê°€
        4. ğŸ’¡ ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œì„  ì œì•ˆ
        ë¶„ì„ì€ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """
        
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        model_name = st.session_state.get('gemini_model_name', 'ì•Œ ìˆ˜ ì—†ìŒ')
        return f"AI ë¶„ì„ ì˜¤ë¥˜ ({model_name} ëª¨ë¸ ì‚¬ìš© ì¤‘): {e}"

# --------------------------------------------------------------------------
# ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜
# --------------------------------------------------------------------------

def to_excel_formatted(df, format_type=None):
    """ë°ì´í„°í”„ë ˆì„ì„ ì„œì‹ì´ ì ìš©ëœ ì—‘ì…€ íŒŒì¼ë¡œ ë³€í™˜"""
    output = BytesIO()
    df_to_save = df.copy().fillna('')
    
    if format_type == 'ecount_upload':
        df_to_save = df_to_save.rename(columns={'ì ìš”_ì „í‘œ': 'ì ìš”', 'ì ìš”_í’ˆëª©': 'ì ìš”.1'})

    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df_to_save.to_excel(writer, index=False, sheet_name='Sheet1')
    
    workbook = openpyxl.load_workbook(output)
    sheet = workbook.active

    center_alignment = Alignment(horizontal='center', vertical='center')
    for row in sheet.iter_rows():
        for cell in row:
            cell.alignment = center_alignment

    for column_cells in sheet.columns:
        try:
            max_length = max(len(str(cell.value)) for cell in column_cells if cell.value)
            adjusted_width = min((max_length + 2) * 1.2, 50)
            sheet.column_dimensions[column_cells[0].column_letter].width = adjusted_width
        except (ValueError, TypeError):
            pass

    thin_border = Border(left=Side(style='thin'), right=Side(style='thin'), top=Side(style='thin'), bottom=Side(style='thin'))
    pink_fill = PatternFill(start_color="FFEBEE", end_color="FFEBEE", fill_type="solid")

    for row in sheet.iter_rows(min_row=1, max_row=sheet.max_row):
        for cell in row:
            cell.border = thin_border

    if format_type == 'packing_list':
        bundle_start_row = 2
        for row_num in range(2, sheet.max_row + 2):
            is_new_bundle = (row_num <= sheet.max_row and sheet.cell(row=row_num, column=1).value) or row_num > sheet.max_row
            if is_new_bundle and row_num > 2:
                bundle_end_row = row_num - 1
                try:
                    bundle_num_str = str(sheet.cell(row=bundle_start_row, column=1).value)
                    if bundle_num_str.isdigit() and int(bundle_num_str) % 2 != 0:
                        for r in range(bundle_start_row, bundle_end_row + 1):
                            for c in range(1, sheet.max_column + 1):
                                sheet.cell(row=r, column=c).fill = pink_fill
                except (ValueError, IndexError):
                    pass
                
                if bundle_start_row < bundle_end_row:
                    sheet.merge_cells(start_row=bundle_start_row, start_column=1, end_row=bundle_end_row, end_column=1)
                    sheet.merge_cells(start_row=bundle_start_row, start_column=4, end_row=bundle_end_row, end_column=4)
                bundle_start_row = row_num
    elif format_type == 'quantity_summary':
        for row_idx, row in enumerate(sheet.iter_rows(min_row=2, max_row=sheet.max_row)):
            if row_idx % 2 == 0:
                for cell in row:
                    cell.fill = pink_fill
    
    final_output = BytesIO()
    workbook.save(final_output)
    return final_output.getvalue()

def process_all_files(file1, file2, file3, df_master):
    """ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜"""
    try:
        df_smartstore = pd.read_excel(file1)
        df_ecount_orig = pd.read_excel(file2)
        df_godomall = pd.read_excel(file3)

        df_ecount_orig['original_order'] = range(len(df_ecount_orig))
        
        df_godomall.rename(columns={'íšŒ í• ì¸ ê¸ˆì•¡': 'íšŒì› í• ì¸ ê¸ˆì•¡', 'ìì²´ì˜µì…˜ì½”ë“œ': 'ì¬ê³ ê´€ë¦¬ì½”ë“œ'}, inplace=True, errors='ignore')
        
        cols_to_numeric = ['ìƒí’ˆë³„ í’ˆëª©ê¸ˆì•¡', 'ì´ ë°°ì†¡ ê¸ˆì•¡', 'íšŒì› í• ì¸ ê¸ˆì•¡', 'ì¿ í° í• ì¸ ê¸ˆì•¡', 'ì‚¬ìš©ëœ ë§ˆì¼ë¦¬ì§€', 'ì´ ê²°ì œ ê¸ˆì•¡']
        for col in cols_to_numeric:
            if col in df_godomall.columns:
                df_godomall[col] = pd.to_numeric(df_godomall[col].astype(str).str.replace('[ì›,]', '', regex=True), errors='coerce').fillna(0)
        
        df_godomall['ë³´ì •ëœ_ë°°ì†¡ë¹„'] = np.where(df_godomall.duplicated(subset=['ìˆ˜ì·¨ì¸ ì´ë¦„']), 0, df_godomall.get('ì´ ë°°ì†¡ ê¸ˆì•¡', 0))
        df_godomall['ìˆ˜ì •ë _ê¸ˆì•¡_ê³ ë„ëª°'] = (df_godomall.get('ìƒí’ˆë³„ í’ˆëª©ê¸ˆì•¡', 0) + df_godomall['ë³´ì •ëœ_ë°°ì†¡ë¹„'] - 
                                     df_godomall.get('íšŒì› í• ì¸ ê¸ˆì•¡', 0) - df_godomall.get('ì¿ í° í• ì¸ ê¸ˆì•¡', 0) - 
                                     df_godomall.get('ì‚¬ìš©ëœ ë§ˆì¼ë¦¬ì§€', 0))
        
        warnings = [f"- [ê¸ˆì•¡ ë¶ˆì¼ì¹˜] **{name}**ë‹˜: {group['ìˆ˜ì •ë _ê¸ˆì•¡_ê³ ë„ëª°'].sum() - group['ì´ ê²°ì œ ê¸ˆì•¡'].iloc[0]:,.0f}ì› ì°¨ì´" for name, group in df_godomall.groupby('ìˆ˜ì·¨ì¸ ì´ë¦„') if 'ì´ ê²°ì œ ê¸ˆì•¡' in group and abs(group['ìˆ˜ì •ë _ê¸ˆì•¡_ê³ ë„ëª°'].sum() - group['ì´ ê²°ì œ ê¸ˆì•¡'].iloc[0]) > 1]

        df_final = df_ecount_orig.copy().rename(columns={'ê¸ˆì•¡': 'ì‹¤ê²°ì œê¸ˆì•¡'})
        
        key_cols = ['ì¬ê³ ê´€ë¦¬ì½”ë“œ', 'ì£¼ë¬¸ìˆ˜ëŸ‰', 'ìˆ˜ë ¹ìëª…']
        smartstore_prices = df_smartstore.rename(columns={'ì‹¤ê²°ì œê¸ˆì•¡': 'ìˆ˜ì •ë _ê¸ˆì•¡_ìŠ¤í† ì–´'})[key_cols + ['ìˆ˜ì •ë _ê¸ˆì•¡_ìŠ¤í† ì–´']].drop_duplicates(subset=key_cols)
        godomall_prices = df_godomall.rename(columns={'ìˆ˜ì·¨ì¸ ì´ë¦„': 'ìˆ˜ë ¹ìëª…', 'ìƒí’ˆìˆ˜ëŸ‰': 'ì£¼ë¬¸ìˆ˜ëŸ‰'})[key_cols + ['ìˆ˜ì •ë _ê¸ˆì•¡_ê³ ë„ëª°']].drop_duplicates(subset=key_cols)
        
        for df in [df_final, smartstore_prices, godomall_prices]:
            for col in ['ì¬ê³ ê´€ë¦¬ì½”ë“œ', 'ìˆ˜ë ¹ìëª…']: df[col] = df[col].astype(str).str.strip()
            for col in ['ì£¼ë¬¸ìˆ˜ëŸ‰']: df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)
        
        df_final = pd.merge(df_final, smartstore_prices, on=key_cols, how='left')
        df_final = pd.merge(df_final, godomall_prices, on=key_cols, how='left')

        df_final['ì‹¤ê²°ì œê¸ˆì•¡'] = np.where(df_final['ì‡¼í•‘ëª°'] == 'ê³ ë„ëª°5', df_final['ìˆ˜ì •ë _ê¸ˆì•¡_ê³ ë„ëª°'], df_final['ì‹¤ê²°ì œê¸ˆì•¡'])
        df_final['ì‹¤ê²°ì œê¸ˆì•¡'] = np.where(df_final['ì‡¼í•‘ëª°'] == 'ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´', df_final['ìˆ˜ì •ë _ê¸ˆì•¡_ìŠ¤í† ì–´'], df_final['ì‹¤ê²°ì œê¸ˆì•¡'])
        df_final['ì‹¤ê²°ì œê¸ˆì•¡'].fillna(df_ecount_orig['ê¸ˆì•¡'], inplace=True)
        
        df_main_result = df_final[['ì¬ê³ ê´€ë¦¬ì½”ë“œ', 'SKUìƒí’ˆëª…', 'ì£¼ë¬¸ìˆ˜ëŸ‰', 'ì‹¤ê²°ì œê¸ˆì•¡', 'ì‡¼í•‘ëª°', 'ìˆ˜ë ¹ìëª…', 'original_order']]
        
        name_groups = df_main_result.groupby('ìˆ˜ë ¹ìëª…')['original_order'].apply(list)
        warnings.extend([f"- [ë™ëª…ì´ì¸ ì˜ì‹¬] **{name}**ë‹˜ì˜ ì£¼ë¬¸ì´ ë–¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤." for name, orders in name_groups.items() if len(orders) > 1 and (max(orders) - min(orders) + 1) != len(orders)])
        
        df_quantity_summary = df_main_result.groupby('SKUìƒí’ˆëª…', as_index=False)['ì£¼ë¬¸ìˆ˜ëŸ‰'].sum().rename(columns={'ì£¼ë¬¸ìˆ˜ëŸ‰': 'ê°œìˆ˜'})
        
        df_packing = df_main_result.sort_values('original_order')[['SKUìƒí’ˆëª…', 'ì£¼ë¬¸ìˆ˜ëŸ‰', 'ìˆ˜ë ¹ìëª…', 'ì‡¼í•‘ëª°']].copy()
        is_first = ~df_packing['ìˆ˜ë ¹ìëª…'].duplicated(keep='first')
        df_packing['ë¬¶ìŒë²ˆí˜¸'] = is_first.cumsum()
        df_packing_list = df_packing.copy()
        df_packing_list['ë¬¶ìŒë²ˆí˜¸'] = df_packing_list['ë¬¶ìŒë²ˆí˜¸'].where(is_first, '')
        df_packing_list = df_packing_list[['ë¬¶ìŒë²ˆí˜¸', 'SKUìƒí’ˆëª…', 'ì£¼ë¬¸ìˆ˜ëŸ‰', 'ìˆ˜ë ¹ìëª…', 'ì‡¼í•‘ëª°']]

        df_merged = pd.merge(df_main_result, df_master[['SKUì½”ë“œ', 'ê³¼ì„¸ì—¬ë¶€', 'ì…ìˆ˜ëŸ‰']], left_on='ì¬ê³ ê´€ë¦¬ì½”ë“œ', right_on='SKUì½”ë“œ', how='left')
        warnings.extend([f"- [ë¯¸ë“±ë¡] {row['ì¬ê³ ê´€ë¦¬ì½”ë“œ']}: {row['SKUìƒí’ˆëª…']}" for _, row in df_merged[df_merged['SKUì½”ë“œ'].isna()].iterrows()])

        client_map = {'ì¿ íŒ¡': 'ì¿ íŒ¡ ì£¼ì‹íšŒì‚¬', 'ê³ ë„ëª°5': 'ê³ ë˜ë¯¸ìì‚¬ëª°_í˜„ê¸ˆì˜ìˆ˜ì¦(ê³ ë„ëª°)', 'ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´': 'ìŠ¤í† ì–´íŒœ', 'ë°°ë¯¼ìƒíšŒ': 'ì£¼ì‹íšŒì‚¬ ìš°ì•„í•œí˜•ì œë“¤(ë°°ë¯¼ìƒíšŒ)', 'ì´ì§€ì›°ëª°': 'ì£¼ì‹íšŒì‚¬ í˜„ëŒ€ì´ì§€ì›°'}
        
        df_ecount = pd.DataFrame()
        df_ecount['ì¼ì'] = datetime.now().strftime("%Y%m%d")
        df_ecount['ê±°ë˜ì²˜ëª…'] = df_merged['ì‡¼í•‘ëª°'].map(client_map).fillna(df_merged['ì‡¼í•‘ëª°'])
        df_ecount['ì¶œí•˜ì°½ê³ '] = 'ê³ ë˜ë¯¸'
        df_ecount['ê±°ë˜ìœ í˜•'] = np.where(df_merged['ê³¼ì„¸ì—¬ë¶€'] == 'ë©´ì„¸', 12, 11)
        df_ecount['ì ìš”_ì „í‘œ'] = 'ì˜¤ì „/ì˜¨ë¼ì¸'
        df_ecount['í’ˆëª©ì½”ë“œ'] = df_merged['ì¬ê³ ê´€ë¦¬ì½”ë“œ']
        
        is_box = df_merged['SKUìƒí’ˆëª…'].str.contains("BOX", na=False)
        ì…ìˆ˜ëŸ‰ = pd.to_numeric(df_merged['ì…ìˆ˜ëŸ‰'], errors='coerce').fillna(1)
        base_qty = np.where(is_box, df_merged['ì£¼ë¬¸ìˆ˜ëŸ‰'] * ì…ìˆ˜ëŸ‰, df_merged['ì£¼ë¬¸ìˆ˜ëŸ‰'])
        is_3pack = df_merged['SKUìƒí’ˆëª…'].str.contains("3ê°œì…|3ê°œ", na=False)
        df_ecount['ìˆ˜ëŸ‰'] = np.where(is_3pack, base_qty * 3, base_qty).astype(int)
        df_ecount['ë°•ìŠ¤'] = np.where(is_box, df_merged['ì£¼ë¬¸ìˆ˜ëŸ‰'], np.nan)
        
        df_merged['ì‹¤ê²°ì œê¸ˆì•¡'] = pd.to_numeric(df_merged['ì‹¤ê²°ì œê¸ˆì•¡'], errors='coerce').fillna(0)
        ê³µê¸‰ê°€ì•¡ = np.where(df_merged['ê³¼ì„¸ì—¬ë¶€'] == 'ê³¼ì„¸', df_merged['ì‹¤ê²°ì œê¸ˆì•¡'] / 1.1, df_merged['ì‹¤ê²°ì œê¸ˆì•¡'])
        
        df_ecount['ê³µê¸‰ê°€ì•¡'] = ê³µê¸‰ê°€ì•¡.round().astype(int)
        df_ecount['ë¶€ê°€ì„¸'] = (df_merged['ì‹¤ê²°ì œê¸ˆì•¡'] - df_ecount['ê³µê¸‰ê°€ì•¡']).round().astype(int)
        
        df_ecount['ì‡¼í•‘ëª°ê³ ê°ëª…'] = df_merged['ìˆ˜ë ¹ìëª…']
        df_ecount['original_order'] = df_merged['original_order']
        
        sort_order = ['ê³ ë˜ë¯¸ìì‚¬ëª°_í˜„ê¸ˆì˜ìˆ˜ì¦(ê³ ë„ëª°)', 'ìŠ¤í† ì–´íŒœ', 'ì¿ íŒ¡ ì£¼ì‹íšŒì‚¬', 'ì£¼ì‹íšŒì‚¬ ìš°ì•„í•œí˜•ì œë“¤(ë°°ë¯¼ìƒíšŒ)', 'ì£¼ì‹íšŒì‚¬ í˜„ëŒ€ì´ì§€ì›°']
        df_ecount['ê±°ë˜ì²˜ëª…_sort'] = pd.Categorical(df_ecount['ê±°ë˜ì²˜ëª…'], categories=sort_order, ordered=True)
        df_ecount = df_ecount.sort_values(by=['ê±°ë˜ì²˜ëª…_sort', 'ê±°ë˜ìœ í˜•', 'original_order']).drop(columns=['ê±°ë˜ì²˜ëª…_sort', 'original_order'])
        
        ecount_columns = ['ì¼ì', 'ìˆœë²ˆ', 'ê±°ë˜ì²˜ì½”ë“œ', 'ê±°ë˜ì²˜ëª…', 'ë‹´ë‹¹ì', 'ì¶œí•˜ì°½ê³ ', 'ê±°ë˜ìœ í˜•', 'í†µí™”', 'í™˜ìœ¨', 'ì ìš”_ì „í‘œ', 'ë¯¸ìˆ˜ê¸ˆ', 'ì´í•©ê³„', 'ì—°ê²°ì „í‘œ', 'í’ˆëª©ì½”ë“œ', 'í’ˆëª©ëª…', 'ê·œê²©', 'ë°•ìŠ¤', 'ìˆ˜ëŸ‰', 'ë‹¨ê°€', 'ì™¸í™”ê¸ˆì•¡', 'ê³µê¸‰ê°€ì•¡', 'ë¶€ê°€ì„¸', 'ì ìš”_í’ˆëª©', 'ìƒì‚°ì „í‘œìƒì„±', 'ì‹œë¦¬ì–¼/ë¡œíŠ¸', 'ê´€ë¦¬í•­ëª©', 'ì‡¼í•‘ëª°ê³ ê°ëª…']
        df_ecount_upload = df_ecount.reindex(columns=ecount_columns, fill_value='')

        return (df_main_result.drop(columns=['original_order']), df_quantity_summary, df_packing_list, df_ecount_upload, True, "âœ… ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!", warnings)

    except Exception as e:
        st.error(traceback.format_exc())
        return None, None, None, None, False, f"âŒ ì˜¤ë¥˜: {str(e)}", []

def create_analytics_dashboard(df_records):
    """ë¶„ì„ ëŒ€ì‹œë³´ë“œ ìƒì„±"""
    if df_records.empty:
        st.warning("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    st.header("íŒë§¤ ë°ì´í„° ë¶„ì„")
    col1, col2, col3, col4 = st.columns(4)
    total_revenue = df_records['ì‹¤ê²°ì œê¸ˆì•¡'].sum()
    total_orders = len(df_records['ìˆ˜ë ¹ìëª…'].unique())
    col1.metric("ğŸ’° ì´ ë§¤ì¶œ", f"â‚©{total_revenue:,.0f}")
    col2.metric("ğŸ“¦ ì´ ì£¼ë¬¸ìˆ˜", f"{total_orders:,}")
    col3.metric("ğŸ’µ í‰ê·  ì£¼ë¬¸ì•¡", f"â‚©{total_revenue/total_orders if total_orders else 0:,.0f}")
    col4.metric("ğŸ‘¥ ê³ ê°ìˆ˜", f"{df_records['ìˆ˜ë ¹ìëª…'].nunique():,}")
    
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“ˆ ì¼ë³„ íŠ¸ë Œë“œ", "ğŸ† ë² ìŠ¤íŠ¸ì…€ëŸ¬", "ğŸ›’ ì±„ë„ ë¶„ì„", "ğŸ¤– AI ì¸ì‚¬ì´íŠ¸"])
    
    with tab1:
        st.subheader("ì¼ë³„ ë§¤ì¶œ íŠ¸ë Œë“œ")
        df_records['ì£¼ë¬¸ì¼ì'] = pd.to_datetime(df_records['ì£¼ë¬¸ì¼ì'])
        daily_sales = df_records.groupby(df_records['ì£¼ë¬¸ì¼ì'].dt.date)['ì‹¤ê²°ì œê¸ˆì•¡'].sum()
        st.line_chart(daily_sales)
    
    with tab2:
        st.subheader("ìƒí’ˆë³„ íŒë§¤ TOP 10")
        st.bar_chart(df_records.groupby('SKUìƒí’ˆëª…')['ì£¼ë¬¸ìˆ˜ëŸ‰'].sum().nlargest(10))
    
    with tab3:
        st.subheader("ì±„ë„ë³„ ë§¤ì¶œ")
        st.bar_chart(df_records.groupby('ì‡¼í•‘ëª°')['ì‹¤ê²°ì œê¸ˆì•¡'].sum())
    
    with tab4:
        if GEMINI_AVAILABLE:
            with st.spinner("ğŸ¤– AIê°€ ë°ì´í„°ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                st.markdown("### ğŸ¤– AI íŒë§¤ ë¶„ì„ ë¦¬í¬íŠ¸")
                st.markdown(analyze_sales_with_ai(df_records))
        else:
            st.warning("AI ë¶„ì„ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ google-generativeaië¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.")

# --------------------------------------------------------------------------
# ë©”ì¸ ì•±
# --------------------------------------------------------------------------
def main():
    with st.sidebar:
        st.title("ğŸ“Š Order Pro v2.3")
        st.markdown("---")
        menu = st.radio("ë©”ë‰´ ì„ íƒ", ["ğŸ“‘ ì£¼ë¬¸ ì²˜ë¦¬", "ğŸ“ˆ íŒë§¤ ë¶„ì„", "âš™ï¸ ì„¤ì •"])
        st.markdown("---")
        st.caption("ğŸ“Œ ì‹œìŠ¤í…œ ìƒíƒœ")
        st.success("âœ… Graph API ì—°ê²°" if GRAPH_AVAILABLE and get_graph_token() else ("âš ï¸ Graph API ì¸ì¦ í•„ìš”" if GRAPH_AVAILABLE else "ğŸ’¾ ë¡œì»¬ ëª¨ë“œ"))
        st.success("âœ… AI í™œì„±í™”" if GEMINI_AVAILABLE else "ğŸ¤– AI ë¹„í™œì„±í™”")
        if st.button("ğŸ”„ ìºì‹œ ì´ˆê¸°í™”"):
            st.cache_data.clear()
            st.cache_resource.clear()
            st.success("ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
            st.rerun()

    if menu == "ğŸ“‘ ì£¼ë¬¸ ì²˜ë¦¬":
        st.title("ğŸ“‘ ì£¼ë¬¸ ì²˜ë¦¬ ìë™í™”")
        st.info("ğŸ’¡ SharePoint ì—°ë™ ë° ì²˜ë¦¬ ê²°ê³¼ ìë™ ëˆ„ì  ê¸°ëŠ¥ì´ í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        
        with st.expander("ğŸ“Š ë§ˆìŠ¤í„° ë°ì´í„° ìƒíƒœ", expanded=True):
            df_master = load_master_data_from_sharepoint()
            if df_master.empty:
                st.warning("âš ï¸ SharePointì—ì„œ ë§ˆìŠ¤í„° ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë¡œì»¬ ì—…ë¡œë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
                uploaded_master = st.file_uploader("ë§ˆìŠ¤í„° ë°ì´í„° ì—…ë¡œë“œ (xlsx, xls, csv)", type=['xlsx', 'xls', 'csv'])
                if uploaded_master:
                    try:
                        df_master = pd.read_excel(uploaded_master) if uploaded_master.name.endswith(('xlsx', 'xls')) else pd.read_csv(uploaded_master)
                        df_master = df_master.drop_duplicates(subset=['SKUì½”ë“œ'], keep='first')
                        st.success(f"âœ… ë¡œì»¬ ë§ˆìŠ¤í„° ë°ì´í„° {len(df_master)}ê°œ ë¡œë“œ ì™„ë£Œ")
                    except Exception as e:
                        st.error(f"íŒŒì¼ì„ ì½ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
                        df_master = pd.DataFrame()
            
            if not df_master.empty:
                col1, col2, col3 = st.columns(3)
                col1.metric("ì´ SKU", f"{len(df_master):,}ê°œ")
                col2.metric("ê³¼ì„¸ ìƒí’ˆ", f"{(df_master['ê³¼ì„¸ì—¬ë¶€']=='ê³¼ì„¸').sum():,}ê°œ")
                col3.metric("ë©´ì„¸ ìƒí’ˆ", f"{(df_master['ê³¼ì„¸ì—¬ë¶€']=='ë©´ì„¸').sum():,}ê°œ")

        st.markdown("---")
        st.header("1ï¸âƒ£ ì›ë³¸ íŒŒì¼ ì—…ë¡œë“œ")
        col1, col2, col3 = st.columns(3)
        file1 = col1.file_uploader("ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´", type=['xlsx', 'xls'])
        file2 = col2.file_uploader("ì´ì¹´ìš´íŠ¸", type=['xlsx', 'xls'])
        file3 = col3.file_uploader("ê³ ë„ëª°", type=['xlsx', 'xls'])
        
        st.header("2ï¸âƒ£ ë°ì´í„° ì²˜ë¦¬")
        if st.button("ğŸš€ ì²˜ë¦¬ ì‹œì‘", type="primary", disabled=not all([file1, file2, file3, not df_master.empty])):
            with st.spinner('ì²˜ë¦¬ ì¤‘...'):
                result = process_all_files(file1, file2, file3, df_master)
                df_main, df_qty, df_pack, df_ecount, success, message, warnings = result
            
            if success:
                st.balloons()
                st.success(message)
                
                if GRAPH_AVAILABLE:
                    with st.spinner('SharePointì— ê¸°ë¡ ì €ì¥ ì¤‘...'):
                        save_success, save_msg = save_to_sharepoint_records(df_main, df_ecount)
                        st.success(save_msg) if save_success else st.warning(save_msg)
                
                if warnings:
                    with st.expander(f"âš ï¸ í™•ì¸ í•„ìš” ({len(warnings)}ê±´)"):
                        st.markdown("\n".join(warnings))
                
                st.session_state['last_result'] = df_main
                st.session_state['processed_date'] = df_ecount['ì¼ì'].iloc[0]
                
                st.markdown("---")
                st.header("3ï¸âƒ£ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                tabs = st.tabs(["ğŸ¢ ì´ì¹´ìš´íŠ¸", "ğŸ“‹ í¬ì¥ë¦¬ìŠ¤íŠ¸", "ğŸ“¦ ìˆ˜ëŸ‰ìš”ì•½", "âœ… ìµœì¢…ê²°ê³¼"])
                
                tabs[0].dataframe(df_ecount.head(20), use_container_width=True)
                tabs[0].download_button("ğŸ“¥ ì´ì¹´ìš´íŠ¸ ì—…ë¡œë“œìš© ë‹¤ìš´ë¡œë“œ", to_excel_formatted(df_ecount, 'ecount_upload'), f"ì´ì¹´ìš´íŠ¸_{timestamp}.xlsx")
                
                tabs[1].dataframe(df_pack.head(20), use_container_width=True)
                tabs[1].download_button("ğŸ“¥ í¬ì¥ë¦¬ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ", to_excel_formatted(df_pack, 'packing_list'), f"í¬ì¥ë¦¬ìŠ¤íŠ¸_{timestamp}.xlsx")
                
                tabs[2].dataframe(df_qty, use_container_width=True)
                tabs[2].download_button("ğŸ“¥ ìˆ˜ëŸ‰ìš”ì•½ ë‹¤ìš´ë¡œë“œ", to_excel_formatted(df_qty, 'quantity_summary'), f"ìˆ˜ëŸ‰ìš”ì•½_{timestamp}.xlsx")

                tabs[3].dataframe(df_main.head(20), use_container_width=True)
                tabs[3].download_button("ğŸ“¥ ìµœì¢…ê²°ê³¼ ë‹¤ìš´ë¡œë“œ", to_excel_formatted(df_main), f"ìµœì¢…ê²°ê³¼_{timestamp}.xlsx")
            else:
                st.error(message)
    
    elif menu == "ğŸ“ˆ íŒë§¤ ë¶„ì„":
        st.title("ğŸ“ˆ íŒë§¤ ë¶„ì„")
        if 'last_result' in st.session_state and not st.session_state['last_result'].empty:
            df_records = st.session_state['last_result'].copy()
            df_records['ì£¼ë¬¸ì¼ì'] = pd.to_datetime(st.session_state['processed_date'], format='%Y%m%d')
            create_analytics_dashboard(df_records)
        else:
            st.info("ë¨¼ì € 'ì£¼ë¬¸ ì²˜ë¦¬' ë©”ë‰´ì—ì„œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")
    
    elif menu == "âš™ï¸ ì„¤ì •":
        st.title("âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
        st.header("ğŸ“ Microsoft Graph API ì„¤ì •")
        if "sharepoint" in st.secrets:
            st.text_input("Tenant ID", value=st.secrets["sharepoint"].get("tenant_id", "")[:20]+"...", disabled=True)
            st.text_input("Client ID", value=st.secrets["sharepoint"].get("client_id", "")[:20]+"...", disabled=True)
            if st.button("ğŸ”„ Graph API ì—°ê²° í…ŒìŠ¤íŠ¸"):
                with st.spinner("í…ŒìŠ¤íŠ¸ ì¤‘..."):
                    st.success("âœ… Microsoft Graph API ì—°ê²° ì„±ê³µ!") if get_graph_token() else st.error("âŒ Graph API ì—°ê²° ì‹¤íŒ¨")
        else:
            st.warning("Graph API ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤. (st.secrets.sharepoint)")
        
        st.header("ğŸ¤– AI ì„¤ì •")
        if "GEMINI_API_KEY" in st.secrets:
            st.text_input("Gemini API Key", value=st.secrets["GEMINI_API_KEY"][:10]+"...", disabled=True)
            if st.button("ğŸ”„ AI ì—°ê²° í…ŒìŠ¤íŠ¸"):
                with st.spinner("í…ŒìŠ¤íŠ¸ ì¤‘..."):
                    model = init_gemini()
                    st.success(f"âœ… Gemini AI ì—°ê²° ì„±ê³µ! (ëª¨ë¸: {st.session_state.get('gemini_model_name', 'N/A')})") if model else st.error("âŒ AI ì—°ê²° ì‹¤íŒ¨")
        else:
            st.warning("Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (st.secrets.GEMINI_API_KEY)")

if __name__ == "__main__":
    main()
