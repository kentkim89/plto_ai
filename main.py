import streamlit as st
import pandas as pd
import io
import numpy as np
import openpyxl
from openpyxl.styles import PatternFill, Border, Side, Alignment
from datetime import datetime, timedelta
import hashlib
import json
import traceback
import requests
from io import BytesIO
import base64

# --------------------------------------------------------------------------
# í˜ì´ì§€ ì„¤ì • (ê°€ì¥ ë¨¼ì € ì‹¤í–‰)
# --------------------------------------------------------------------------
st.set_page_config(
    page_title="ì£¼ë¬¸ ì²˜ë¦¬ ìë™í™” Pro v2.0",
    layout="wide",
    page_icon="ğŸ“Š",
    initial_sidebar_state="expanded"
)

# --------------------------------------------------------------------------
# ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ìš©ì„± ì²´í¬
# --------------------------------------------------------------------------

# Microsoft Graph API ì‚¬ìš©
GRAPH_AVAILABLE = False
try:
    import msal
    GRAPH_AVAILABLE = True
except ImportError:
    pass

# Plotly ì‚¬ìš©
PLOTLY_AVAILABLE = False
try:
    import plotly.express as px
    import plotly.graph_objects as go
    PLOTLY_AVAILABLE = True
except ImportError:
    pass

# Gemini AI ì‚¬ìš©
GEMINI_AVAILABLE = False
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    pass

# --------------------------------------------------------------------------
# Microsoft Graph API ì—°ê²° í•¨ìˆ˜
# --------------------------------------------------------------------------

@st.cache_resource
def get_graph_token():
    """Microsoft Graph API í† í° íšë“"""
    if not GRAPH_AVAILABLE:
        return None
    
    try:
        if "sharepoint" not in st.secrets:
            return None
        
        tenant_id = st.secrets["sharepoint"]["tenant_id"]
        client_id = st.secrets["sharepoint"]["client_id"]
        client_secret = st.secrets["sharepoint"]["client_secret"]
        
        # MSAL ì•± ìƒì„±
        authority = f"https://login.microsoftonline.com/{tenant_id}"
        app = msal.ConfidentialClientApplication(
            client_id,
            authority=authority,
            client_credential=client_secret
        )
        
        # í† í° íšë“
        result = app.acquire_token_silent(["https://graph.microsoft.com/.default"], account=None)
        if not result:
            result = app.acquire_token_for_client(scopes=["https://graph.microsoft.com/.default"])
        
        if "access_token" in result:
            return result["access_token"]
        else:
            st.error(f"í† í° íšë“ ì‹¤íŒ¨: {result.get('error_description', 'Unknown error')}")
            return None
            
    except Exception as e:
        st.error(f"Graph API ì—°ê²° ì‹¤íŒ¨: {e}")
        return None

@st.cache_data(ttl=600)
def load_master_data_from_sharepoint():
    """Microsoft Graph APIë¥¼ í†µí•´ SharePointì—ì„œ ë§ˆìŠ¤í„° ë°ì´í„° ë¡œë“œ"""
    try:
        # Graph API í† í° íšë“
        if GRAPH_AVAILABLE:
            token = get_graph_token()
            if token:
                headers = {
                    'Authorization': f'Bearer {token}',
                    'Accept': 'application/json'
                }
                
                # ë°©ë²• 1: ì‚¬ì´íŠ¸ IDë¡œ íŒŒì¼ ê²€ìƒ‰
                try:
                    # ì‚¬ì´íŠ¸ ê²€ìƒ‰
                    site_url = "https://graph.microsoft.com/v1.0/sites/goremi.sharepoint.com:/sites/data"
                    site_response = requests.get(site_url, headers=headers)
                    
                    if site_response.status_code == 200:
                        site_data = site_response.json()
                        site_id = site_data['id']
                        
                        # ë“œë¼ì´ë¸Œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                        drives_url = f"https://graph.microsoft.com/v1.0/sites/{site_id}/drives"
                        drives_response = requests.get(drives_url, headers=headers)
                        
                        if drives_response.status_code == 200:
                            drives = drives_response.json()['value']
                            
                            # ê° ë“œë¼ì´ë¸Œì—ì„œ íŒŒì¼ ê²€ìƒ‰
                            for drive in drives:
                                drive_id = drive['id']
                                
                                # íŒŒì¼ ê²€ìƒ‰
                                search_url = f"https://graph.microsoft.com/v1.0/drives/{drive_id}/root/search(q='plto_master_data.xlsx')"
                                search_response = requests.get(search_url, headers=headers)
                                
                                if search_response.status_code == 200:
                                    items = search_response.json().get('value', [])
                                    
                                    for item in items:
                                        if item['name'] == 'plto_master_data.xlsx':
                                            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                                            download_url = f"https://graph.microsoft.com/v1.0/drives/{drive_id}/items/{item['id']}/content"
                                            file_response = requests.get(download_url, headers=headers)
                                            
                                            if file_response.status_code == 200:
                                                df_master = pd.read_excel(io.BytesIO(file_response.content))
                                                df_master = df_master.drop_duplicates(subset=['SKUì½”ë“œ'], keep='first')
                                                st.success("âœ… Microsoft Graph APIë¡œ ë§ˆìŠ¤í„° ë°ì´í„° ë¡œë“œ ì„±ê³µ!")
                                                return df_master
                except Exception as e:
                    st.info(f"Graph API ë°©ë²• 1 ì‹¤íŒ¨: {e}")
                
                # ë°©ë²• 2: ê³µìœ  ë§í¬ë¥¼ í†µí•œ ì ‘ê·¼
                try:
                    if "sharepoint_files" in st.secrets:
                        share_url = st.secrets["sharepoint_files"]["plto_master_data_file_url"]
                        
                        # ê³µìœ  ë§í¬ë¥¼ í†µí•œ íŒŒì¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                        if "sharepoint.com" in share_url:
                            # ê³µìœ  ë§í¬ë¥¼ base64ë¡œ ì¸ì½”ë”©
                            encoded_url = base64.b64encode(share_url.encode()).decode()
                            # u! ì ‘ë‘ì‚¬ ì¶”ê°€
                            sharing_token = f"u!{encoded_url.rstrip('=').replace('/', '_').replace('+', '-')}"
                            
                            # Graph APIë¡œ ê³µìœ  ì•„ì´í…œ ì ‘ê·¼
                            shares_url = f"https://graph.microsoft.com/v1.0/shares/{sharing_token}/driveItem/content"
                            file_response = requests.get(shares_url, headers=headers)
                            
                            if file_response.status_code == 200:
                                df_master = pd.read_excel(io.BytesIO(file_response.content))
                                df_master = df_master.drop_duplicates(subset=['SKUì½”ë“œ'], keep='first')
                                st.success("âœ… ê³µìœ  ë§í¬ë¥¼ í†µí•´ ë§ˆìŠ¤í„° ë°ì´í„° ë¡œë“œ ì„±ê³µ!")
                                return df_master
                except Exception as e:
                    st.info(f"Graph API ë°©ë²• 2 ì‹¤íŒ¨: {e}")
        
        # Graph API ì‹¤íŒ¨ ì‹œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì‹œë„
        if "sharepoint_files" in st.secrets:
            file_url = st.secrets["sharepoint_files"]["plto_master_data_file_url"]
            
            # ìµëª… ë‹¤ìš´ë¡œë“œ ì‹œë„
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(file_url, headers=headers, timeout=30, allow_redirects=True)
            if response.status_code == 200 and len(response.content) > 100:
                df_master = pd.read_excel(io.BytesIO(response.content))
                df_master = df_master.drop_duplicates(subset=['SKUì½”ë“œ'], keep='first')
                st.success("âœ… ì§ì ‘ ë‹¤ìš´ë¡œë“œë¡œ ë§ˆìŠ¤í„° ë°ì´í„° ë¡œë“œ ì„±ê³µ!")
                return df_master
                
    except Exception as e:
        st.error(f"âŒ ë§ˆìŠ¤í„° ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return pd.DataFrame()

def save_to_sharepoint_records(df_main_result, df_ecount_upload):
    """Microsoft Graph APIë¥¼ í†µí•´ ì²˜ë¦¬ ê²°ê³¼ ì €ì¥"""
    try:
        if not GRAPH_AVAILABLE:
            st.info("Graph API ì—†ì´ëŠ” ìë™ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False, "ìë™ ì €ì¥ ë¶ˆê°€"
        
        token = get_graph_token()
        if not token:
            return False, "ì¸ì¦ ì‹¤íŒ¨"
        
        # ìƒˆ ë ˆì½”ë“œ ì¤€ë¹„
        new_records = pd.DataFrame()
        order_date = df_ecount_upload['ì¼ì'].iloc[0] if not df_ecount_upload.empty else datetime.now().strftime("%Y%m%d")
        
        new_records['ì£¼ë¬¸ì¼ì'] = order_date
        new_records['ì²˜ë¦¬ì¼ì‹œ'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        new_records['ì¬ê³ ê´€ë¦¬ì½”ë“œ'] = df_main_result['ì¬ê³ ê´€ë¦¬ì½”ë“œ']
        new_records['SKUìƒí’ˆëª…'] = df_main_result['SKUìƒí’ˆëª…']
        new_records['ì£¼ë¬¸ìˆ˜ëŸ‰'] = df_main_result['ì£¼ë¬¸ìˆ˜ëŸ‰']
        new_records['ì‹¤ê²°ì œê¸ˆì•¡'] = df_main_result['ì‹¤ê²°ì œê¸ˆì•¡']
        new_records['ì‡¼í•‘ëª°'] = df_main_result['ì‡¼í•‘ëª°']
        new_records['ìˆ˜ë ¹ìëª…'] = df_main_result['ìˆ˜ë ¹ìëª…']
        
        # Excelë¡œ ë³€í™˜
        output = BytesIO()
        new_records.to_excel(output, index=False, sheet_name='Records')
        output.seek(0)
        
        # ì„ì‹œ ì €ì¥ (ì‹¤ì œ ì—…ë¡œë“œëŠ” ì¶”ê°€ êµ¬í˜„ í•„ìš”)
        return True, f"âœ… {len(new_records)}ê°œ ë ˆì½”ë“œ ì²˜ë¦¬ ì™„ë£Œ"
        
    except Exception as e:
        return False, f"ì €ì¥ ì‹¤íŒ¨: {e}"

def load_record_data_from_sharepoint():
    """Graph APIë¥¼ í†µí•´ ê¸°ë¡ ë°ì´í„° ë¡œë“œ"""
    return pd.DataFrame()

# --------------------------------------------------------------------------
# AI ë¶„ì„ í•¨ìˆ˜
# --------------------------------------------------------------------------

@st.cache_resource
def init_gemini():
    """Gemini AI ì´ˆê¸°í™”"""
    if not GEMINI_AVAILABLE:
        return None
    
    try:
        # ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ API í‚¤ ì°¾ê¸°
        api_key = None
        
        # ë°©ë²• 1: ì§ì ‘ ì ‘ê·¼
        if "GEMINI_API_KEY" in st.secrets:
            api_key = st.secrets["GEMINI_API_KEY"]
        # ë°©ë²• 2: ì†ì„±ìœ¼ë¡œ ì ‘ê·¼
        elif hasattr(st.secrets, "GEMINI_API_KEY"):
            api_key = st.secrets.GEMINI_API_KEY
        # ë°©ë²• 3: get ë©”ì„œë“œ
        else:
            api_key = st.secrets.get("GEMINI_API_KEY", None)
        
        if api_key:
            genai.configure(api_key=api_key)
            return genai.GenerativeModel('gemini-pro')
        else:
            st.warning("Gemini API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        st.warning(f"Gemini AI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    return None

def analyze_sales_with_ai(df_records):
    """AIë¥¼ ì‚¬ìš©í•œ íŒë§¤ ë°ì´í„° ë¶„ì„"""
    if not GEMINI_AVAILABLE:
        return "AI ë¶„ì„ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
    
    try:
        model = init_gemini()
        if not model:
            # ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ë‹¤ì‹œ ì‹œë„
            api_key = st.secrets.get("GEMINI_API_KEY", None)
            if api_key:
                genai.configure(api_key=api_key)
                # ë‹¤ì–‘í•œ ëª¨ë¸ ì‹œë„
                for model_name in ['gemini-1.5-flash', 'gemini-1.5-pro', 'gemini-1.0-pro']:
                    try:
                        model = genai.GenerativeModel(model_name)
                        break
                    except:
                        continue
        
        if not model or df_records.empty:
            return "AI ëª¨ë¸ì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
        summary = {
            "ì´_ì£¼ë¬¸ìˆ˜": len(df_records),
            "ì´_ë§¤ì¶œ": float(df_records['ì‹¤ê²°ì œê¸ˆì•¡'].sum()),
            "ìƒí’ˆ_ì¢…ë¥˜": int(df_records['SKUìƒí’ˆëª…'].nunique()),
            "ê³ ê°ìˆ˜": int(df_records['ìˆ˜ë ¹ìëª…'].nunique()),
            "ë² ìŠ¤íŠ¸ì…€ëŸ¬_TOP5": df_records.groupby('SKUìƒí’ˆëª…')['ì£¼ë¬¸ìˆ˜ëŸ‰'].sum().nlargest(5).to_dict(),
            "ì±„ë„ë³„_ë§¤ì¶œ": {k: float(v) for k, v in df_records.groupby('ì‡¼í•‘ëª°')['ì‹¤ê²°ì œê¸ˆì•¡'].sum().to_dict().items()}
        }
        
        prompt = f"""
        ì˜¨ë¼ì¸ ì‡¼í•‘ëª° íŒë§¤ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:
        
        {json.dumps(summary, ensure_ascii=False, indent=2, default=str)}
        
        ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”:
        1. ğŸ“ˆ íŒë§¤ íŠ¸ë Œë“œ ë¶„ì„
        2. ğŸ† ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì¸ì‚¬ì´íŠ¸
        3. ğŸ›’ ì±„ë„ë³„ ì„±ê³¼ í‰ê°€
        4. ğŸ’¡ ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œì„  ì œì•ˆ
        
        ë¶„ì„ì€ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """
        
        response = model.generate_content(prompt)
        return response.text
        
    except Exception as e:
        # ì—ëŸ¬ ë©”ì‹œì§€ì— ëª¨ë¸ ì •ë³´ í¬í•¨
        return f"AI ë¶„ì„ ì˜¤ë¥˜: {e}\n\nì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ í™•ì¸í•˜ë ¤ë©´ ì„¤ì • í˜ì´ì§€ì—ì„œ 'AI ì—°ê²° í…ŒìŠ¤íŠ¸'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."

# --------------------------------------------------------------------------
# ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜
# --------------------------------------------------------------------------

def to_excel_formatted(df, format_type=None):
    """ë°ì´í„°í”„ë ˆì„ì„ ì„œì‹ì´ ì ìš©ëœ ì—‘ì…€ íŒŒì¼ë¡œ ë³€í™˜"""
    output = BytesIO()
    df_to_save = df.fillna('')
    
    if format_type == 'ecount_upload':
        df_to_save = df_to_save.rename(columns={'ì ìš”_ì „í‘œ': 'ì ìš”', 'ì ìš”_í’ˆëª©': 'ì ìš”.1'})

    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df_to_save.to_excel(writer, index=False, sheet_name='Sheet1')
    
    output.seek(0)
    workbook = openpyxl.load_workbook(output)
    sheet = workbook.active

    # ì„œì‹ ì ìš©
    center_alignment = Alignment(horizontal='center', vertical='center')
    for row in sheet.iter_rows():
        for cell in row:
            cell.alignment = center_alignment

    # ì—´ ë„ˆë¹„ ì¡°ì •
    for column_cells in sheet.columns:
        max_length = 0
        column = column_cells[0].column_letter
        for cell in column_cells:
            try:
                if cell.value and len(str(cell.value)) > max_length:
                    max_length = len(str(cell.value))
            except:
                pass
        adjusted_width = min((max_length + 2) * 1.2, 50)
        sheet.column_dimensions[column].width = adjusted_width
    
    thin_border = Border(
        left=Side(style='thin'), right=Side(style='thin'),
        top=Side(style='thin'), bottom=Side(style='thin')
    )
    pink_fill = PatternFill(start_color="FFEBEE", end_color="FFEBEE", fill_type="solid")

    # íŠ¹ë³„ ì„œì‹
    if format_type == 'packing_list':
        for row in sheet.iter_rows(min_row=1, max_row=sheet.max_row):
            for cell in row:
                cell.border = thin_border
        
        bundle_start_row = 2
        for row_num in range(2, sheet.max_row + 2):
            current_bundle_cell = sheet.cell(row=row_num, column=1) if row_num <= sheet.max_row else None
            
            if (current_bundle_cell and current_bundle_cell.value) or (row_num > sheet.max_row):
                if row_num > 2:
                    bundle_end_row = row_num - 1
                    prev_bundle_num_str = str(sheet.cell(row=bundle_start_row, column=1).value)
                    
                    if prev_bundle_num_str.isdigit() and int(prev_bundle_num_str) % 2 != 0:
                        for r in range(bundle_start_row, bundle_end_row + 1):
                            for c in range(1, sheet.max_column + 1):
                                sheet.cell(row=r, column=c).fill = pink_fill
                    
                    if bundle_start_row < bundle_end_row:
                        sheet.merge_cells(start_row=bundle_start_row, start_column=1,
                                        end_row=bundle_end_row, end_column=1)
                        sheet.merge_cells(start_row=bundle_start_row, start_column=4,
                                        end_row=bundle_end_row, end_column=4)
                
                bundle_start_row = row_num

    elif format_type == 'quantity_summary':
        for row_idx, row in enumerate(sheet.iter_rows(min_row=1, max_row=sheet.max_row)):
            for cell in row:
                cell.border = thin_border
            if row_idx > 0 and row_idx % 2 != 0:
                for cell in row:
                    cell.fill = pink_fill
    
    final_output = BytesIO()
    workbook.save(final_output)
    final_output.seek(0)
    
    return final_output.getvalue()

def process_all_files(file1, file2, file3, df_master):
    """ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜"""
    try:
        # íŒŒì¼ ì½ê¸°
        df_smartstore = pd.read_excel(file1)
        df_ecount_orig = pd.read_excel(file2)
        df_godomall = pd.read_excel(file3)

        df_ecount_orig['original_order'] = range(len(df_ecount_orig))
        
        # ì»¬ëŸ¼ëª… í˜¸í™˜ì„± ì²˜ë¦¬
        if 'íšŒ í• ì¸ ê¸ˆì•¡' in df_godomall.columns:
            df_godomall.rename(columns={'íšŒ í• ì¸ ê¸ˆì•¡': 'íšŒì› í• ì¸ ê¸ˆì•¡'}, inplace=True)
        if 'ìì²´ì˜µì…˜ì½”ë“œ' in df_godomall.columns:
            df_godomall.rename(columns={'ìì²´ì˜µì…˜ì½”ë“œ': 'ì¬ê³ ê´€ë¦¬ì½”ë“œ'}, inplace=True)
        
        # ë°ì´í„° í´ë¦¬ë‹
        cols_to_numeric = ['ìƒí’ˆë³„ í’ˆëª©ê¸ˆì•¡', 'ì´ ë°°ì†¡ ê¸ˆì•¡', 'íšŒì› í• ì¸ ê¸ˆì•¡',
                          'ì¿ í° í• ì¸ ê¸ˆì•¡', 'ì‚¬ìš©ëœ ë§ˆì¼ë¦¬ì§€', 'ì´ ê²°ì œ ê¸ˆì•¡']
        for col in cols_to_numeric:
            if col in df_godomall.columns:
                df_godomall[col] = pd.to_numeric(
                    df_godomall[col].astype(str).str.replace('[ì›,]', '', regex=True),
                    errors='coerce'
                ).fillna(0)
        
        # ë°°ì†¡ë¹„ ì¤‘ë³µ ë°©ì§€
        df_godomall['ë³´ì •ëœ_ë°°ì†¡ë¹„'] = np.where(
            df_godomall.duplicated(subset=['ìˆ˜ì·¨ì¸ ì´ë¦„']),
            0,
            df_godomall['ì´ ë°°ì†¡ ê¸ˆì•¡']
        )
        
        df_godomall['ìˆ˜ì •ë _ê¸ˆì•¡_ê³ ë„ëª°'] = (
            df_godomall['ìƒí’ˆë³„ í’ˆëª©ê¸ˆì•¡'] + df_godomall['ë³´ì •ëœ_ë°°ì†¡ë¹„'] -
            df_godomall['íšŒì› í• ì¸ ê¸ˆì•¡'] - df_godomall['ì¿ í° í• ì¸ ê¸ˆì•¡'] -
            df_godomall['ì‚¬ìš©ëœ ë§ˆì¼ë¦¬ì§€']
        )
        
        # ê²½ê³  ìˆ˜ì§‘
        warnings = []
        
        # ê³ ë„ëª° ê¸ˆì•¡ ê²€ì¦
        for name, group in df_godomall.groupby('ìˆ˜ì·¨ì¸ ì´ë¦„'):
            calculated = group['ìˆ˜ì •ë _ê¸ˆì•¡_ê³ ë„ëª°'].sum()
            actual = group['ì´ ê²°ì œ ê¸ˆì•¡'].iloc[0]
            diff = calculated - actual
            if abs(diff) > 1:
                warnings.append(f"- [ê¸ˆì•¡ ë¶ˆì¼ì¹˜] **{name}**ë‹˜: {diff:,.0f}ì› ì°¨ì´")

        # ë©”ì¸ ì²˜ë¦¬
        df_final = df_ecount_orig.copy().rename(columns={'ê¸ˆì•¡': 'ì‹¤ê²°ì œê¸ˆì•¡'})
        
        # ë³‘í•© ì¤€ë¹„
        key_cols = ['ì¬ê³ ê´€ë¦¬ì½”ë“œ', 'ì£¼ë¬¸ìˆ˜ëŸ‰', 'ìˆ˜ë ¹ìëª…']
        smartstore_prices = df_smartstore.rename(
            columns={'ì‹¤ê²°ì œê¸ˆì•¡': 'ìˆ˜ì •ë _ê¸ˆì•¡_ìŠ¤í† ì–´'}
        )[key_cols + ['ìˆ˜ì •ë _ê¸ˆì•¡_ìŠ¤í† ì–´']].drop_duplicates(subset=key_cols, keep='first')
        
        godomall_prices = df_godomall.rename(
            columns={'ìˆ˜ì·¨ì¸ ì´ë¦„': 'ìˆ˜ë ¹ìëª…', 'ìƒí’ˆìˆ˜ëŸ‰': 'ì£¼ë¬¸ìˆ˜ëŸ‰'}
        )[['ì¬ê³ ê´€ë¦¬ì½”ë“œ', 'ìˆ˜ë ¹ìëª…', 'ì£¼ë¬¸ìˆ˜ëŸ‰', 'ìˆ˜ì •ë _ê¸ˆì•¡_ê³ ë„ëª°']].drop_duplicates(
            subset=['ì¬ê³ ê´€ë¦¬ì½”ë“œ', 'ìˆ˜ë ¹ìëª…', 'ì£¼ë¬¸ìˆ˜ëŸ‰'], keep='first'
        )
        
        # ë°ì´í„° íƒ€ì… í†µì¼
        for col in ['ì¬ê³ ê´€ë¦¬ì½”ë“œ', 'ìˆ˜ë ¹ìëª…']:
            df_final[col] = df_final[col].astype(str).str.strip()
            smartstore_prices[col] = smartstore_prices[col].astype(str).str.strip()
            godomall_prices[col] = godomall_prices[col].astype(str).str.strip()
        
        for col in ['ì£¼ë¬¸ìˆ˜ëŸ‰']:
            df_final[col] = pd.to_numeric(df_final[col], errors='coerce').fillna(0).astype(int)
            smartstore_prices[col] = pd.to_numeric(smartstore_prices[col], errors='coerce').fillna(0).astype(int)
            godomall_prices[col] = pd.to_numeric(godomall_prices[col], errors='coerce').fillna(0).astype(int)
        
        df_final['ì‹¤ê²°ì œê¸ˆì•¡'] = pd.to_numeric(df_final['ì‹¤ê²°ì œê¸ˆì•¡'], errors='coerce').fillna(0).astype(int)
        
        # ë³‘í•©
        df_final = pd.merge(df_final, smartstore_prices, on=key_cols, how='left')
        df_final = pd.merge(df_final, godomall_prices, on=key_cols, how='left')

        # ê¸ˆì•¡ ì—…ë°ì´íŠ¸
        df_final['ì‹¤ê²°ì œê¸ˆì•¡'] = np.where(
            df_final['ì‡¼í•‘ëª°'] == 'ê³ ë„ëª°5',
            df_final['ìˆ˜ì •ë _ê¸ˆì•¡_ê³ ë„ëª°'].fillna(df_final['ì‹¤ê²°ì œê¸ˆì•¡']),
            df_final['ì‹¤ê²°ì œê¸ˆì•¡']
        )
        df_final['ì‹¤ê²°ì œê¸ˆì•¡'] = np.where(
            df_final['ì‡¼í•‘ëª°'] == 'ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´',
            df_final['ìˆ˜ì •ë _ê¸ˆì•¡_ìŠ¤í† ì–´'].fillna(df_final['ì‹¤ê²°ì œê¸ˆì•¡']),
            df_final['ì‹¤ê²°ì œê¸ˆì•¡']
        )
        
        # ê²°ê³¼ ìƒì„±
        df_main_result = df_final[[
            'ì¬ê³ ê´€ë¦¬ì½”ë“œ', 'SKUìƒí’ˆëª…', 'ì£¼ë¬¸ìˆ˜ëŸ‰', 'ì‹¤ê²°ì œê¸ˆì•¡',
            'ì‡¼í•‘ëª°', 'ìˆ˜ë ¹ìëª…', 'original_order'
        ]]
        
        # ë™ëª…ì´ì¸ ì²´í¬
        name_groups = df_main_result.groupby('ìˆ˜ë ¹ìëª…')['original_order'].apply(list)
        for name, orders in name_groups.items():
            if len(orders) > 1 and (max(orders) - min(orders) + 1) != len(orders):
                warnings.append(f"- [ë™ëª…ì´ì¸ ì˜ì‹¬] **{name}**ë‹˜ì˜ ì£¼ë¬¸ì´ ë–¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.")
        
        # ìš”ì•½ ìƒì„±
        df_quantity_summary = df_main_result.groupby('SKUìƒí’ˆëª…', as_index=False)['ì£¼ë¬¸ìˆ˜ëŸ‰'].sum()
        df_quantity_summary.columns = ['SKUìƒí’ˆëª…', 'ê°œìˆ˜']
        
        # í¬ì¥ ë¦¬ìŠ¤íŠ¸
        df_packing = df_main_result.sort_values('original_order')[[
            'SKUìƒí’ˆëª…', 'ì£¼ë¬¸ìˆ˜ëŸ‰', 'ìˆ˜ë ¹ìëª…', 'ì‡¼í•‘ëª°'
        ]].copy()
        
        is_first = df_packing['ìˆ˜ë ¹ìëª…'] != df_packing['ìˆ˜ë ¹ìëª…'].shift(1)
        df_packing['ë¬¶ìŒë²ˆí˜¸'] = is_first.cumsum()
        df_packing_list = df_packing.copy()
        df_packing_list['ë¬¶ìŒë²ˆí˜¸'] = df_packing_list['ë¬¶ìŒë²ˆí˜¸'].where(is_first, '')
        df_packing_list = df_packing_list[[
            'ë¬¶ìŒë²ˆí˜¸', 'SKUìƒí’ˆëª…', 'ì£¼ë¬¸ìˆ˜ëŸ‰', 'ìˆ˜ë ¹ìëª…', 'ì‡¼í•‘ëª°'
        ]]

        # ì´ì¹´ìš´íŠ¸ ë°ì´í„° ìƒì„±
        df_merged = pd.merge(
            df_main_result,
            df_master[['SKUì½”ë“œ', 'ê³¼ì„¸ì—¬ë¶€', 'ì…ìˆ˜ëŸ‰']],
            left_on='ì¬ê³ ê´€ë¦¬ì½”ë“œ',
            right_on='SKUì½”ë“œ',
            how='left'
        )
        
        # ë¯¸ë“±ë¡ ìƒí’ˆ ì²´í¬
        for _, row in df_merged[df_merged['SKUì½”ë“œ'].isna()].iterrows():
            warnings.append(f"- [ë¯¸ë“±ë¡] {row['ì¬ê³ ê´€ë¦¬ì½”ë“œ']}: {row['SKUìƒí’ˆëª…']}")

        # ê±°ë˜ì²˜ ë§¤í•‘
        client_map = {
            'ì¿ íŒ¡': 'ì¿ íŒ¡ ì£¼ì‹íšŒì‚¬',
            'ê³ ë„ëª°5': 'ê³ ë˜ë¯¸ìì‚¬ëª°_í˜„ê¸ˆì˜ìˆ˜ì¦(ê³ ë„ëª°)',
            'ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´': 'ìŠ¤í† ì–´íŒœ',
            'ë°°ë¯¼ìƒíšŒ': 'ì£¼ì‹íšŒì‚¬ ìš°ì•„í•œí˜•ì œë“¤(ë°°ë¯¼ìƒíšŒ)',
            'ì´ì§€ì›°ëª°': 'ì£¼ì‹íšŒì‚¬ í˜„ëŒ€ì´ì§€ì›°'
        }
        
        # ì´ì¹´ìš´íŠ¸ ì—…ë¡œë“œ ë°ì´í„° ìƒì„±
        df_ecount = pd.DataFrame()
        df_ecount['ì¼ì'] = datetime.now().strftime("%Y%m%d")
        df_ecount['ê±°ë˜ì²˜ëª…'] = df_merged['ì‡¼í•‘ëª°'].map(client_map).fillna(df_merged['ì‡¼í•‘ëª°'])
        df_ecount['ì¶œí•˜ì°½ê³ '] = 'ê³ ë˜ë¯¸'
        df_ecount['ê±°ë˜ìœ í˜•'] = np.where(df_merged['ê³¼ì„¸ì—¬ë¶€'] == 'ë©´ì„¸', 12, 11)
        df_ecount['ì ìš”_ì „í‘œ'] = 'ì˜¤ì „/ì˜¨ë¼ì¸'
        df_ecount['í’ˆëª©ì½”ë“œ'] = df_merged['ì¬ê³ ê´€ë¦¬ì½”ë“œ']
        
        # ìˆ˜ëŸ‰ ê³„ì‚°
        is_box = df_merged['SKUìƒí’ˆëª…'].str.contains("BOX", na=False)
        ì…ìˆ˜ëŸ‰ = pd.to_numeric(df_merged['ì…ìˆ˜ëŸ‰'], errors='coerce').fillna(1)
        base_qty = np.where(is_box, df_merged['ì£¼ë¬¸ìˆ˜ëŸ‰'] * ì…ìˆ˜ëŸ‰, df_merged['ì£¼ë¬¸ìˆ˜ëŸ‰'])
        is_3pack = df_merged['SKUìƒí’ˆëª…'].str.contains("3ê°œì…|3ê°œ", na=False)
        final_qty = np.where(is_3pack, base_qty * 3, base_qty)
        
        df_ecount['ë°•ìŠ¤'] = np.where(is_box, df_merged['ì£¼ë¬¸ìˆ˜ëŸ‰'], np.nan)
        df_ecount['ìˆ˜ëŸ‰'] = final_qty.astype(int)
        
        # ê¸ˆì•¡ ê³„ì‚°
        df_merged['ì‹¤ê²°ì œê¸ˆì•¡'] = pd.to_numeric(df_merged['ì‹¤ê²°ì œê¸ˆì•¡'], errors='coerce').fillna(0)
        ê³µê¸‰ê°€ì•¡ = np.where(
            df_merged['ê³¼ì„¸ì—¬ë¶€'] == 'ê³¼ì„¸',
            df_merged['ì‹¤ê²°ì œê¸ˆì•¡'] / 1.1,
            df_merged['ì‹¤ê²°ì œê¸ˆì•¡']
        )
        df_ecount['ê³µê¸‰ê°€ì•¡'] = ê³µê¸‰ê°€ì•¡
        df_ecount['ë¶€ê°€ì„¸'] = df_merged['ì‹¤ê²°ì œê¸ˆì•¡'] - df_ecount['ê³µê¸‰ê°€ì•¡']
        
        df_ecount['ì‡¼í•‘ëª°ê³ ê°ëª…'] = df_merged['ìˆ˜ë ¹ìëª…']
        df_ecount['original_order'] = df_merged['original_order']
        
        # ì»¬ëŸ¼ ì •ë¦¬
        ecount_columns = [
            'ì¼ì', 'ìˆœë²ˆ', 'ê±°ë˜ì²˜ì½”ë“œ', 'ê±°ë˜ì²˜ëª…', 'ë‹´ë‹¹ì', 'ì¶œí•˜ì°½ê³ ',
            'ê±°ë˜ìœ í˜•', 'í†µí™”', 'í™˜ìœ¨', 'ì ìš”_ì „í‘œ', 'ë¯¸ìˆ˜ê¸ˆ', 'ì´í•©ê³„',
            'ì—°ê²°ì „í‘œ', 'í’ˆëª©ì½”ë“œ', 'í’ˆëª©ëª…', 'ê·œê²©', 'ë°•ìŠ¤', 'ìˆ˜ëŸ‰',
            'ë‹¨ê°€', 'ì™¸í™”ê¸ˆì•¡', 'ê³µê¸‰ê°€ì•¡', 'ë¶€ê°€ì„¸', 'ì ìš”_í’ˆëª©',
            'ìƒì‚°ì „í‘œìƒì„±', 'ì‹œë¦¬ì–¼/ë¡œíŠ¸', 'ê´€ë¦¬í•­ëª©', 'ì‡¼í•‘ëª°ê³ ê°ëª…'
        ]
        
        for col in ecount_columns:
            if col not in df_ecount:
                df_ecount[col] = ''
        
        df_ecount['ê³µê¸‰ê°€ì•¡'] = df_ecount['ê³µê¸‰ê°€ì•¡'].round().astype('Int64')
        df_ecount['ë¶€ê°€ì„¸'] = df_ecount['ë¶€ê°€ì„¸'].round().astype('Int64')
        df_ecount['ê±°ë˜ìœ í˜•'] = pd.to_numeric(df_ecount['ê±°ë˜ìœ í˜•'])
        
        # ì •ë ¬
        sort_order = [
            'ê³ ë˜ë¯¸ìì‚¬ëª°_í˜„ê¸ˆì˜ìˆ˜ì¦(ê³ ë„ëª°)',
            'ìŠ¤í† ì–´íŒœ',
            'ì¿ íŒ¡ ì£¼ì‹íšŒì‚¬',
            'ì£¼ì‹íšŒì‚¬ ìš°ì•„í•œí˜•ì œë“¤(ë°°ë¯¼ìƒíšŒ)',
            'ì£¼ì‹íšŒì‚¬ í˜„ëŒ€ì´ì§€ì›°'
        ]
        
        df_ecount['ê±°ë˜ì²˜ëª…_sort'] = pd.Categorical(
            df_ecount['ê±°ë˜ì²˜ëª…'],
            categories=sort_order,
            ordered=True
        )
        
        df_ecount = df_ecount.sort_values(
            by=['ê±°ë˜ì²˜ëª…_sort', 'ê±°ë˜ìœ í˜•', 'original_order']
        ).drop(columns=['ê±°ë˜ì²˜ëª…_sort', 'original_order'])
        
        df_ecount_upload = df_ecount[ecount_columns]

        return (
            df_main_result.drop(columns=['original_order']),
            df_quantity_summary,
            df_packing_list,
            df_ecount_upload,
            True,
            "âœ… ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!",
            warnings
        )

    except Exception as e:
        return None, None, None, None, False, f"âŒ ì˜¤ë¥˜: {str(e)}", []

def create_analytics_dashboard(df_records):
    """ë¶„ì„ ëŒ€ì‹œë³´ë“œ ìƒì„±"""
    if df_records.empty:
        st.warning("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê¸°ë³¸ ë©”íŠ¸ë¦­
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        total_revenue = df_records['ì‹¤ê²°ì œê¸ˆì•¡'].sum()
        st.metric("ğŸ’° ì´ ë§¤ì¶œ", f"â‚©{total_revenue:,.0f}")
    
    with col2:
        total_orders = len(df_records)
        st.metric("ğŸ“¦ ì´ ì£¼ë¬¸ìˆ˜", f"{total_orders:,}")
    
    with col3:
        avg_order = total_revenue / total_orders if total_orders > 0 else 0
        st.metric("ğŸ’µ í‰ê·  ì£¼ë¬¸ì•¡", f"â‚©{avg_order:,.0f}")
    
    with col4:
        unique_customers = df_records['ìˆ˜ë ¹ìëª…'].nunique()
        st.metric("ğŸ‘¥ ê³ ê°ìˆ˜", f"{unique_customers:,}")
    
    # ì°¨íŠ¸ íƒ­
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“ˆ ì¼ë³„ íŠ¸ë Œë“œ", "ğŸ† ë² ìŠ¤íŠ¸ì…€ëŸ¬", "ğŸ›’ ì±„ë„ ë¶„ì„", "ğŸ¤– AI ì¸ì‚¬ì´íŠ¸"])
    
    with tab1:
        st.subheader("ì¼ë³„ ë§¤ì¶œ íŠ¸ë Œë“œ")
        daily_sales = df_records.groupby('ì£¼ë¬¸ì¼ì')['ì‹¤ê²°ì œê¸ˆì•¡'].sum().reset_index()
        st.line_chart(daily_sales.set_index('ì£¼ë¬¸ì¼ì')['ì‹¤ê²°ì œê¸ˆì•¡'])
    
    with tab2:
        st.subheader("ìƒí’ˆë³„ íŒë§¤ TOP 10")
        top_products = df_records.groupby('SKUìƒí’ˆëª…')['ì£¼ë¬¸ìˆ˜ëŸ‰'].sum().nlargest(10)
        st.bar_chart(top_products)
    
    with tab3:
        st.subheader("ì±„ë„ë³„ ë§¤ì¶œ")
        channel_sales = df_records.groupby('ì‡¼í•‘ëª°')['ì‹¤ê²°ì œê¸ˆì•¡'].sum()
        st.bar_chart(channel_sales)
    
    with tab4:
        if GEMINI_AVAILABLE:
            with st.spinner("ğŸ¤– AIê°€ ë°ì´í„°ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                ai_insights = analyze_sales_with_ai(df_records)
                if ai_insights:
                    st.markdown("### ğŸ¤– AI íŒë§¤ ë¶„ì„ ë¦¬í¬íŠ¸")
                    st.markdown(ai_insights)
                else:
                    st.info("AI ë¶„ì„ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.warning("AI ë¶„ì„ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ google-generativeaië¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.")

# --------------------------------------------------------------------------
# ë©”ì¸ ì•±
# --------------------------------------------------------------------------

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.title("ğŸ“Š Order Pro v2.0")
    st.markdown("---")
    
    menu = st.radio(
        "ë©”ë‰´ ì„ íƒ",
        ["ğŸ“‘ ì£¼ë¬¸ ì²˜ë¦¬", "ğŸ“ˆ íŒë§¤ ë¶„ì„", "âš™ï¸ ì„¤ì •"],
        index=0
    )
    
    st.markdown("---")
    st.caption("ğŸ“Œ ì‹œìŠ¤í…œ ìƒíƒœ")
    
    # Graph API ìƒíƒœ
    if GRAPH_AVAILABLE:
        token = get_graph_token()
        if token:
            st.success("âœ… Graph API ì—°ê²°")
        else:
            st.warning("âš ï¸ Graph API ì¸ì¦ í•„ìš”")
    else:
        st.info("ğŸ’¾ ë¡œì»¬ ëª¨ë“œ")
    
    # AI ìƒíƒœ
    if GEMINI_AVAILABLE:
        st.success("âœ… AI í™œì„±í™”")
    else:
        st.info("ğŸ¤– AI ë¹„í™œì„±í™”")
    
    # ìºì‹œ ì´ˆê¸°í™”
    if st.button("ğŸ”„ ìºì‹œ ì´ˆê¸°í™”"):
        st.cache_data.clear()
        st.cache_resource.clear()
        st.success("ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
        st.rerun()

# ë©”ì¸ ì½˜í…ì¸ 
if menu == "ğŸ“‘ ì£¼ë¬¸ ì²˜ë¦¬":
    st.title("ğŸ“‘ ì£¼ë¬¸ ì²˜ë¦¬ ìë™í™”")
    st.info("ğŸ’¡ SharePoint ì—°ë™ ë° ìë™ ì €ì¥ ê¸°ëŠ¥ì´ í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    
    # ë§ˆìŠ¤í„° ë°ì´í„° ì„¹ì…˜
    with st.expander("ğŸ“Š ë§ˆìŠ¤í„° ë°ì´í„° ìƒíƒœ", expanded=True):
        # ë””ë²„ê·¸ ëª¨ë“œ ì¶”ê°€
        debug = st.checkbox("ğŸ” ë””ë²„ê·¸ ëª¨ë“œ", value=False)
        
        if debug:
            st.info("SharePoint ì„¤ì • í™•ì¸")
            if "sharepoint_files" in st.secrets:
                st.code(f"""
URL: {st.secrets['sharepoint_files']['plto_master_data_file_url'][:100]}...
Site: {st.secrets['sharepoint_files']['site_name']}
File: {st.secrets['sharepoint_files']['file_name']}
                """)
            
            if st.button("ğŸ§ª ìˆ˜ë™ í…ŒìŠ¤íŠ¸"):
                st.cache_data.clear()
        
        df_master = load_master_data_from_sharepoint()
        
        if not df_master.empty:
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ì´ SKU", f"{len(df_master):,}ê°œ")
            with col2:
                st.metric("ê³¼ì„¸ ìƒí’ˆ", f"{(df_master['ê³¼ì„¸ì—¬ë¶€']=='ê³¼ì„¸').sum():,}ê°œ")
            with col3:
                st.metric("ë©´ì„¸ ìƒí’ˆ", f"{(df_master['ê³¼ì„¸ì—¬ë¶€']=='ë©´ì„¸').sum():,}ê°œ")
            
            if debug:
                st.success("âœ… ë§ˆìŠ¤í„° ë°ì´í„° ë¡œë“œ ì„±ê³µ!")
                st.dataframe(df_master.head(), use_container_width=True)
        else:
            st.warning("âš ï¸ ë§ˆìŠ¤í„° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ë™ ì—…ë¡œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
            # íŒŒì¼ ì§ì ‘ ì—…ë¡œë“œ ì˜µì…˜
            uploaded_master = st.file_uploader("ë§ˆìŠ¤í„° ë°ì´í„° ì—…ë¡œë“œ", type=['xlsx', 'xls', 'csv'])
            if uploaded_master:
                try:
                    if uploaded_master.name.endswith('.csv'):
                        df_master = pd.read_csv(uploaded_master)
                    else:
                        df_master = pd.read_excel(uploaded_master)
                    df_master = df_master.drop_duplicates(subset=['SKUì½”ë“œ'], keep='first')
                    st.success(f"âœ… {len(df_master)}ê°œ SKU ë¡œë“œ ì™„ë£Œ")
                except Exception as e:
                    st.error(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    
    st.markdown("---")
    
    # íŒŒì¼ ì—…ë¡œë“œ
    st.header("1ï¸âƒ£ ì›ë³¸ íŒŒì¼ ì—…ë¡œë“œ")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        file1 = st.file_uploader("ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´", type=['xlsx', 'xls'])
    with col2:
        file2 = st.file_uploader("ì´ì¹´ìš´íŠ¸", type=['xlsx', 'xls'])
    with col3:
        file3 = st.file_uploader("ê³ ë„ëª°", type=['xlsx', 'xls'])
    
    # ì²˜ë¦¬ ì‹¤í–‰
    st.header("2ï¸âƒ£ ë°ì´í„° ì²˜ë¦¬")
    
    if st.button("ğŸš€ ì²˜ë¦¬ ì‹œì‘", type="primary", disabled=not(file1 and file2 and file3)):
        if file1 and file2 and file3:
            if not df_master.empty:
                with st.spinner('ì²˜ë¦¬ ì¤‘...'):
                    result = process_all_files(file1, file2, file3, df_master)
                    df_main, df_qty, df_pack, df_ecount, success, message, warnings = result
                
                if success:
                    st.balloons()
                    st.success(message)
                    
                    # Graph APIë¡œ ì €ì¥
                    if GRAPH_AVAILABLE:
                        with st.spinner('SharePointì— ê¸°ë¡ ì €ì¥ ì¤‘...'):
                            save_success, save_msg = save_to_sharepoint_records(df_main, df_ecount)
                            if save_success:
                                st.success(save_msg)
                            else:
                                st.warning(save_msg)
                    
                    # ê²½ê³  í‘œì‹œ
                    if warnings:
                        with st.expander(f"âš ï¸ í™•ì¸ í•„ìš” ({len(warnings)}ê±´)"):
                            for w in warnings:
                                st.markdown(w)
                    
                    # ì„¸ì…˜ ì €ì¥
                    st.session_state['last_result'] = df_main
                    
                    # ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
                    st.markdown("---")
                    st.header("3ï¸âƒ£ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")
                    
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    
                    tabs = st.tabs(["ğŸ¢ ì´ì¹´ìš´íŠ¸", "ğŸ“‹ í¬ì¥ë¦¬ìŠ¤íŠ¸", "ğŸ“¦ ìˆ˜ëŸ‰ìš”ì•½", "âœ… ìµœì¢…ê²°ê³¼"])
                    
                    with tabs[0]:
                        st.dataframe(df_ecount.head(20), use_container_width=True)
                        st.download_button(
                            "ğŸ“¥ ì´ì¹´ìš´íŠ¸ ì—…ë¡œë“œìš© ë‹¤ìš´ë¡œë“œ",
                            to_excel_formatted(df_ecount, 'ecount_upload'),
                            f"ì´ì¹´ìš´íŠ¸_{timestamp}.xlsx",
                            mime="application/vnd.ms-excel"
                        )
                    
                    with tabs[1]:
                        st.dataframe(df_pack.head(20), use_container_width=True)
                        st.download_button(
                            "ğŸ“¥ í¬ì¥ë¦¬ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ",
                            to_excel_formatted(df_pack, 'packing_list'),
                            f"í¬ì¥ë¦¬ìŠ¤íŠ¸_{timestamp}.xlsx",
                            mime="application/vnd.ms-excel"
                        )
                    
                    with tabs[2]:
                        st.dataframe(df_qty, use_container_width=True)
                        st.download_button(
                            "ğŸ“¥ ìˆ˜ëŸ‰ìš”ì•½ ë‹¤ìš´ë¡œë“œ",
                            to_excel_formatted(df_qty, 'quantity_summary'),
                            f"ìˆ˜ëŸ‰ìš”ì•½_{timestamp}.xlsx",
                            mime="application/vnd.ms-excel"
                        )
                    
                    with tabs[3]:
                        st.dataframe(df_main.head(20), use_container_width=True)
                        st.download_button(
                            "ğŸ“¥ ìµœì¢…ê²°ê³¼ ë‹¤ìš´ë¡œë“œ",
                            to_excel_formatted(df_main),
                            f"ìµœì¢…ê²°ê³¼_{timestamp}.xlsx",
                            mime="application/vnd.ms-excel"
                        )
                else:
                    st.error(message)
            else:
                st.error("ë§ˆìŠ¤í„° ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤!")
        else:
            st.warning("3ê°œ íŒŒì¼ì„ ëª¨ë‘ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”!")

elif menu == "ğŸ“ˆ íŒë§¤ ë¶„ì„":
    st.title("ğŸ“ˆ íŒë§¤ ë¶„ì„")
    
    if 'last_result' in st.session_state:
        df_records = st.session_state['last_result'].copy()
        df_records['ì£¼ë¬¸ì¼ì'] = datetime.now()
        create_analytics_dashboard(df_records)
    else:
        st.info("ë¨¼ì € ì£¼ë¬¸ ì²˜ë¦¬ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")

elif menu == "âš™ï¸ ì„¤ì •":
    st.title("âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
    
    # Graph API ì„¤ì •
    st.header("ğŸ“ Microsoft Graph API ì„¤ì •")
    
    if "sharepoint" in st.secrets:
        col1, col2 = st.columns(2)
        with col1:
            st.text_input("Tenant ID", value=st.secrets["sharepoint"]["tenant_id"][:20]+"...", disabled=True)
            st.text_input("Client ID", value=st.secrets["sharepoint"]["client_id"][:20]+"...", disabled=True)
        with col2:
            st.text_input("Site Name", value=st.secrets["sharepoint_files"]["site_name"], disabled=True)
            st.text_input("File Name", value=st.secrets["sharepoint_files"]["file_name"], disabled=True)
        
        if st.button("ğŸ”„ Graph API ì—°ê²° í…ŒìŠ¤íŠ¸"):
            with st.spinner("í…ŒìŠ¤íŠ¸ ì¤‘..."):
                token = get_graph_token()
                if token:
                    st.success("âœ… Microsoft Graph API ì—°ê²° ì„±ê³µ!")
                else:
                    st.error("âŒ Graph API ì—°ê²° ì‹¤íŒ¨")
    else:
        st.warning("Graph API ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # AI ì„¤ì •
    st.header("ğŸ¤– AI ì„¤ì •")
    
    if "GEMINI_API_KEY" in st.secrets:
        st.text_input("Gemini API Key", value=st.secrets["GEMINI_API_KEY"][:10]+"...", disabled=True)
        
        if st.button("ğŸ”„ AI ì—°ê²° í…ŒìŠ¤íŠ¸"):
            with st.spinner("í…ŒìŠ¤íŠ¸ ì¤‘..."):
                model = init_gemini()
                if model:
                    st.success("âœ… Gemini AI ì—°ê²° ì„±ê³µ!")
                else:
                    st.error("âŒ AI ì—°ê²° ì‹¤íŒ¨")
    else:
        st.warning("Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # ì‹œìŠ¤í…œ ì •ë³´
    st.header("â„¹ï¸ ì‹œìŠ¤í…œ ì •ë³´")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Graph API", "í™œì„±í™”" if GRAPH_AVAILABLE else "ë¹„í™œì„±í™”")
    with col2:
        st.metric("AI ë¶„ì„", "í™œì„±í™”" if GEMINI_AVAILABLE else "ë¹„í™œì„±í™”")
    with col3:
        st.metric("ë²„ì „", "v2.0")
